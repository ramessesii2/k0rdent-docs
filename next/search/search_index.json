{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the k0rdent documentation","text":""},{"location":"#introductions","title":"Introductions","text":"<p>k0rdent has been developed to provide a way to manage distributed infrastructure at massive scale leveraging kubernetes.</p> <p>The project is based on the premise that:</p> <ul> <li>Kubernetes and its ecosystem are mature and inherently stable.</li> <li>Large scale adoption of Kubernetes means that k0rdent can run anywhere.</li> <li>Community standards and open source projects ensure support and reduce adoption risk.</li> </ul> <p>The goal of the k0rdent project is to provide platform engineers with the means to deliver a distributed container management environment (DCME) and enable them to compose unique internal developer platforms (IDP) to support a diverse range of complex modern application workloads.</p> <p>Another way to think of k0rdent is as a \"super control plane\" designed to ensure the consistent provisioning and lifecycle management of kubernetes clusters and the services that make them useful.</p> <p>In short: Kubernetes clusters at scale, managed centrally, template driven, based on open community driven standards, enabling Golden Paths ... k0rdent aspires to do all of that.</p> <p>Whether you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, k0rdent provides a consistent way to do it. With full life-cycle management, including provisioning, configuration, and maintenance, k0rdent is designed to be a repeatable and secure way to manage your Kubernetes clusters in a central location.</p>"},{"location":"#structure-and-history","title":"Structure and History","text":"<p>The project has a number of components, including:</p> <ul> <li> <p>k0rdent: the overall project</p> <ul> <li> <p>k0rdent Cluster Manager (KCM)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>k0rdent State Manager (KSM)</p> <p>Installation and life-cycle management of deployed services.</p> <ul> <li>This is currently rolled into kcm, but may be split out in the future</li> <li>ksm leverages Project Sveltos     for an increasing amount of functionality</li> </ul> </li> <li> <p>k0rdent Observability and FinOps (KOF)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul> </li> </ul> <p>There are a few historical names that may show up in the code and in older docs, including:</p> <ul> <li>Project 2A: the original codename of k0rdent, 2A references the hexadecimal 0x2A,    or 42, which encompasses our hopes for the project.</li> <li>HMC or hmc: the original repository name for k0rdent and KCM   development</li> <li>motel: the original repository and codename for KOF</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the k0rdent Quick Start Guide to get started with a small deployment.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>k0rdent leverages the Cluster API provider ecosystem; the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works. </p> <ul> <li>AWS</li> <li>Azure</li> <li>vSphere</li> <li>OpenStack</li> <li>GCP</li> </ul> <p>k0rdent also includes a way to add custom providers, so it's possible to integrate it with other hyperscalers,  or even into an existing local infrastructure.</p>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to the development process and developer-specific notes is located in the main k0rdent repository.</p>"},{"location":"#where-to-go-from-here","title":"Where to go from here","text":"<p>This documentation covers all aspects of administering, using, and contributing to k0rdent, including:</p> <ul> <li>k0rdent concepts</li> <li>QuickStarts</li> <li>Administrator Guide</li> <li>User Guide</li> <li>k0rdent Templates Reference</li> <li>Troubleshooting</li> <li>Glossary</li> <li>Appendix</li> <li>Contributor's Guide</li> </ul>"},{"location":"admin/","title":"Administering k0rdent","text":"<p>Before you start working with k0rdent, it helps to understand a few basics.</p>"},{"location":"admin/#how-k0rdent-works","title":"How k0rdent works","text":"<p>k0rdent has several important subsystems, notably:</p> <ul> <li>KCM - k0rdent Cluster Manager: KCM wraps and manages Kubernetes Cluster API (ClusterAPI) and lets you treat clusters as Kubernetes objects. Within a k0rdent management cluster, you'll have a <code>ClusterDeployment</code> object that represents a deployed cluster, with <code>Machine</code> objects, and so on. When you create a <code>ClusterDeployment</code>, k0rdent deploys the cluster, when you delete it, k0rdent deletes it, and so on.</li> <li>KSM - k0rdent State Manager: KSM wraps and manages several interoperating open source projects such as Helm and Sveltos, which lets you treat services and applications as Kubernetes objects.</li> </ul> <p>Together, KCM and KSM interoperate to create a complete, template-driven system for defining and managing Internal Development Platforms (IDPs) made up of suites of services, plus a cluster and its components as realized on a particular cloud or infrastructure substrate.</p> <ul> <li> <p>ClusterAPI providers: ClusterAPI uses <code>providers</code> to manage different clouds and infrastructures, including bare metal. k0rdent ships with providers for AWS, Azure, OpenStack and vSphere, and you can add additional providers in order to control other clouds or infrastructures that ClusterAPI supports.</p> </li> <li> <p>Templates: When you create a cluster, that cluster is based on a template, which specifies all of the various information about the cluster, such as where to find images, and so on. These templates get installed into k0rdent, but they don't do anything until you reference them in a <code>ClusterDeployment</code> that represents an actual cluster.</p> <pre><code>graph LR;\n    subgraph Infrastructure\n        WN1[\"ClusterDeployment Worker Node\"]\n        CP[\"ClusterDeployment Managed Server Control Plane\"]\n        WN2[\"ClusterDeployment Worker Node\"]\n    end\n\n    subgraph \"k0rdent Mgmnt Cluster\"\n        CT[\"Cluster Template\"]\n        P[\"Provider\"]\n    end\n\n    CP --&gt;|Controls| WN1\n    CP --&gt;|Controls| WN2\n    CT --&gt;|Defines| CP &amp; WN1 &amp; WN2\n    P --&gt;|Provisions| Infrastructure\n</code></pre> <p>k0rdent can also manage these clusters, upgrading them, scaling them, or installing software and services.</p> </li> <li> <p>Services: To add (or manage) services, you also use templates. These <code>ServiceTemplate</code> objects are like <code>ClusterTemplate</code> objects, in that you install them into the cluster, but until they're actually referenced, they don't do anything. When you reference a <code>ServiceTemplate</code> as part of a <code>ClusterDeployment</code>, k0rdent knows to install that service into that cluster.</p> <pre><code>graph TB;\n\n    subgraph \"k0rdent Mgmnt Cluster\"\n        ST[\"Service Template\"]\n    end\n\n    subgraph Infrastructure\n        CP[\"ClusterDeployment Managed Server Control Plane\"]\n\n        subgraph WN1[\"Worker Node\"]\n        POD1[App]\n        end\n\n        subgraph WN2[\"Worker Node\"]\n        POD2[App]\n        end\n    end\n    CP --&gt;|Controls| WN1\n    CP --&gt;|Controls| WN2\n    POD1 --&gt;|Defined By| ST\n    POD2 --&gt;|Defined By| ST\n</code></pre> <p>These services can be actual services, such as Nginx or Kyverno, or they can be user applications.</p> </li> </ul>"},{"location":"admin/#how-credentials-work","title":"How Credentials work","text":"<p>Of course you can't do any of this without permissions. As a human, you can log into, say, AWS, and tell it to create a new instance on which you are going to install Kubernetes, but how does k0rdent get that permission? It gets it through the use of <code>Credential</code> objects.</p> <p>When you create a <code>ClusterDeployment</code> or deploy an application, you include a reference to a <code>Credential</code> object that has been installed in the k0rdent management cluster. Depending on whether the target infrastructure is AWS, Azure, or something else, that <code>Credential</code> might reference an access key and secret, or it might reference a service provider, but all of that gets abstracted out by the time you get to the <code>Credential</code>, which is what you'll actually reference.</p> <pre><code>graph LR;\n    Secret[\"Secret\n    (includes Provider-specific passwords\n    secret keys, etc.)\"] --&gt; ClusterIdentity[\"ClusterIdentity\n    (Provider-specific)\"];\n    ClusterIdentity --&gt; Credential[\"Credential\"];</code></pre> <p>By abstracting everything out to create a standard <code>Credential</code> object, users never have to have access to actual credentials (lowercase \"c\"). This enables the administrator to keep those credentials private, and to rotate them as necessary without disturbing users or their applications. The administrator simply updates the <code>Credential</code> object and everything continues to work.</p> <p>You can find more information on creating these <code>Credential</code> objects in the Credentials chapter.</p>"},{"location":"admin/#k0rdent-and-gitops","title":"k0rdent and GitOps","text":"<p>At its heart, k0rdent is a Kubernetes-native way to declaratively specify what should be happening in the infrastructure and have that maintained. In other words, if you want to, say, scale up a cluster, you would give that cluster a new definition that includes the additional nodes, and then k0rdent, seeing that reality no longer matches the definition, it will reconcile the difference.</p> <p>In some ways this way of working is similar to GitOps, in which you commit definitions and tools such as Flux or ArgoCD ensure that reality matches the definition. We can say that k0rdent is GitOps-compatible, in the sense that you can (and should) consider storing k0rdent templates and YAML object definitions in Git repos, and can (and may want to) use GitOps tools like ArgoCD to modify and manage them upstream of k0rdent itself.</p> <p>The main difference is that k0rdent's way of representing clusters and services is fully compliant with Kubernetes-native tools like ClusterAPI, Sveltos and Helm. So you could, if you needed to, port much of what you do with k0rdent templates and objects directly to other solution environments that leverage these standard tools.</p>"},{"location":"admin/#the-k0rdent-initialization-process","title":"The k0rdent initialization process","text":""},{"location":"admin/#the-process","title":"The process","text":"<p>The k0rdent initialization process involves tools such as Helm and FluxCD.</p> <ol> <li><code>helm install kcm</code> brings up the bootstrap components (yellow in the above diagram).</li> <li><code>kcm-controller-manager</code> sets up webhooks to validate its <code>CustomResource</code> objects, then cert-manager handles the webhooks\u2019 certificates.</li> <li><code>kcm-controller-manager</code> generates a <code>Release</code> object corresponding to the KCM helm chart version.</li> <li><code>kcm-controller-manager</code> (or rather the release-controller inside it) generates template objects (<code>ProviderTemplate</code>/<code>ClusterTemplate</code>/<code>ServiceTemplate</code>) corresponding to a <code>Release</code> to be further processed.</li> <li><code>kcm-controller-manager</code> generates a <code>HelmRelease</code> object for every standard template. Note that this includes the KCM helm chart itself.</li> <li>Flux (source-controller and helm-controller pods) reconciles the HelmRelease objects. In other words, it installs all the helm charts referred to in the templates. After this point, the deployment is completely controlled by Flux.</li> <li><code>kcm-controller-manager</code> creates a <code>Management</code> object that refers to the above <code>Release</code> and the <code>ProviderTemplate</code> objects. The <code>Management</code> object represents the k0rdent management cluster as a whole. The management cluster Day-2 operations (such as upgrade) are  executed by manipulating the <code>Release</code> and <code>Management</code> objects.</li> <li><code>kcm-controller-manager</code> generates an empty <code>AccessManagement</code> object. <code>AccessManagement</code> defines access rules for <code>ClusterTemplate</code>/<code>ServiceTemplate</code> propagation across user namespaces. Further, the <code>AccessManagement</code> might be edited and used along with admin-created <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects.</li> </ol> <p>This Administration Guide provides information on:</p> <ul> <li>Installing and preparing k0rdent for use</li> <li>Working with clusters</li> <li>Working with services</li> <li>Hosted control planes</li> <li>k0rdent Observability &amp; FinOps</li> <li>Upgrading k0rdent</li> <li>Access Management (Credentials and RBAC)</li> <li>Backup and Restore</li> <li>Regional Clusters</li> </ul>"},{"location":"admin/access/","title":"Access Control","text":"<p>k0rdent gives you fine-grained control over access to resources, whether  it's by creating credentials for cloud resources or creating roles within the cluster to control access to specific services or other Kubernetes resources.</p> <ul> <li>k0rdent Credentials Management</li> <li>k0rdent Role Based Access Control (RBAC)</li> </ul>"},{"location":"admin/access/credentials/","title":"The Credential System","text":"<p>In order for k0rdent to be able to take action on a particular provider, it must have the proper credentials. You can create those credentials as Kubernetes objects.</p> <ul> <li>The Credentials Process</li> <li>Credential Propagation</li> <li>Cluster Identity Distribution</li> </ul>"},{"location":"admin/access/credentials/cluster-identity-distribution/","title":"Cluster Identity Distribution","text":"<p>To deploy clusters correctly, the <code>ClusterIdentity</code> object (for example, <code>AWSClusterStaticIdentity</code>) and all of its referenced resources (such as <code>Secrets</code>) must exist in the same cluster where the CAPI objects are created. This configuration is different for each provider.</p> <p>Starting from v1.5.0 the Cluster Identity Distribution system was introduced. This applies to the regional <code>Credential</code> objects (with <code>spec.region</code>) and for the <code>Credential</code> distributed by the k0rdent AccessManagement system (Credentials with <code>k0rdent.mirantis.com/managed: \"true\"</code> label) (see The Credential Distribution System).</p> <p>Warning</p> <p> To ensure proper Cluster Identity distribution, make sure the following requirements are met:</p> <ol> <li> <p>The ProviderInterface object is correctly configured. For details, see  ProviderInterface Configuration.</p> </li> <li> <p>When distributing <code>ClusterIdentity</code> objects to regional clusters, the provider that defines the corresponding <code>ClusterIdentity</code> CRDs must be enabled on the management and on the regional cluster. For example, to distribute an <code>AWSClusterStaticIdentity</code> to a regional cluster, the AWS provider must be enabled on both the management and regional clusters.</p> </li> </ol>"},{"location":"admin/access/credentials/cluster-identity-distribution/#cluster-identity-distribution-process","title":"Cluster Identity Distribution Process","text":"<p>When you create a <code>Credential</code> object, Cluster Identity distribution begins. The example below walks through the process step-by-step:</p> <ol> <li> <p>User creates the following <code>Credential</code> and identity objects for the Azure provider in <code>region1</code> region:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: azure-cluster-credential\n  namespace: test\nspec:\n  region: region1\n  description: \"Azure Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: test\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  name: azure-cluster-identity\n  namespace: test\nspec:\n  allowedNamespaces: {}\n  clientID: \"${AZURE_CLIENT_ID}\"\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: test\n  tenantID: \"${AZURE_TENANT_ID}\"\n  type: ServicePrincipal\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: test\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: \"${AZURE_CLIENT_SECRET}\"\ntype: Opaque\n</code></pre> </li> <li> <p>The KCM controller retrieves all the <code>ProviderInterfaces</code> from the regional cluster registered with <code>region1</code> Region and looks for the <code>AzureClusterIdentity</code> object definition under <code>spec.clusterIdentities</code> of each <code>ProviderInterface</code> object. If nothing found, the cluster identity distribution will not work.</p> </li> <li> <p>The KCM controller copies the <code>test/azure-cluster-identity-secret</code> <code>Secret</code> and <code>test/azure-cluster-identity</code> <code>AzureClusterIdentity</code> objects from the management to the regional cluster.</p> </li> </ol>"},{"location":"admin/access/credentials/cluster-identity-distribution/#providerinterface-configuration","title":"ProviderInterface Configuration","text":"<p>A <code>Credential</code> in k0rdent could reference more than one <code>ClusterIdentity</code> type and each of those identities might have its own transitive references. It\u2019s important to have a mechanism to reliably identify all referenced resources to e.g. distribute them across namespaces and multiple regions.</p> <p>The <code>ProviderInterface</code> object which is a part of the <code>ProviderTemplate</code> should define a set of identity kinds, each with its own reference-resolution instructions. Providers included with k0rdent already have a preconfigured ProviderInterface as part of the <code>ProviderTemplate</code>. If you are using a custom or Bring-Your-Own provider, you must properly configure the ProviderInterface <code>spec.clusterIdentities</code> field to enable Cluster Identity distribution.</p> <p>The example of the <code>ProviderInterface</code> object for the Azure provider:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ProviderInterface\nmetadata:\n  name: cluster-api-provider-azure\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  clusterGVKs:\n    - group: infrastructure.cluster.x-k8s.io\n      version: v1beta1\n      kind: AzureCluster\n    - group: infrastructure.cluster.x-k8s.io\n      version: v1alpha1\n      kind: AzureASOManagedCluster\n  clusterIdentities:\n    - group: infrastructure.cluster.x-k8s.io\n      version: v1beta1\n      kind: AzureClusterIdentity\n      references:\n        - group: \"\"\n          version: v1\n          kind: Secret\n          nameFieldPath: spec.clientSecret.name\n          namespaceFieldPath: spec.clientSecret.namespace\n    - group: \"\"\n      version: v1\n      kind: Secret\n  description: \"Azure infrastructure provider for Cluster API\"\n</code></pre> <p>The <code>spec.clusterIdentities</code> section defines the cluster identity objects supported by this provider. Each item of the <code>clusterIdentities</code> array defines how to locate and resolve a referenced object associated with a ClusterIdentity.</p> <p>At the moment of writing, the Azure provider supports two kinds of Cluster Identities: <code>AzureClusterIdentity</code> and <code>Secret</code>. <code>Secret</code> identities do not have any transitive references while the <code>AzureClusterIdentity</code> references a <code>Secret</code> with the name defined under <code>spec.clientSecret.name</code> and the namespace defined under <code>spec.clientSecret.namespace</code>.</p> <p>Warning</p> <p> Cluster Identity distribution will not work if the ProviderInterface for a particular provider does not exist or does not have <code>spec.clusterIdentities</code> field defined. The KCM controller will not fail, but it will not create any cluster identity resources automatically. You will have to create it manually.</p>"},{"location":"admin/access/credentials/credentials-process/","title":"The process","text":"<p>In order to pass credentials to k0rdent so it can take action, the following has to happen:</p> <ol> <li> <p>The lead platform engineer, or whoever has access to the actual provider credentials, creates a <code>Secret</code> that includes that information. For example, for an AWS cluster, it might look like this:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n    name: aws-cluster-identity-secret\n    namespace: kcm-system\ntype: Opaque\nstringData:\n    AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n    SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Once this secret is created, it can be referenced without the user having access to the content, and thus the actual credentials.</p> </li> <li> <p>A provider-specific <code>ClusterIdentity</code> gets created. The <code>ClusterIdentity</code> references the <code>Secret</code> from step one. For example, for an AWS cluster, this object might look like this:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces: {}\n</code></pre> <p>Notice that it references the <code>aws-cluster-identity-secret</code> we created earlier. It also specifies the namespaces in which this <code>ClusterIdentity</code> can be used. (In this case there are no restrictions.)</p> </li> <li> <p>Now you can create a <code>Credential</code> object that references the <code>ClusterIdentity</code>, thus making the credentials available and specifying the namespaces where it can be used. Continuing our AWS example:</p> <p>Note</p> <p> A <code>Credential</code> may optionally specify the <code>spec.region</code> field. When set, all <code>ClusterDeployment</code> objects that reference this <code>Credential</code> will be deployed to the corresponding regional cluster. In this case, the required <code>ClusterIdentity</code> resources must exist in that regional cluster. For k0rdent v1.5.0 the <code>ClusterIdentity</code> resources for regional <code>Credential</code> are automatically synced with the regional cluster. Learn more in Creating a Credential in a Region.</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: aws-cluster-credential\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> Notice that it references the previous <code>ClusterIdentity</code> (in this case an <code>AWSClusterStaticIdentity</code>). Also notice that you can use the <code>.spec.description</code> field to add additional text about the <code>Credential</code> so users can choose if multiple <code>Credential</code> objects are available.</p> </li> <li> <p>Finally, when you create a <code>ClusterDeployment</code>, you reference the <code>Credential</code> object in order to enable k0rdent to pass that information to the infrastructure provider:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n    name: my-aws-clusterdeployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-16\n  credential: aws-cluster-credential\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> <p>As you can see, the user doesn't have to pass anything but the name of the <code>Credential</code> in order to deploy the cluster. So all an administrator has to do is add these <code>Credential</code>objects to the system and make them available. Note also that the <code>Credential</code> has to be available in the <code>ClusterDeployment</code>s namespace. (See Cloud provider credentials propagation for more information on how that works. )</p> </li> <li> <p>Optionally, certain credentials MAY be propagated to the <code>ClusterDeployment</code> after it is created.</p> <p>The following diagram illustrates the process:</p> <pre><code>flowchart TD\n  Step1[\"&lt;b&gt;Step 1&lt;/b&gt; (Lead Engineer):&lt;br/&gt;Create ClusterIdentity and Secret objects where ClusterIdentity references Secret\"]\n  Step1 --&gt; Step2[\"&lt;b&gt;Step 2&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create Credential object referencing ClusterIdentity\"]\n  Step2 --&gt; Step3[\"&lt;b&gt;Step 3&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create ClusterDeployment referencing Credential object\"]\n  Step3 --&gt; Step4[\"&lt;b&gt;Step 4&lt;/b&gt; (Any Engineer):&lt;br/&gt;Apply ClusterDeployment, wait for provisioning &amp; reconciliation, then propagate credentials to nodes if necessary\"]</code></pre> <p>By design steps 1 and 2 should be executed by the lead engineer who has access to the credentials. Thus credentials could be used by engineers without a need to have access to actual credentials or underlying resources, like <code>ClusterIdentity</code>.</p> </li> </ol>"},{"location":"admin/access/credentials/credentials-propagation/","title":"Cloud provider credentials propagation","text":"<p>Some components in the cluster deployment require cloud provider credentials to be passed for proper functioning. For example, Cloud Controller Manager (CCM) requires provider credentials to create load balancers and provide other functionality.</p> <p>This poses a challenge for credentials delivery. Currently <code>cloud-init</code> is used to pass all necessary credentials, but this approach has several problems:</p> <ul> <li>Credentials are stored unencrypted in the instance metadata.</li> <li>Rotation of the credentials is impossible without complete instance   redeployment.</li> <li>Possible leaks, since credentials are copied to several <code>Secret</code> objects   related to bootstrap data.</li> </ul> <p>To solve these problems in k0rdent we're using the Sveltos controller, which can render the CCM template with all necessary data from the CAPI provider resources (like <code>ClusterIdentity</code>) and can create secrets directly on the cluster deployment.</p> <p>Note</p> <p> CCM template examples can be found in <code>*-credentials.yaml</code> here. Look for the <code>ConfigMap</code> object that has the <code>projectsveltos.io/template: \"true\"</code> annotation and <code>*-resource-template</code> as the object name.</p> <p>This eliminates the need to pass anything credentials-related to <code>cloud-init</code> and makes it possible to rotate credentials automatically without the need for instance redeployment.</p> <p>Also, this automation makes it possible to separate roles and responsibilities so that only the lead engineer has access to credentials, and other engineers can use them without seeing values and even any access to underlying infrastructure platform.</p> <p>The process is fully automated and credentials will be propagated automatically within the <code>ClusterDeployment</code> reconciliation process; user only needs to provide the correct <code>Credential</code> object.</p>"},{"location":"admin/access/credentials/credentials-propagation/#provider-specific-notes","title":"Provider-specific notes","text":"<p>Since this feature depends on the provider, it's important to review any provider-specific notes and clarifications.</p> <p>Note</p> <p>More detailed research notes can be found here.</p>"},{"location":"admin/access/credentials/credentials-propagation/#aws","title":"AWS","text":"<p>Since AWS uses roles, which are assigned to instances, no additional credentials will be created.</p> <p>The AWS provider supports 3 types of <code>ClusterIdentity</code> and, which one to use depends on your specific use case. More information regarding CAPA <code>ClusterIdentity</code> resources can be found in the CRD Reference.</p>"},{"location":"admin/access/credentials/credentials-propagation/#azure","title":"Azure","text":"<p>Currently the Cluster API Azure (CAPZ) provider creates <code>azure.json</code> <code>Secret</code> objects in the same namespace as the <code>Cluster</code> object. By design they should be referenced in the <code>cloud-init</code> YAML later during bootstrap process.</p> <p>In k0rdent these <code>Secret</code> objects aren't used and will not be added to the <code>cloud-init</code>, but engineers can access them without restrictions, which is a security issue.</p>"},{"location":"admin/access/credentials/credentials-propagation/#openstack","title":"OpenStack","text":"<p>For OpenStack, CAPO relies on a <code>clouds.yaml</code> file. In k0rdent, you provide this file in a Kubernetes <code>Secret</code> that references OpenStack credentials (ideally application credentials for enhanced security). During reconciliation, KCM automatically generates the cloud-config required by OpenStack\u2019s cloud-controller-manager.</p> <p>For more details, refer to the KCM OpenStack Credential Propagation doc.</p>"},{"location":"admin/access/credentials/credentials-propagation/#adopted-clusters","title":"Adopted clusters","text":"<p>Credentials for adopted clusters consist of a secret containing a kubeconfig file to access the existing kubernetes cluster.  The kubeconfig file for the cluster should be contained in the value key of the secret object. The following is an example of  a secret that contains the kubeconfig for an adopted cluster. To create this secret, first create or obtain a kubeconfig file  for the cluster that is being adopted and then run the following command to base64 encode it:</p> <pre><code>cat kubeconfig | base64 -w 0\n</code></pre> <p>Once you have obtained a base64 encoded kubeconfig file create a secret:</p> <pre><code>apiVersion: v1\ndata:\n  value: &lt;base64 encoded kubeconfig file&gt;\nkind: Secret\nmetadata:\n  name: adopted-cluster-kubeconf\n  namespace: &lt;namespace&gt;\ntype: Opaque\n</code></pre>"},{"location":"admin/access/credentials/credentials-propagation/#the-credential-distribution-system","title":"The Credential Distribution System","text":"<p>k0rdent provides a mechanism to distribute <code>Credential</code> objects across namespaces using the <code>AccessManagement</code> object. This object defines a set of <code>accessRules</code> that determine how credentials are distributed.</p> <p>Each access rule specifies:</p> <ol> <li>The target namespaces where credentials should be delivered.</li> <li>A list of <code>Credential</code> names to distribute to those namespaces.</li> </ol> <p>The KCM controller copies the specified <code>Credential</code> objects from the <code>system</code> (defaults to <code>kcm-system</code>) namespace to the target namespaces based on the <code>accessRules</code> in the <code>AccessManagement</code> spec.</p> <p>Note</p> <p> Starting from v1.5.0 KCM controller also copies all the cluster identity resources and all the referenced identity objects across namespaces. Follow Cluster Identity Distribution System for details.</p> <p>Info</p> <p> Access rules can also include <code>Cluster</code> and <code>Service</code> Template Chains (<code>ClusterTemplateChain</code> objects and <code>ServiceTemplateChain</code> objects) to distribute templates to target namespaces. For more details, read: Template Life Cycle Management.</p>"},{"location":"admin/access/credentials/credentials-propagation/#how-to-configure-credential-distribution","title":"How to Configure Credential Distribution","text":"<p>To configure the distribution of <code>Credential</code> objects:</p> <ol> <li>Edit the <code>AccessManagement</code> object.</li> <li>Populate the <code>.spec.accessRules</code> field with the list of <code>Credential</code> names and the target namespaces.</li> </ol> <p>Here\u2019s an example configuration:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - dev\n        - test\n    credentials:\n      - aws-demo\n      - azure-demo\n</code></pre> <p>In this example, the <code>aws-demo</code> and <code>azure-demo</code> <code>Credential</code> objects will be distributed to the <code>dev</code> and <code>test</code> namespaces.</p>"},{"location":"admin/access/credentials/credentials-propagation/#regional-credential-distribution","title":"Regional Credential Distribution","text":"<p>To deploy clusters correctly, the <code>ClusterIdentity</code> object (for example, <code>AWSClusterStaticIdentity</code>) and all of its referenced resources (such as <code>Secrets</code>) must exist in the same cluster where the CAPI objects are created. When a cluster is deployed in a specific region, all required ClusterIdentity objects must be present in the corresponding regional cluster.</p> <p>Starting from v1.5.0, when creating regional Credentials (with <code>spec.region</code>), all ClusterIdentity objects and their referenced resources are automatically synchronized with the regional cluster.</p> <p>For example, when you create the following Credential object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: aws-cluster-credential\n  namespace: test\nspec:\n  region: region1\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>and <code>AWSClusterStaticIdentity</code> references the <code>aws-cluster-identity-secret</code> secret:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces: {}\n</code></pre> <p>The KCM controller will do the following:</p> <ol> <li>Copy the <code>aws-cluster-identity-secret</code> from the system namespace of the management cluster to the system namespace of the regional cluster.</li> <li>Copy the <code>aws-cluster-identity</code> AWSClusterStaticIdentity object from the management cluster to the regional cluster.</li> </ol> <p>Warning</p> <p> For Cluster Identity distribution to function correctly, the corresponding ProviderInterface must be properly configured. Providers included with k0rdent already have a preconfigured ProviderInterface as part of the ProviderTemplate. If you are using a custom or Bring-Your-Own provider, you must properly configure the ProviderInterface to enable Cluster Identity distribution. For detailed instructions, refer to the Cluster Identity Distribution System documentation.</p>"},{"location":"admin/access/rbac/","title":"Role Based Access Control","text":"<p>k0rdent provides the opportunity to use Role Based Access Control in order to try to use the principle of least privilege and only give users access to the objects and resources they absolutely have to have.</p> <ul> <li>What Roles Do</li> <li>Role Definitions</li> <li>Limiting Access</li> </ul>"},{"location":"admin/access/rbac/limiting-access/","title":"Limiting Access","text":"<p>While you could certainly run k0rdent with all of your people having access to everything and putting all of their <code>ClusterDeployment</code> objects, and so on in the <code>kcm-system</code> namespace, you've probably already realized that this is not a good idea. It's a bit like having everyone use <code>root</code> to do all of their daily business.  It's asking for trouble.</p> <p>Fortunately, k0rdent makes it possible to avoid this situation.  In general, there are two situations of concern:</p> <ul> <li>Limiting access to system credentials</li> <li>Limiting access to the <code>kcm-system</code> namespace</li> </ul> <p>Both are straightforward, and the principles apply to any restrictions you may wish to apply.</p>"},{"location":"admin/access/rbac/limiting-access/#limiting-credential-access","title":"Limiting credential access","text":"<p>In order for k0rdent to manage infrastructure, you must provide it with access keys, passwords, and so on. In order to avoid making these credentials (little \"c\") available to all your developers, you can store them as <code>Secret</code> objects in the management cluster.</p> <p>However, you still need to hide those <code>Secret</code> objects from the developer to keep those credentials safe.  Fortunately, the developer never needs to use a <code>Secret</code> directly; it's embedded in a <code>Credential</code>, along with the cluster identity. So to keep credentials safe, you need to limit the developer's access to <code>kcm-system</code> namespace objects to just <code>Credential</code> objects.</p> <p>The <code>kcm-credentials-viewer-role</code> <code>ClusterRole</code> provides read-only access to <code>Credential</code> objects,  and nothing else:</p> <p><pre><code>kubectl describe clusterrole kcm-credentials-viewer-role\n</code></pre> <pre><code>Name:         kcm-credentials-viewer-role\nLabels:       app.kubernetes.io/managed-by=Helm\n              helm.toolkit.fluxcd.io/name=kcm\n              helm.toolkit.fluxcd.io/namespace=kcm-system\n              k0rdent.mirantis.com/aggregate-to-namespace-editor=true\n              k0rdent.mirantis.com/aggregate-to-namespace-viewer=true\nAnnotations:  meta.helm.sh/release-name: kcm\n              meta.helm.sh/release-namespace: kcm-system\nPolicyRule:\n  Resources                         Non-Resource URLs  Resource Names  Verbs\n  ---------                         -----------------  --------------  -----\n  credentials.k0rdent.mirantis.com  []                 []              [get list watch]\n</code></pre></p> <p>So to assign these limited privileges to a developer, create the  following <code>RoleBinding</code>, which defines what they can do in the referenced namespace (in this case, <code>kcm-system</code>).</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: credential-viewer\n  namespace: kcm-system\nsubjects:\n  - kind: User\n    name: user\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kcm-credentials-viewer-role\nEOF\n</code></pre> <p>You can use the same principle to control where developers can actually put their applications and <code>ClusterDeployment</code> objects.</p>"},{"location":"admin/access/rbac/limiting-access/#keep-users-out-of-the-kcm-system-namespace","title":"Keep users out of the <code>kcm-system</code> namespace","text":"<p>The <code>kcm-system</code> namespace should be reserved for core components that come with k0rdent That means users should not create resources (such as <code>Secret</code>, <code>ClusterDeployment</code>, or <code>ServiceTemplate</code> objects)  in this namespace.</p> <p>Instead, create a namespace for your user(s):</p> <pre><code>kubectl create namespace user-cluster-ns\n</code></pre> <p>You can then give them the ability to create clusters in that namespace using the <code>kcm-namespace-editor-role</code> <code>ClusterRole</code>:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: user-cluster-ns-editor\n  namespace: user-cluster-ns\nsubjects:\n  - kind: User\n    name: user\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kcm-namespace-editor-role\n</code></pre> <p>This <code>ClusterRole</code> enables the user to create clusters but limits their ability to add  new <code>ServiceTemplate</code> objects:</p> <p><pre><code>kubectl describe clusterrole kcm-namespace-editor-role -n kcm-system\n</code></pre> <pre><code>Name:         kcm-namespace-editor-role\nLabels:       app.kubernetes.io/managed-by=Helm\n              helm.toolkit.fluxcd.io/name=kcm\n              helm.toolkit.fluxcd.io/namespace=kcm-system\nAnnotations:  meta.helm.sh/release-name: kcm\n              meta.helm.sh/release-namespace: kcm-system\nPolicyRule:\n  Resources                                      Non-Resource URLs  Resource Names  Verbs\n  ---------                                      -----------------  --------------  -----\n  clusterdeployments.k0rdent.mirantis.com        []                 []              [create delete get list patch update watch]\n  clusteripamclaims.k0rdent.mirantis.com/status  []                 []              [create delete get list patch update watch]\n  clusteripamclaims.k0rdent.mirantis.com         []                 []              [create delete get list patch update watch]\n  secrets                                        []                 []              [get list watch]\n  helmcharts.helm.toolkit.fluxcd.io              []                 []              [get list watch]\n  helmrepositories.helm.toolkit.fluxcd.io        []                 []              [get list watch]\n  clustertemplatechains.k0rdent.mirantis.com     []                 []              [get list watch]\n  clustertemplates.k0rdent.mirantis.com          []                 []              [get list watch]\n  credentials.k0rdent.mirantis.com               []                 []              [get list watch]\n  servicetemplatechains.k0rdent.mirantis.com     []                 []              [get list watch]\n  servicetemplates.k0rdent.mirantis.com          []                 []              [get list watch]\n</code></pre></p> <p>Creating <code>ServiceTemplate</code> objects enables new workloads on created clusters, so this is something that you will want to keep track of, either enabling creation on a case-by-case basis or keeping that as something to be handled by admins in the <code>kcm-system</code> namespace.</p> <p>For more information on the <code>ClusterRoles</code> available in k0rdent see the roles summary.</p>"},{"location":"admin/access/rbac/roles-summary/","title":"Roles summary","text":"<p>Note</p> <p> The names of the <code>ClusterRole</code> objects may have different prefixes depending on the name of the k0rdent Helm chart. The <code>ClusterRole</code> object definitions below use the <code>kcm</code> prefix, which is the default name of the k0rdent Helm chart.</p>"},{"location":"admin/access/rbac/roles-summary/#global-admin","title":"Global Admin","text":"<p>The <code>Global Admin</code> role provides full administrative access across all the k0rdent system.</p> <p>Name: <code>kcm-global-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to the k0rdent API</li> <li>Full access to Flux Helm repositories and Helm charts</li> <li>Full access to Cluster API identities</li> <li>Full access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Admin</code> role is authorized to perform the following actions:</p> <ol> <li>Manage the k0rdent configuration</li> <li>Manage namespaces in the management cluster</li> <li>Manage <code>ProviderTemplate</code> objects: add new templates or remove unneeded ones</li> <li>Manage <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in any namespace, including adding and removing templates</li> <li>Manage Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in any namespace</li> <li>Manage access rules for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, including distributing templates across namespaces using    <code>TemplateChain</code> objects</li> <li>Manage upgrade sequences for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects</li> <li>Manage and deploy Services across multiple clusters in any namespace by modifying <code>MultiClusterService</code> resources</li> <li>Manage <code>ClusterDeployment</code> objects in any namespace</li> <li>Manage <code>Credential</code> and <code>Secret</code> objects in any namespace</li> <li>Upgrade k0rdent</li> <li>Uninstall k0rdent</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#global-viewer","title":"Global Viewer","text":"<p>The <code>Global Viewer</code> role grants read-only access across the k0rdent system. It does not permit any modifications, including the creation of clusters.</p> <p>Name: <code>kcm-global-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-viewer: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to k0rdent API</li> <li>Read access to Flux Helm repositories and Helm charts</li> <li>Read access to Cluster API identities</li> <li>Read access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Viewer</code> role is authorized to perform the following actions:</p> <ol> <li>View the k0rdent configuration</li> <li>List namespaces available in the management cluster</li> <li>List and get the detailed information about available <code>ProviderTemplate</code> objects</li> <li>List available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in any namespace</li> <li>List and view detailed information about Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in any namespace</li> <li>View access rules for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, including <code>TemplateChain</code> objects in any namespace</li> <li>View full details about the created <code>MultiClusterService</code> objects</li> <li>List and view detailed information about <code>ClusterDeployment</code> objects in any namespace</li> <li>List and view detailed information about created <code>Credential</code> and <code>Secret</code> objects in any namespace</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#namespace-admin","title":"Namespace Admin","text":"<p>The <code>Namespace Admin</code> role provides full administrative access within a namespace.</p> <p>Name: <code>kcm-namespace-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployment</code>, <code>Credential</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in the namespace</li> <li>Full access to <code>TemplateChain</code> objects in the namespace</li> <li>Full access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Admin</code> role is authorized to perform the following actions within the namespace:</p> <ol> <li>Create and manage all <code>ClusterDeployment</code> objects in the namespace</li> <li>Create and manage <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in the namespace</li> <li>Manage the distribution and upgrade sequences of Templates within the namespace</li> <li>Create and manage Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> <li>Manage <code>Credential</code> objects created by any user in the namespace</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#namespace-editor","title":"Namespace Editor","text":"<p>The <code>Namespace Editor</code> role allows users to create and modify <code>ClusterDeployment</code> objects within namespace using predefined <code>Credential</code> and <code>Template</code> objects.</p> <p>Name: <code>kcm-namespace-editor-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployment</code> objects in the allowed namespace</li> <li>Read access to <code>Credential</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code>, and <code>TemplateChain</code> objects in the namespace</li> <li>Read access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Editor</code> role has the following permissions in the namespace:</p> <ol> <li>Can create and manage <code>ClusterDeployment</code> objects in the namespace using existing <code>Credential</code> and <code>Template</code> objects</li> <li>Can list and view detailed information about the <code>Credential</code> objects available in the namespace</li> <li>Can list and view detailed information about the available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects and the <code>Template</code>  upgrade sequences</li> <li>Can list and view detailed information about the Flux <code>HelmRepository</code> and <code>HelmChart</code> objects</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#namespace-viewer","title":"Namespace Viewer","text":"<p>The <code>Namespace Viewer</code> role grants read-only access to resources within a namespace.</p> <p>Name: <code>kcm-namespace-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to <code>ClusterDeployment</code> objects in the namespace</li> <li>Read access to <code>Credential</code>, <code>ClusterTemplate</code>, <code>ServiceTemplate</code>, and <code>TemplateChain</code> objects in the namespace</li> <li>Read access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Viewer</code> role has the following permissions in the namespace:</p> <ol> <li>Can list and view detailed information about all the <code>ClusterDeployment</code> objects in the allowed namespace</li> <li>Can list and view detailed information about <code>Credential</code> objects available in the specific namespace</li> <li>Can list and view detailed information about available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, and <code>Template</code>    upgrade sequences</li> <li>Can list and view detailed information about Flux <code>HelmRepository</code> and <code>HelmChart</code> objects</li> </ol>"},{"location":"admin/access/rbac/what-roles-do/","title":"What roles do","text":"<p>k0rdent leverages the Kubernetes RBAC system and provides a set of standard <code>ClusterRole</code> objects with associated permissions. These standard <code>ClusterRole</code> objects are created as part of the k0rdent helm chart. k0rdent roles are based on labels and aggregated permissions, meaning they automatically collect rules from other <code>ClusterRole</code> objects with specific labels.</p> <p>The following table outlines the roles available in k0rdent, along with their respective read/write or read-only permissions:</p> Roles Global Admin Global Viewer Namespace Admin Namespace Editor Namespace Viewer Scope Global Global Namespace Namespace Namespace k0rdent management r/w r/o - - - Namespaces management r/w r/o - - - Provider Templates r/w r/o - - - Global Template Management r/w r/o - - - Multi Cluster Service Management r/w r/o - - - Template Chain Management r/w r/o r/w r/o r/o Cluster and Service Templates r/w r/o r/w r/o r/o Credentials r/w r/o r/w r/o r/o Flux Helm objects r/w r/o r/w r/o r/o Cluster Deployments r/w r/o r/w r/w r/o"},{"location":"admin/backup/","title":"Backing Up and Restoring a k0rdent Management Cluster","text":"<p>Any production system needs to provide Disaster Recovery features, and the heart of these capabilities is the ability to perform backup and restore operations. In this chapter we'll look at backing up and restoring k0rdent management cluster so that in case of an emergency, you can restore your system to its previous condition or recreate it on another cluster.</p> <p>While it's possible to back up a Kubernetes cluster manually, it's better to build on the work of others. In this case we're going to leverage the <code>velero</code> project for backup management on the backend and see how it integrates with k0rdent to ensure data persistence and recovery.</p>"},{"location":"admin/backup/#motivation","title":"Motivation","text":"<p>The primary goal of this feature is to provide a reliable and efficient way to back up and restore a k0rdent deployment in the event of a disaster that impacts the management cluster. By using <code>velero</code> as the backup provider, we can create consistent backups across different cloud storage options while maintaining the integrity of critical resources.</p> <p>The main goal of the feature is to provide:</p> <ul> <li>Management Backup: The ability to backup all configuration objects created and managed by k0rdent, including   into an offsite location.</li> <li>Restore: The ability to create configuration objects from a specific Management Backup in order to create a management   cluster in the same state that existed at the time of backup without (re)provisioning of cloud resources.</li> <li>Disaster Recovery: The ability to restore k0rdent on another management cluster, plus ensuring that clusters are not   recreated or lost.</li> <li>Rollback: The possibility to manually restore after a specific event, such as a failed k0rdent upgrade</li> </ul>"},{"location":"admin/backup/#velero-as-provider-for-management-backups","title":"Velero as Provider for Management Backups","text":"<p><code>Velero</code> is an open-source tool that simplifies backing up and restoring clusters as well as individual resources. It seamlessly integrates into the k0rdent management environment to provide robust disaster recovery capabilities.</p> <p>The <code>velero</code> instance is part of the Helm chart that installs k0rdent, which means that it can be customized if necessary.</p> <p>k0rdent manages the schedule and is responsible for collecting data to be included in a backup.</p> <ul> <li>Preparing for Backups</li> <li>Scheduled Management Backups</li> <li>Management Backup on Demand</li> <li>What's Included in a Backup</li> <li>Restoring From Backup</li> <li>Upgrades and Rollbacks</li> <li>Caveats</li> <li>Customization</li> </ul>"},{"location":"admin/backup/caveats/","title":"Caveats / Limitations","text":"<p>The credentials stored in backups can and will get stale, so a proper rotation should be considered beforehand.</p> <p>Only plugins that support Object Store can be used to store backups into an object storage.</p> <p>All <code>velero</code> caveats and limitations are transitively implied in k0rdent. In particular, that means no backup encryption is provided until it is implemented by a <code>velero</code> plugin that supports encryption and cloud storage backups.</p>"},{"location":"admin/backup/caveats/#velero-backups-restores-deletion","title":"Velero Backups / Restores deletion","text":""},{"location":"admin/backup/caveats/#delete-restores","title":"Delete Restores","text":"<p>To delete a <code>velero</code> <code>Restore</code> from the management cluster and from cloud storage, delete <code>restores.velero.io</code> object(s), such as with the following command:</p> <pre><code>kubectl delete restores.velero.io -n kcm-system &lt;restore-name&gt;\n</code></pre> <p>Warning</p> <p> Deletion of a <code>Restore</code> object deletes it from both the management cluster and from cloud storage.</p>"},{"location":"admin/backup/caveats/#delete-backups","title":"Delete Backups","text":"<p>To remove a <code>velero</code> <code>Backup</code> from the management cluster, delete <code>backups.velero.io</code> object(s), such as with the following command:</p> <pre><code>kubectl delete backups.velero.io -n kcm-system &lt;velero-backup-name&gt;\n</code></pre> <p>Hint</p> <p> The command above only removes objects from the cluster; the data continues to persist on the cloud storage.</p> <p>The deleted object will be recreated in the cluster if its <code>BackupStorageLocation</code> <code>.spec.backupSyncPeriod</code> is set and does not equal <code>0</code>.</p> <p>To delete a <code>velero</code> <code>Backup</code> from the management cluster and from cloud storage, create the following <code>DeleteBackupRequest</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: DeleteBackupRequest\nmetadata:\n  name: delete-backup-completely\n  namespace: kcm-system\nspec:\n  backupName: &lt;velero-backup&gt;\n</code></pre> <p>Warning</p> <p> Deletion of a <code>Backup</code> object via the <code>DeleteBackupRequest</code> deletes it from both the management cluster and from the cloud storage.</p> <p>For reference, follow the official documentation.</p>"},{"location":"admin/backup/customization/","title":"Customization","text":"<p>This section covers different topics of customization regarding backing up and restoring k0rdent.</p>"},{"location":"admin/backup/customization/#velero-installation","title":"Velero installation","text":"<p>The Velero helm chart is supplied with the k0rdent helm chart and is enabled by default. There are 2 ways of customizing the chart values:</p> <ol> <li> <p>Install using <code>helm</code> and add corresponding parameters to the <code>helm install</code> command.</p> <p>Note</p> <p> Only a plugin that supports Object Store is required during restoration; the other parameters are optional.</p> <p>For example, this command installs k0rdent via <code>helm install</code> with a configured plugin, <code>BackupStorageLocation</code> and propagated credentials:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set-file regional.velero.credentials.secretContents.cloud=&lt;full-path-to-file&gt; \\\n --set regional.velero.credentials.useSecret=true \\\n --set regional.velero.backupsEnabled=true \\\n --set regional.velero.configuration.backupStorageLocation[0].name=&lt;backup-storage-location-name&gt; \\\n --set regional.velero.configuration.backupStorageLocation[0].provider=&lt;provider-name&gt; \\\n --set regional.velero.configuration.backupStorageLocation[0].bucket=&lt;bucket-name&gt; \\\n --set regional.velero.configuration.backupStorageLocation[0].config.region=&lt;region&gt; \\\n --set regional.velero.initContainers[0].name=velero-plugin-for-&lt;provider-name&gt; \\\n --set regional.velero.initContainers[0].image=velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt; \\\n --set regional.velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set regional.velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre> </li> <li> <p>Create or modify the existing <code>Management</code> object in the <code>.spec.config.kcm</code>.</p> <p>Note</p> <p> Only a plugin that supports Object Store is required during restoration; the other parameters are optional.</p> <p>For example, this is a <code>Management</code> object with a configured plugin and enabled metrics:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  # ...\n  core:\n    kcm:\n      config:\n        regional:\n          velero:\n            initContainers:\n            - name: velero-plugin-for-&lt;provider-name&gt;\n              image: velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt;\n              imagePullPolicy: IfNotPresent\n              volumeMounts:\n              - mountPath: /target\n                name: plugins\n            metrics:\n              enabled: true\n  # ...\n</code></pre> </li> </ol> <p>To fully disable <code>velero</code>, set the <code>regional.velero.enabled</code> parameter to <code>false</code>.</p>"},{"location":"admin/backup/customization/#schedule-expression-format","title":"Schedule Expression Format","text":"<p>The <code>ManagementBackup</code> <code>.spec.schedule</code> field accepts a correct Cron expression, along with the nonstandard predefined scheduling definitions and an extra definition <code>@every</code> with a number and a valid time unit (valid time units are <code>ns</code>, <code>us</code> (or <code>\u00b5s</code>), <code>ms</code>, <code>s</code>, <code>m</code>, <code>h</code>).</p> <p>The following list contains acceptable <code>.spec.schedule</code> example values:</p> <ul> <li><code>0 */1 * * *</code> (standard Cron expression)</li> <li><code>@hourly</code> (nonstandard predefined definition)</li> <li><code>@every 1h</code> (extra definition)</li> </ul>"},{"location":"admin/backup/customization/#putting-extra-objects-in-a-management-backup","title":"Putting Extra Objects in a Management Backup","text":"<p>If you need to back up objects other than those backed up by default, you can add the label <code>k0rdent.mirantis.com/component=\"kcm\"</code> to these objects.</p> <p>All objects containing the label will be automatically added to the management backup.</p>"},{"location":"admin/backup/ondemand-backups/","title":"Management Backup on Demand","text":"<p>To create a single backup of the existing k0rdent management cluster information, you can create a <code>ManagementBackup</code> object using a YAML document and the <code>kubectl</code> CLI. The object then creates only one instance of a backup. For example you can backup to the location created previously.</p> <p>Note</p> <p> For the regional cluster case, make sure to setup the same location on the regional cluster.</p> <p>Create a one time backup of your k0rdent cluster by creating the following object:</p> <pre><code>kubectl create -f - &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ManagementBackup\nmetadata:\n  name: example-backup\nspec:\n  storageLocation: aws-s3\nEOF\n</code></pre> <p>Confirm the backup was successful by navigating to the appropriate storage console UI or from the command line:</p> <pre><code>kubectl get managementbackups\n</code></pre> <p>The <code>managementbackup</code> should show as <code>Completed</code>:</p> <pre><code>NAME              LASTBACKUPSTATUS   NEXTBACKUP   AGE\nexample-backup    Completed                       8m\n</code></pre>"},{"location":"admin/backup/prepare-backups/","title":"Preparing for Backups","text":"<p>Note</p> <p> The following instructions are tailored for AWS. Please adapt them to your chosen platform and storage.</p> <p>Before you create a manual one-off or scheduled backup, review the steps below and update your configuration accordingly:</p> <ol> <li> <p>Verify whether the <code>velero</code> plugins have been installed as suggested in Velero installation.   If the <code>velero</code> plugins with the desired storage option   are already configured, please skip the next step.</p> </li> <li> <p>If no <code>velero</code> plugins have yet been installed in your k0rdent cluster,    start by editing the <code>kcm</code> <code>management</code> object so    that the velero plugin details are filled in under <code>spec.core.kcm</code>:</p> <pre><code>kubectl edit managements kcm\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  # ... \n  core:\n    kcm:\n      config:\n        regional:\n          velero:\n            initContainers:\n            - name: velero-plugin-for-&lt;PROVIDER-NAME&gt;\n              image: velero/velero-plugin-for-&lt;PROVIDER-NAME&gt;:&lt;PROVIDER-PLUGIN-TAG&gt;\n              imagePullPolicy: IfNotPresent\n              volumeMounts:\n              - mountPath: /target\n                name: plugins\n  # ...\n</code></pre> <p>For the regional cluster case, edit the corresponding <code>region</code> object in a similar way:</p> <pre><code>kubectl edit regions &lt;region-name&gt;\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Region\nmetadata:\n  name: &lt;region-name&gt;\nspec:\n  # ... \n  core:\n    kcm:\n      config:\n        velero:\n          initContainers:\n          - name: velero-plugin-for-&lt;PROVIDER-NAME&gt;\n            image: velero/velero-plugin-for-&lt;PROVIDER-NAME&gt;:&lt;PROVIDER-PLUGIN-TAG&gt;\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - mountPath: /target\n              name: plugins\n  # ...\n</code></pre> <p>Note, that the only change is the absence of the <code>regional</code> subpath.</p> <p>Please review Velero's Docker Hub image plugin repositories to help identify the required <code>&lt;PROVIDER-NAME&gt;</code>. Once the required image has been identified, select from the available tags to determine the correct <code>&lt;PROVIDER-PLUGIN-TAG&gt;</code>. In the case of AWS, the name would be <code>velero-plugin-for-aws</code>, we can select from the available tags.</p> </li> <li> <p>Prepare a storage location, such as an Amazon S3 bucket, to store k0rdent backups.</p> </li> <li> <p>Prepare a yaml containing a <code>BackupStorageLocation</code>    object referencing a <code>Secret</code> with credentials to access the cloud storage    (if the multiple credentials feature is supported by the plugin).    For example, you can create the <code>BackupStorageLocation</code> and the related <code>Secret</code>    yaml for the Amazon S3 configuration by following these steps.</p> <p>First create a file called <code>credentials.txt</code> with your credentials, as in:</p> <pre><code>[default]\naws_access_key_id = EXAMPLE_ACCESS_KEY_ID\naws_secret_access_key = EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>The IAM user being used in this configuration will require certain permissions for the   appropriate Velero S3 bucket access. Review the necessary permissions here. Reference the JSON policy file named <code>velero-policy.json</code> and take care to replace <code>${BUCKET}</code> with the correct bucket name).</p> <p>Note</p> <p> If you're using EKS, the \"user\" is actually a role. If you get an error such as...</p> <pre><code>AccessDenied: User: arn:aws:sts::026090528175:assumed-role/eksctl-JohnDoeEKSK0rdentMgmtClus-NodeInstanceRole-j0olMRJHrM0A/i-0f7dad2d91447f173 is not authorized to perform: s3:ListBucket on resource: \"arn:aws:s3:::nick-chase-backup-bucket\" because no identity-based policy allows the s3:ListBucket action\n</code></pre> <p>...you can extract the role from the message (in this example, it's the assumed-role) and create the policy. For example:</p> <pre><code>aws iam put-role-policy\n--role-name eksctl-JohnDoeEKSK0rdentMgmtClus-NodeInstanceRole-j0olMRJHrM0A\n--policy-name velero\n--policy-document file://velero-policy.json\n</code></pre> <p>Generate the necessary base64-encoded credentials using:</p> <pre><code>base64 -w0 credentials.txt; echo\n</code></pre> <p>Use this base64 value in the <code>data.cloud</code> field in the <code>Secret</code> object   to be created by the next listing, also make sure to substitute the   appropriate <code>AWS-REGION-NAME</code> and <code>BUCKET-NAME</code>:</p> <pre><code>kubectl create -f - &lt;&lt; EOF\n---\napiVersion: v1\ndata:\n  # base64-encoded credentials for Amazon S3 in the following format:\n  # [default]\n  # aws_access_key_id = EXAMPLE_ACCESS_KEY_ID\n  # aws_secret_access_key = EXAMPLE_SECRET_ACCESS_KEY\n  cloud: &lt;BASE64_VALUE&gt;\nkind: Secret\nmetadata:\n  name: cloud-credentials\n  namespace: kcm-system\ntype: Opaque\n---\napiVersion: velero.io/v1\nkind: BackupStorageLocation\nmetadata:\n  name: aws-s3\n  namespace: kcm-system\nspec:\n  config:\n    region: &lt;AWS-REGION-NAME&gt;\n  default: true # optional, if not set, then storage location name must always be set in ManagementBackup\n  objectStorage:\n    bucket: &lt;BUCKET-NAME&gt;\n  provider: aws\n  backupSyncPeriod: 1m\n  credential:\n    name: cloud-credentials\n    key: cloud\nEOF\n</code></pre> <p>For the regional cluster case, do exactly this step on the regional cluster.</p> <p>Note, that all of the management and regional cluster must have the same <code>BackupStorageLocation</code> specification with the same credentials.</p> </li> <li> <p>Confirm that the previous steps were applied correctly:</p> <pre><code>kubectl get managements kcm -o yaml\nkubectl get regions &lt;region-name&gt; -o yaml\n</code></pre> <p>The <code>management</code> or <code>region</code> configuration yaml should have the new velero plugin details, as shown in step 2.</p> <p>Now make sure the <code>backupstoragelocation</code> shows as <code>Available</code> on the management cluster (and regional cluster if applicable):</p> <pre><code>kubectl get backupstoragelocation -n kcm-system\n</code></pre> <pre><code>NAME     PHASE       LAST VALIDATED   AGE   DEFAULT\naws-s3   Available   27s              2d    true\n</code></pre> </li> </ol> <p>You can get more information on how to build these objects at the official Velero documentation.</p>"},{"location":"admin/backup/restore/","title":"Restoring From Backup","text":"<p>Note</p> <p> Please refer to the official migration documentation to familiarize yourself with potential limitations of the Velero backup system.</p> <p>In the event of disaster, you can restore from a backup by doing the following:</p> <ol> <li> <p>Create a clean k0rdent installation, including <code>velero</code> and its plugins.    Specifically, you want to avoid creating a <code>Management</code> object and similar objects because they    will be part of your restored cluster. You can remove these objects after installation, but you    can also install k0rdent without them in the first place:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set controller.createManagement=false \\\n --set controller.createAccessManagement=false \\\n --set controller.createRelease=false \\\n --set controller.createTemplates=false \\\n --set regional.velero.initContainers[0].name=velero-plugin-for-&lt;provider-name&gt; \\\n --set regional.velero.initContainers[0].image=velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt; \\\n --set regional.velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set regional.velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre> </li> <li> <p>Create the <code>BackupStorageLocation</code>/<code>Secret</code> objects that were created during the preparation stage    of creating a backup (preferably the same depending on a plugin).</p> </li> <li> <p>If a k0rdent management cluster version is less than <code>1.2.0</code>, apply the workaround to avoid failure during the restoration.    Related known issue: Velero #9023.</p> <p>The fix is to \"rename\" the Velero deployment via the override: in the <code>Management</code> object, add to the path <code>spec.core.kcm.config.velero</code> a new key <code>fullnameOverride</code> with the value <code>velero</code>.</p> <p>Example of a patch if the path <code>spec.core.kcm.config.velero</code> does not yet exist:</p> <pre><code>kubectl patch managements kcm \\\n  --type=json \\\n  -p='[{\"op\": \"add\", \"path\": \"/spec/core/kcm/config/velero\", \"value\": {\"fullnameOverride\": \"velero\"}}]'\n</code></pre> <p>Example how it should look after the required change:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        velero:\n          fullnameOverride: velero\n</code></pre> <p>This ensures that the Velero <code>Deployment</code> name is exactly <code>velero</code>, which is a requirement due to the aforementioned known issue.</p> </li> <li> <p>Restore the <code>kcm</code> system creating the <code>Restore</code> object.    Follow one of the case that is applicable to clusters' configuration in use:</p> <ol> <li> <p>If there are no regional clusters     or all regional clusters' infrastructure is healthy.</p> <p>Note that it is important to set the <code>.spec.existingResourcePolicy</code> field value to <code>update</code>.</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  includedNamespaces:\n  - '*'\n</code></pre> </li> <li> <p>If one or more regional clusters require reprovisioning.</p> <p>The following listing will create a <code>ConfigMap</code> object along with the <code>Restore</code> object, and it allows <code>Velero</code> to set the pause annotation to all of <code>regions</code> objects.</p> <p>Substitute <code>&lt;cluster-deployment-name&gt;</code> with the corresponding names of <code>ClusterDeployment</code> objects used for provisioning of the corresponding region cluster.</p> <p>Note that it is important to set the <code>.spec.existingResourcePolicy</code> field value to <code>update</code>.</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: add-region-pause-anno\n  namespace: kcm-system\ndata:\n  add-region-pause-anno: |\n    version: v1\n    resourceModifierRules:\n    - conditions:\n        groupResource: regions.k0rdent.mirantis.com\n      mergePatches:\n      - patchData: |\n          {\n            \"metadata\": {\n              \"annotations\": {\n                \"k0rdent.mirantis.com/region-pause\": \"true\"\n              }\n            }\n          }\n---\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  includedNamespaces:\n  - '*'\n  labelSelector:\n    matchExpressions:\n      - key: cluster.x-k8s.io/cluster-name\n        operator: NotIn\n        values: [\"&lt;cluster-deployment-name&gt;\"]\n      # Add new entries accordingly if more regional clusters require reprovisioning\n      # - key: cluster.x-k8s.io/cluster-name\n      #   operator: NotIn\n      #   values: [\"&lt;cluster-deployment-name&gt;\"]\n  resourceModifier:\n    kind: ConfigMap\n    name: add-region-pause-anno\n</code></pre> </li> </ol> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> <li> <p>If there were one or more regional clusters    that required reprovisioning, then:</p> <ol> <li> <p>On the management cluster, wait for the <code>regions</code> object readiness:</p> <pre><code>kubectl wait regions kcm --for=condition=Ready=True --timeout 30m\n</code></pre> </li> <li> <p>Manually ensure that the freshly reprovisioned regional cluster     runs and is accessable.</p> </li> <li> <p>On the regional cluster, repeat the second step, creating the     <code>BackupStorageLocation</code>/<code>Secret</code> objects that were created during     the preparation stage.</p> </li> <li> <p>On the regional cluster, restore the cluster by creating a new     <code>Restore</code> object:</p> <p>Note that in this case the <code>.spec.existingResourcePolicy</code> field is not set.</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;region-name&gt;-&lt;backup-name&gt;\n  excludedResources:\n  - mutatingwebhookconfiguration.admissionregistration.k8s.io\n  - validatingwebhookconfiguration.admissionregistration.k8s.io\n  includedNamespaces:\n  - '*'\n</code></pre> </li> <li> <p>On the regional cluster, wait until the <code>Restore</code>     status is <code>Completed</code> and all <code>ClusterDeployment</code> objects are ready.</p> </li> <li> <p>On the management cluster, unpause provisioning of     regional <code>ClusterDeployment</code> objects by removing annotation     from the <code>regions</code> object:</p> <pre><code>kubectl annotate regions &lt;region-name&gt; 'k0rdent.mirantis.com/region-pause-'\n</code></pre> </li> </ol> </li> </ol>"},{"location":"admin/backup/restore/#caveats","title":"Caveats","text":"<p>For some CAPI providers it is necessary to make changes to the <code>Restore</code> object due to the large number of different resources and logic in each provider. The resources described below are not excluded from a <code>ManagementBackup</code> by default to avoid logical dependencies on one or another provider, and to create a provider-agnostic system.</p> <p>Note</p> <p> The described caveats apply only to the <code>Restore</code> object creation step and do not affect the other steps.</p> <p>Note</p> <p> The below mentioned exclusions (<code>excludedResources</code>) are applicable to any of the <code>Restore</code> examples on this page, including those tailored for regional clusters.</p>"},{"location":"admin/backup/restore/#azure-capz","title":"Azure (CAPZ)","text":"<p>The following resources should be excluded from the <code>Restore</code> object:</p> <ul> <li><code>natgateways.network.azure.com</code></li> <li><code>resourcegroups.resources.azure.com</code></li> <li><code>virtualnetworks.network.azure.com</code></li> <li><code>virtualnetworkssubnets.network.azure.com</code></li> </ul> <p>Due to the webhook conversion, objects of these resources cannot be restored, and they will be created in the management cluster by the <code>CAPZ</code> provider automatically with the same <code>spec</code> as in the backup.</p> <p>The resulting <code>Restore</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  excludedResources:\n  - natgateways.network.azure.com\n  - resourcegroups.resources.azure.com\n  - virtualnetworks.network.azure.com\n  - virtualnetworkssubnets.network.azure.com\n  includedNamespaces:\n  - '*'\n</code></pre>"},{"location":"admin/backup/restore/#vsphere-capv","title":"vSphere (CAPV)","text":"<p>The following resources should be excluded from the <code>Restore</code> object:</p> <ul> <li><code>mutatingwebhookconfiguration.admissionregistration.k8s.io</code></li> <li><code>validatingwebhookconfiguration.admissionregistration.k8s.io</code></li> </ul> <p>Due to the Velero Restoration Order, some of the <code>CAPV</code> core objects cannot be restored, and they will not be recreated automatically. Because all of the objects have already passed both mutations and validations, there is not much sense in validating them again. The webhook configurations will be restored during installation of the <code>CAPV</code> provider.</p> <p>The resulting <code>Restore</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  excludedResources:\n  - mutatingwebhookconfiguration.admissionregistration.k8s.io\n  - validatingwebhookconfiguration.admissionregistration.k8s.io\n  includedNamespaces:\n  - '*'\n</code></pre>"},{"location":"admin/backup/scheduled-backups/","title":"Scheduled Management Backups","text":"<p>Backups should be run on a schedule consistent with the policy requirements of the environment. For example a production environment might be set for \"daily\" backups, while a testing environment is set for \"weekly\".</p>"},{"location":"admin/backup/scheduled-backups/#create-a-management-backup","title":"Create a Management Backup","text":"<p>Periodic backups are handled by a <code>ManagementBackup</code> object, which uses a Cron expression for its <code>.spec.schedule</code> field. If the <code>.spec.schedule</code> field is not set, a backup on demand will be created instead.</p> <p>Optionally, set the name of the <code>.spec.backup.storageLocation</code> of the <code>BackupStorageLocation</code> object. The default location is the <code>BackupStorageLocation</code> object with <code>.spec.default</code> set to <code>true</code>.</p> <p>Note</p> <p> For the regional cluster case, make sure to setup the same location on the regional cluster.</p> <p>For example, you can create a <code>ManagementBackup</code> object that backs up to the storage object created in the preparation step every 6 hours (ref: Kubernetes CronJob schedule syntax, \"Vixie cron\" step values).</p> <p>Start the scheduled backup process by creating the following object:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ManagementBackup\nmetadata:\n  name: kcm\nspec:\n  schedule: \"0 */6 * * *\"\n  storageLocation: aws-s3\nEOF\n</code></pre> <p>Confirm the backup creation was successful by navigating to the appropriate storage console UI or from the command line:</p> <pre><code>kubectl get managementbackups\n</code></pre> <p>The <code>managementbackup</code> should show as <code>Completed</code>:</p> <pre><code>NAME              LASTBACKUPSTATUS   NEXTBACKUP   AGE\nexample-backup    Completed                       8m  \n</code></pre>"},{"location":"admin/backup/upgrades-rollbacks/","title":"Upgrades and rollbacks","text":"<p>The Disaster Recovery Feature provides a way to create backups on each <code>kcm</code> upgrade automatically.</p>"},{"location":"admin/backup/upgrades-rollbacks/#automatic-management-backups","title":"Automatic Management Backups","text":"<p>Each <code>ManagementBackup</code> with a non-empty <code>.spec.schedule</code> field can enable the automatic creation of backups before upgrading to a new version.</p> <p>To enable, set the <code>.spec.performOnManagementUpgrade</code> to <code>true</code>.</p> <p>For example, a <code>ManagementBackup</code> object with enabled auto-backup before the <code>kcm</code> version upgrade looks like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ManagementBackup\nmetadata:\n  name: example-backup\nspec:\n  schedule: \"0 */6 * * *\"\n  performOnManagementUpgrade: true\n</code></pre> <p>After the enablement, before each upgrade of <code>kcm</code> to a new version, a new backup will be created.</p> <p>Automatically created backups have the following name template to make it easier to find them: the name of the <code>ManagementBackup</code> object with enabled <code>performOnManagementUpgrade</code> concatenates with the name of the release before the upgrade, for example, <code>example-backup-kcm-1-5-0</code>.</p> <p>Automatically created backups have the label <code>k0rdent.mirantis.com/release-backup</code> with the name of the release before the upgrade as its value to simplify querying if required.</p>"},{"location":"admin/backup/upgrades-rollbacks/#rollbacks","title":"Rollbacks","text":"<p>If during the <code>kcm</code> upgrade a failure happens, a rollback operation should be performed to restore the <code>kcm</code> to its before-the-upgrade state:</p> <ol> <li> <p>Follow the first 2 steps from the restoration section, creating a clean <code>kcm</code>    installation and <code>BackupStorageLocation</code>/<code>Secret</code>.</p> <p>Warning</p> <p> Please consider the restoration caveats section before proceeding.</p> </li> <li> <p>Create the <code>ConfigMap</code> object with patches to revert the <code>Management</code> <code>.spec.release</code>, substitute the <code>&lt;version-before-upgrade&gt;</code> with    the version of <code>kcm</code> before the upgrade, and create the <code>Restore</code> object,    propagating the <code>ConfigMap</code> to it:</p> <pre><code>---\napiVersion: v1\ndata:\n  patch-mgmt-spec-release: |\n    version: v1\n    resourceModifierRules:\n    - conditions:\n        groupResource: managements.k0rdent.mirantis.com\n      patches:\n      - operation: replace\n        path: \"/spec/release\"\n        value: \"&lt;version-before-upgrade&gt;\"\nkind: ConfigMap\nmetadata:\n  name: patch-mgmt-spec-release\n  namespace: kcm-system\n---\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  includedNamespaces:\n  - '*'\n  resourceModifier: # propagate patches\n    kind: ConfigMap\n    name: patch-mgmt-spec-release\n</code></pre> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> <li>Optionally delete the created <code>ConfigMap</code>.</li> </ol>"},{"location":"admin/backup/whats-included/","title":"What's Included in the Management Backup","text":"<p>The backup includes all of k0rdent <code>kcm</code> component resources, parts of the <code>cert-manager</code> components required for other components creation, and all the required resources of <code>CAPI</code> and the <code>ClusterDeployment</code>s currently in use in the management cluster.</p> <p>By default, objects satisfying these labels will be included in the backup:</p> <pre><code>cluster.x-k8s.io/cluster-name=\"&lt;cluster-deployment-name&gt;\"\nhelm.toolkit.fluxcd.io/name=\"&lt;cluster-deployment-name&gt;\"\n\ncluster.x-k8s.io/provider=\"bootstrap-&lt;provider&gt;\"\ncluster.x-k8s.io/provider=\"control-plane-&lt;provider&gt;\"\ncluster.x-k8s.io/provider=\"infrastructure-&lt;provider&gt;\"\n\ncluster.x-k8s.io/provider=\"cluster-api\"\n\ncontroller.cert-manager.io/fao=\"true\"\n\nk0rdent.mirantis.com/component=\"kcm\"\n</code></pre> <p>An example sorted set of labels, objects satisfying these labels will be included in the backup:</p> <pre><code>cluster.x-k8s.io/cluster-name=\"some-cluster-deployment-name\"\ncluster.x-k8s.io/provider=\"bootstrap-k0sproject-k0smotron\"\ncluster.x-k8s.io/provider=\"cluster-api\"\ncluster.x-k8s.io/provider=\"control-plane-k0sproject-k0smotron\"\ncluster.x-k8s.io/provider=\"infrastructure-aws\"\ncontroller.cert-manager.io/fao=\"true\"\nhelm.toolkit.fluxcd.io/name=\"some-cluster-deployment-name\"\nk0rdent.mirantis.com/component=\"kcm\"\n</code></pre>"},{"location":"admin/clusters/","title":"Working With Clusters","text":"<p>k0rdent enables you to create and manage Kubernetes child clusters on a variety of  infrastructure choices.  You can also \"adopt\" Kubernetes clusters created outside of k0rdent so that k0rdent can manage them. (You can also  \"self-adopt\" the management cluster so you can use k0rdent to manage itself.)</p> <ul> <li>Deploying standalone clusters</li> <li>Updating standalone clusters</li> <li>Adopting clusters</li> <li>IP Address Management (IPAM)</li> </ul>"},{"location":"admin/clusters/admin-adopting-clusters/","title":"Adopting an Existing Cluster","text":"<p>Creating a new cluster isn't the only way to use k0rdent. Adopting an existing Kubernetes cluster enables you to  bring it under k0rdent's management. This process is useful when you already have a running cluster but want  to centralize management and leverage k0rdent's capabilities, such as unified monitoring, configuration, and automation, but you don't want to redeploy your cluster.</p>"},{"location":"admin/clusters/admin-adopting-clusters/#adopting-a-cluster","title":"Adopting a Cluster","text":"<p>To adopt a cluster, k0rdent establishes communication between the management cluster (where kcm is installed)  and the target cluster. This requires proper credentials, network connectivity, and a standardized configuration. </p> <p>Follow these steps to adopt an existing cluster:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A kubeconfig file for the cluster you want to adopt (this file provides access credentials and configuration details    for the cluster).</li> <li>A management cluster with k0rdent installed and running. See the installation instructions    if you need to set it up.</li> <li>Network connectivity between the management cluster and the cluster to be adopted (for example, ensure firewall    rules and VPNs allow communication).</li> </ul> </li> <li> <p>Create a Credential</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure  provider. Follow the instructions in the Credential System, as well as the specific instructions  for your target infrastructure.</p> <p>Tip</p> <p>Double-check that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Configure the management cluster kubeconfig</p> <p>Set the <code>KUBECONFIG</code> environment variable to the path of your management cluster's kubeconfig file so you can  execute commands against the management cluster.</p> <p>For example:</p> <pre><code>export KUBECONFIG=/path/to/management-cluster-kubeconfig\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code> YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is used to define how k0rdent should manage the adopted cluster. Create a  YAML file for the <code>ClusterDeployment</code> object, as shown below:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;CLUSTER_NAME&gt;\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  template: adopted-cluster-&lt;VERSION&gt;\n  credential: &lt;CREDENTIAL_NAME&gt;\n  dryRun: &lt;BOOLEAN&gt;\n  config:\n    &lt;CONFIGURATION&gt;\n</code></pre> <p>Replace placeholders such as <code>&lt;CLUSTER_NAME&gt;</code>, <code>&lt;NAMESPACE&gt;</code>, <code>&lt;VERSION&gt;</code>, <code>&lt;CREDENTIAL_NAME&gt;</code>, and <code>&lt;CONFIGURATION&gt;</code> with actual values. The <code>dryRun</code> flag is useful for testing the configuration without making changes to the cluster. For more details, see the Dry Run section.</p> <p>You can also get a list of the available templates with:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-0-1           true\naws-eks-1-0-3                   true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\ndocker-hosted-cp-1-0-4          true\ngcp-gke-1-0-6                   true\ngcp-hosted-cp-1-0-16             true\ngcp-standalone-cp-1-0-15         true\nopenstack-standalone-cp-1-0-17   true\nremote-cluster-1-0-17            true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\n</code></pre></p> <p>Putting it all together, your YAML would look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster\n  namespace: kcm-system\nspec:\n  template: adopted-cluster-1-5-0\n  credential: my-cluster-credential\n  dryRun: false\n  config: {}\n</code></pre> </li> <li> <p>Apply the <code>ClusterDeployment</code> configuration</p> <p>Once your configuration file is ready, apply it to the management cluster using <code>kubectl</code>:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits the <code>ClusterDeployment</code> object to k0rdent, initiating the adoption process.</p> </li> <li> <p>Check the Status of the <code>ClusterDeployment</code> Object</p> <p>To ensure the adoption process is progressing as expected, check the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output includes the current state and any conditions (for example, errors or progress updates). Review  this information to confirm that the adoption is successful.</p> </li> </ol>"},{"location":"admin/clusters/admin-adopting-clusters/#whats-happening-behind-the-scenes","title":"What's Happening Behind the Scenes?","text":"<p>When you adopt a cluster, k0rdent performs several actions:</p> <ol> <li>It validates the credentials and configuration provided in the <code>ClusterDeployment</code> object.</li> <li>It ensures network connectivity between the management cluster and the adopted cluster.</li> <li>It registers the adopted cluster within the k0rdent system, enabling it to be monitored and managed like      any k0rdent-deployed cluster.</li> </ol> <p>This process doesn't change the adopted cluster's existing workloads or configurations. Instead, it enhances your  ability to manage the cluster through k0rdent.</p>"},{"location":"admin/clusters/admin-adopting-clusters/#self-adopting-the-management-cluster","title":"Self-Adopting the Management Cluster","text":"<p>k0rdent makes it easy to manage Kubernetes clusters, but it only manages child clusters represented by a <code>ClusterDeployment</code>. So in order for k0rdent to manage itself, you must adopt the management cluster. Fortunately, because you're using the target cluster's <code>kubeconfig</code>, this is pretty straightforward.</p> <p>For example, adopting a k0s-based management cluster might look like this:</p> <ol> <li> <p>Get the IP address of the control plane:</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code>NAME             STATUS   ROLES           AGE   VERSION       INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME\nip-172-31-8-77   Ready    control-plane   9d    v1.33.2+k0s   172.31.8.77   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-1029-aws   containerd://1.7.27\n</code></pre></p> <p>Because the cluster will be accessing itself, the <code>INTERNAL-IP</code> (in this example, <code>172.21.8.77</code>), is sufficient.</p> </li> <li> <p>Edit the <code>kubeconfig</code>:</p> <p>Make sure that the <code>kubeconfig</code> file references the IP address, rather than <code>localhost</code>.  You can do this by editing the file directly:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CR...tLQo=\n    server: https://172.21.8.77:6443\n  name: local\ncontexts:\n...\n</code></pre> </li> <li> <p>Get the base64-encoded <code>kubeconfig</code>:</p> <p><pre><code>base64 /path/to/kubeconfig\n</code></pre> <pre><code>YXBpVmVyc2lvbjogdjEKY2x1c3RlcnM6Ci0gY2x1c3RlcjoKICAgIGNlcnRpZmljYXRlLWF1dGhv\ncml0eS1kYXRhOiBMUzB0TFMxQ1JVZEpUaUJEUlZKVVNVWkpRMEZVUlMwdExTMHRDazFKU1VSQlJF\n...\ndFZjVlphZVVWWlUyMDVlRFF4UVVoMlpYSnhWVGxvQ2kwdExTMHRSVTVFSUZKVFFTQlFVa2xXUVZS\nRklFdEZXUzB0TFMwdENnPT0K\n</code></pre></p> </li> <li> <p>Create the <code>Credential</code> to access the cluster:</p> <p>Create a file with the <code>Secret</code> and <code>Credential</code> objects, such as <code>adopt-creds.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: self-adopt-cluster-kubeconfig\n  namespace: kcm-system\ntype: Opaque\ndata:\n  value: &lt;BASE64_KUBECONFIG&gt;\n---\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: self-adopt-cluster-credential\n  namespace: kcm-system\nspec:\n  description: \"Credential For Self Adoption of Management Cluster\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: self-adopt-cluster-kubeconfig\n    namespace: kcm-system\n</code></pre> <p>Make sure to remove the line feeds from the encoded <code>kubeconfig</code>.</p> </li> <li> <p>Add the credential objects:</p> <p><pre><code>kubectl apply -f adopt-creds.yaml\n</code></pre> <pre><code>secret/self-adopt-cluster-kubeconfig created\ncredential.k0rdent.mirantis.com/self-adopt-cluster-credential created\n</code></pre></p> </li> <li> <p>Define the <code>ClusterDeployment</code>:</p> <p>First determine the <code>ClusterTemplate</code>:</p> <p><pre><code>kubectl get ClusterTemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-2-0           true\nadopted-cluster-1-0-0           true\nadopted-cluster-1-0-1           true\naws-eks-0-2-0                   true\n...\n</code></pre></p> <p>Create the definition file, such as <code>self-adopt-cluster.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: self-adopted-mgmt\n  namespace: kcm-system\nspec:\n  template: adopted-cluster-1-0-1\n  credential: self-adopt-cluster-credential\n  dryRun: False\n  config: {}\n</code></pre> </li> <li> <p>Add the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl apply -f self-adopt-cluster.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/self-adopted-mgmt created\n</code></pre></p> </li> <li> <p>Verify the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployment -A\n</code></pre> <pre><code>NAMESPACE    NAME                READY   SERVICES   TEMPLATE                MESSAGES          AGE\nkcm-system   self-adopted-mgmt   True    0/0        adopted-cluster-1-0-1   Object is ready   14s\n</code></pre></p> </li> </ol> <p>Now you can manage the k0rdent management cluster just as you'd manage  any other child cluster.</p>"},{"location":"admin/clusters/admin-adopting-clusters/#additional-tips","title":"Additional Tips","text":"<ul> <li>If you encounter issues, double-check that kubeconfig file you used for the adopted cluster is valid    and matches the cluster you're trying to adopt.</li> <li>Use the <code>dryRun</code> option during the first attempt to validate the configuration without making actual changes.</li> </ul>"},{"location":"admin/clusters/cluster-ipam/","title":"IP Address Management (IPAM)","text":"<p><code>k0rdent</code> provides a flexible IP Address Management (IPAM) system that enables deterministic allocation of IP addresses throughout the cluster lifecycle.</p> <p>Warning</p> <p>Keep in mind the following about IPAM support:</p> <ul> <li>At the moment only node network is supported.</li> <li>IPAM is currently unsupported on ARM64 architectures.</li> <li>IPAM has only been tested on VMware VSphere. Support for other providers will be added in the future.</li> </ul> <p>With IPAM enabled, IP addresses can be assigned to both worker and control plane nodes.</p> <p>Administrators can define address ranges using either CIDR blocks or explicit IP lists, enabling:</p> <ul> <li>Predictable, conflict-free assignments</li> <li>Seamless integration with existing network topologies</li> <li>Fine-grained control in multi-tenant or segmented environments</li> </ul>"},{"location":"admin/clusters/cluster-ipam/#deploying-a-cluster-with-ipam","title":"Deploying a Cluster with IPAM","text":"<p>Follow these instructions to configure IPAM for your cluster deployment.</p>"},{"location":"admin/clusters/cluster-ipam/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following before configuring IPAM:</p> <ul> <li>A valid, unused IP space is available (CIDR or static IP list).</li> <li>The reserved space must accommodate:<ul> <li>One IP per control plane node</li> <li>One IP per worker node</li> </ul> </li> <li>A valid network gateway IP</li> </ul>"},{"location":"admin/clusters/cluster-ipam/#define-ipam-configuration","title":"Define IPAM configuration","text":"<p>There are two options for configuring IPAM in k0rdent</p>"},{"location":"admin/clusters/cluster-ipam/#option-1-use-mutual-references-in-clusterdeployment-and-clusteripamclaim","title":"Option 1: Use mutual references in <code>ClusterDeployment</code> and <code>ClusterIPAMClaim</code>","text":"<p>To use mutual references, follow these steps:</p> <ol> <li> <p>Define a <code>ClusterIPAMClaim</code></p> <p>The <code>ClusterIPAMClaim</code> resource reserves the required IP address space for the cluster. The node network segment can be defined using either a <code>cidr</code> or a static list of <code>ipAddresses</code>.</p> <p>Note</p> <p> The value for <code>provider</code> must be <code>in-cluster</code> or <code>ipam-infoblox</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterIPAMClaim\nmetadata:\n  name: &lt;claim-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  provider: &lt;provider-name&gt;\n  nodeNetwork:\n    cidr: &lt;cidr&gt;\n    # ipAddresses:\n    # - &lt;ip-1&gt;\n    # - &lt;ip-2&gt;\n    gateway: &lt;network gateway&gt;\n    prefix: &lt;network prefix&gt;\n  cluster: &lt;cluster-name&gt;\n</code></pre> <ul> <li>The <code>cluster</code> field in <code>ClusterIPAMClaim</code> is immutable once set.</li> <li>The <code>cluster</code> field links the claim to a specific <code>ClusterDeployment</code>, ensuring IPs are reserved before provisioning begins.</li> </ul> </li> <li> <p>Apply the <code>ClusterIPAMClaim</code></p> <p>To create the claim:</p> <pre><code>kubectl apply -f &lt;cluster-ipam-claim-file&gt;.yaml\n</code></pre> <p>To verify the claim:</p> <p><pre><code>kubectl get clusteripamclaim &lt;claim-name&gt; -n &lt;namespace&gt;\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterIPAMClaim\nmetadata:\n  name: &lt;claim-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  cluster: &lt;cluster-name&gt;\n  clusterIPAMRef: &lt;claim-name&gt;\n  nodeNetwork:\n    cidr: &lt;cidr&gt;\n    # ipAddresses:\n    # - &lt;ip-1&gt;\n    # - &lt;ip-2&gt;\n    gateway: &lt;network gateway&gt;\n    prefix: &lt;network prefix&gt;\n  provider: &lt;provider-name&gt;\nstatus:\n  bound: true\n</code></pre></p> <ul> <li><code>.spec.clusterIPAMRef</code>: If this field is set, it indicates that the child <code>ClusterIPAM</code> object was successfully created.</li> <li><code>.status.bound</code>: If <code>true</code>, it means the child <code>ClusterIPAM</code> was successfully reconciled and the defined addresses were allocated.</li> </ul> </li> <li> <p>Define a <code>ClusterDeployment</code></p> <p>Finally, define the <code>ClusterDeployment</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;provider-credential-name&gt;\n  dryRun: &lt;\"true\" | \"false\"&gt;  # Optional; defaults to \"false\"\n  config:\n    &lt;cluster-configuration&gt;\n  ipamClaim:\n    ref: &lt;claim-name&gt;\n</code></pre> </li> </ol>"},{"location":"admin/clusters/cluster-ipam/#option-2-use-inline-ipam-configuration-in-clusterdeployment","title":"Option 2: Use inline IPAM configuration in <code>ClusterDeployment</code>","text":"<p>The IPAM configuration can also be defined inline within the <code>ClusterDeployment</code> resource as follows:</p> <ol> <li> <p>Define a <code>ClusterDeployment</code></p> <p>First, define the <code>ClusterDeployment</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;provider-credential-name&gt;\n  dryRun: &lt;\"true\" | \"false\"&gt;  # Optional; defaults to \"false\"\n  config:\n    &lt;cluster-configuration&gt;\n  ipamClaim:\n    spec:\n      provider: &lt;provider-name&gt;\n      nodeNetwork:\n        cidr: &lt;cidr&gt;\n        # ipAddresses:\n        # - &lt;ip-1&gt;\n        # - &lt;ip-2&gt;\n        gateway: &lt;network gateway&gt;\n        prefix: &lt;network prefix&gt;\n</code></pre> </li> <li> <p>Apply the <code>ClusterDeployment</code>:</p> <pre><code>kubectl apply -f &lt;cluster-deployment-file&gt;.yaml\n</code></pre> </li> <li> <p>Verify IPAM</p> <p>The specified IPAM settings will be used to allocate IP addresses during provisioning. Keep in mind that cluster provisioning will not proceed until IPAM resources are ready and addresses are allocated.</p> <p>To inspect the resulting <code>ClusterIPAM</code> resource:</p> <p><pre><code>kubectl get -n &lt;namespace&gt; ClusterIPAM &lt;claim-name&gt;\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterIPAM\nmetadata:\n  name: &lt;cluster-ipam-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  provider: &lt;provider-name&gt;\n  clusterIPAMClaimRefs: &lt;cluster-ipam-claim-name&gt;\nstatus:\n  phase: Bound\n  providerData:\n    &lt;provider data&gt;\n</code></pre></p> </li> </ol>"},{"location":"admin/clusters/deploy-cluster/","title":"Deploying a Cluster","text":"<p>k0rdent is designed to simplify the process of deploying and managing Kubernetes clusters across various cloud platforms. It does this through the use of <code>ClusterDeployment</code> objects, which include all of the information k0rdent needs to know in order to create the cluster you're looking for. This <code>ClusterDeployment</code> system relies on predefined templates and credentials.</p> <p>A cluster deployment typically involves:</p> <ol> <li>Setting up credentials for the infrastructure provider (for example, AWS, vSphere).</li> <li>Choosing a template that defines the desired cluster configuration (for example, number of nodes, instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster tailored to your specific needs:</p> <ol> <li> <p>Create the <code>Credential</code> object</p> <p>Credentials are essential for k0rdent to communicate with the infrastructure provider (for example, AWS, Azure, vSphere). These credentials enable k0rdent to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created ahead of time and made available to users, so before you look into creating a new one be sure what you're looking for doesn't already exist. You can see all of the existing <code>Credential</code> objects by querying the management cluster:</p> <pre><code>kubectl get credentials --all-namespaces\n</code></pre> <p>If the <code>Credential</code> you need doesn't yet exist, go ahead and create it.</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure provider. Follow the instructions in the chapter about credential management, as well as the specific instructions for your target infrastructure.</p> <p>Note</p> <p> A <code>Credential</code> may optionally specify the <code>spec.region</code> field. When set, all <code>ClusterDeployment</code> objects that reference this <code>Credential</code> will be deployed to the corresponding regional cluster. Learn more in Creating a Credential in a Region.</p> <p>Tip</p> <p>Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in k0rdent are predefined configurations that describe how to set up the cluster. Templates include details such as:</p> <ul> <li>The number and type of control plane and worker nodes</li> <li>Networking settings</li> <li>Regional deployment preferences</li> </ul> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-0-1           true\naws-eks-1-0-3                   true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\ndocker-hosted-cp-1-0-4          true\ngcp-gke-1-0-6                   true\ngcp-hosted-cp-1-0-16             true\ngcp-standalone-cp-1-0-15         true\nopenstack-standalone-cp-1-0-17   true\nremote-cluster-1-0-17            true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\n</code></pre> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-1-0-16 -n kcm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is the main configuration file that defines your cluster's specifications. It includes:</p> <ul> <li>The template to use</li> <li>The credentials for the infrastructure provider</li> <li>Optional customizations such as instance types, regions, and networking</li> </ul> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  cleanupOnDeletion: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run.) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-16\n  credential: aws-credential\n  dryRun: false\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n      rootVolumeSize: 32\n    worker:\n      instanceType: t3.small\n      rootVolumeSize: 32\n</code></pre> <p>Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> <p>Tip</p> <p> If automatic cleanup of potentially orphaned LoadBalancer Services and Storage devices during deletion of the <code>ClusterDeployment</code> object is required, set <code>.spec.cleanupOnDeletion</code> to <code>true</code>. This is a best-effort cleanup: if there is no possibility to acquire a managed cluster's kubeconfig, the cleanup will not happen.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the k0rdent management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to k0rdent. If you've set <code>dryRun</code> to <code>true</code> you can observe what would happen. Otherwise, k0rdent will go ahead and begin provisioning the necessary infrastructure.</p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>k0rdent will now start provisioning resources (for example, VMs and networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>Tip</p> <p>For a detailed view of the provisioning process, use the <code>clusterctl describe</code> command (note that this requires the <code>clusterctl</code> CLI):</p> <pre><code>clusterctl describe cluster &lt;cluster-name&gt; -n &lt;namespace&gt; --show-conditions all\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, retrieve the kubeconfig file for the new cluster. This file enables you to interact with the cluster using <code>kubectl</code>:</p> <pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> <p>You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"admin/clusters/migration/","title":"Migrate <code>ClusterDeployment</code> from one management cluster to another","text":"<p>This guide provides instructions for migrating a <code>ClusterDeployment</code> resource from one k0rdent management cluster to another. This process uses Velero for backup and restore operations between the clusters.</p>"},{"location":"admin/clusters/migration/#prerequisites","title":"Prerequisites","text":"<p>Before beginning the migration process, ensure you have:</p> <ol> <li>Access to both source and target management clusters with admin permissions.</li> <li><code>kubectl</code> configured to access both clusters.</li> <li>Velero properly installed and configured on both clusters (see Preparing for Backups).</li> </ol>"},{"location":"admin/clusters/migration/#prepare-your-clusters","title":"Prepare your clusters","text":"<ol> <li>Source Cluster: Prepare your source cluster for backups by following the instructions    in Preparing for Backups.</li> <li>Target Cluster: Similarly, prepare your target cluster with the same Velero    configuration using the same backup storage location.</li> </ol>"},{"location":"admin/clusters/migration/#migration-steps","title":"Migration Steps","text":""},{"location":"admin/clusters/migration/#1-backup-the-clusterdeployment-from-the-source-cluster","title":"1. Backup the ClusterDeployment from the source cluster","text":"<pre><code># Switch context to source cluster\nkubectl config use-context &lt;source-cluster-context&gt;\n# Or set the source config\nexport KUBECONFIG=\"&lt;path-to-source-cluster-kubeconfig&gt;\"\n\n# Create a backup\nCLD=\"&lt;cluster-deployment-name&gt;\"\nBSL=\"&lt;storage-location-name&gt;\"\nTPL=\"$(kubectl -n &lt;cld-namespace&gt; get clusterdeployments \"$CLD\" -o go-template='{{.spec.template}}')\"\n\n{\n  cat &lt;&lt;EOF\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: cld-migration\n  namespace: kcm-system\nspec:\n  storageLocation: $BSL\n  includedNamespaces:\n  - '*'\n  orLabelSelectors:\n  - matchLabels:\n      k0rdent.mirantis.com/component: kcm\n  - matchLabels:\n      cluster.x-k8s.io/provider: cluster-api\n  - matchLabels:\n      controller.cert-manager.io/fao: \"true\"\n  - matchLabels:\n      helm.toolkit.fluxcd.io/name: $CLD\n  - matchLabels:\n      cluster.x-k8s.io/cluster-name: $CLD\nEOF\n\n  kubectl -n &lt;cluster-template-namespace&gt; get clustertemplates \"$TPL\" \\\n    -o go-template='{{range .status.providers}}{{if ne . \"cluster-api\"}}{{printf \"  - matchLabels:\\n      cluster.x-k8s.io/provider: %s\\n\" .}}{{end}}{{end}}'\n} | kubectl create -f -\n\n\n# Verify backup completion\nkubectl -n kcm-system wait backups.velero.io cld-migration \\\n    --for=jsonpath='{.status.phase}'='Completed' \\\n    --timeout 10m\n</code></pre>"},{"location":"admin/clusters/migration/#2-prepare-the-target-management-cluster","title":"2. Prepare the target Management cluster","text":"<pre><code># Switch context to target cluster\nkubectl config use-context &lt;target-cluster-context&gt;\n# Or set the target config\nexport KUBECONFIG=\"&lt;path-to-target-cluster-kubeconfig&gt;\"\n\n# Verify Velero and BackupStorageLocation are properly configured\nkubectl -n kcm-system get backupstoragelocation\n</code></pre>"},{"location":"admin/clusters/migration/#3-restore-the-clusterdeployment-to-the-target-cluster","title":"3. Restore the ClusterDeployment to the target cluster","text":"<pre><code># Disable admission validating webhook\nkubectl patch managements kcm --type=merge \\\n    --patch='{\"spec\":{\"core\":{\"kcm\":{\"config\":{\"admissionWebhook\":{\"enabled\": false}}}}}}'\n\n# Wait the management cluster to be reconfigured\nkubectl wait management kcm --for=condition=Ready=True --timeout 10m\n\n# Verify the backups synced from the backup storage location\nkubectl -n kcm-system get backups.velero.io\n\n# Create a restore operation on the target cluster\nkubectl create -f - &lt;&lt;EOF\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: restore-cld-migration\n  namespace: kcm-system\nspec:\n  backupName: cld-migration\n  includedNamespaces:\n  - '*'\n  restoreStatus:\n    includedResources:\n    - '*'\n  excludedResources:\n  - mutatingwebhookconfiguration.admissionregistration.k8s.io\n  - validatingwebhookconfiguration.admissionregistration.k8s.io\nEOF\n\n# Wait for the restore process successful completion\nkubectl -n kcm-system wait restores.velero.io restore-cld-migration \\\n    --for=jsonpath='{.status.phase}'='Completed' \\\n    --timeout 10m\n\n# Enable admission validating webhook back (if it was enabled initially)\nkubectl patch managements kcm --type=merge \\\n    --patch='{\"spec\":{\"core\":{\"kcm\":{\"config\":{\"admissionWebhook\":{\"enabled\": true}}}}}}'\n</code></pre>"},{"location":"admin/clusters/migration/#4-verify-the-migration","title":"4. Verify the migration","text":"<pre><code># Verify the ClusterDeployment exists on the target cluster\nkubectl -n &lt;cld-namespace&gt; get clusterdeployments &lt;cluster-deployment-name&gt;\n\n# Check cluster status\nkubectl -n &lt;cld-namespace&gt; describe clusterdeployments &lt;cluster-deployment-name&gt;\n</code></pre>"},{"location":"admin/clusters/migration/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Backup fails: Ensure Velero is properly configured and has access to     the storage location. Check Velero logs for specific errors.</li> <li>Restore fails: Verify that the target cluster can access the backup     storage location and has the necessary permissions.</li> <li>Resource conflicts: If resources already exist in the target cluster,     you may need to delete them first or use the     <code>existingResourcePolicy: update</code> option in your <code>Restore</code>.</li> </ul> <p>For more detailed information on using Velero for backups and restores, refer to the official Velero documentation.</p>"},{"location":"admin/clusters/update-cluster/","title":"Updating a Single Standalone Cluster","text":"<p>k0rdent <code>ClusterTemplate</code> objects are immutable, so the only way to change a <code>ClusterDeployment</code> is to change the template that forms its basis. </p> <p>To update the <code>ClusterDeployment</code>, modify the <code>.spec.template</code> field to use the name of the new <code>ClusterTemplate</code>.  This enables you to apply changes to the cluster configuration. These changes will then be applied to the actual  cluster. For example, if the cluster currently uses <code>t2.large</code> instances, that will be specified in its current template.  To change the cluster to use <code>t2.xlarge</code> instances, you would simply apply a template that references that new size;  k0rdent will then realize the cluster is out of sync and will attempt to remedy the situation by updating the cluster.</p> <p>Follow these steps to update the <code>ClusterDeployment</code>:</p> <ol> <li> <p>Patch the <code>ClusterDeployment</code> with the new template</p> <p>Run the following command, replacing the placeholders with the appropriate values:</p> <pre><code>kubectl patch clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; --patch '{\"spec\":{\"template\":\"&lt;new-template-name&gt;\"}}' --type=merge\n</code></pre> </li> <li> <p>Check the status of the <code>ClusterDeployment</code></p> <p>After applying the patch, verify the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt;\n</code></pre> </li> <li> <p>Inspect the detailed status</p> <p>For more details, use the <code>-o=yaml</code> option to check the <code>.status.conditions</code> field:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; -o=yaml\n</code></pre> </li> </ol> <p>Note that not all updates are possible; <code>ClusterTemplateChain</code> objects limit what templates can be applied.  Consider, for example, this <code>ClusterTemplateChain</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws-standalone-cp-1-0-16\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-2\n      availableUpgrades:\n        - name: aws-standalone-cp-1-0-16\n    - name: aws-standalone-cp-1-0-16\n</code></pre> <p>As you can see from the <code>.spec</code>, the <code>aws-standalone-cp-1-0-16</code> template can be applied to a cluster that also uses the <code>aws-standalone-cp-1-0-16</code> template, or it can be used as an upgrade from a cluster that uses <code>aws-standalone-cp-0.0.2</code>. You wouldn't be able to use this template to update a cluster that uses any other <code>ClusterTemplate</code>.</p> <p>Similarly, the <code>AccessManagement</code> object must have properly configured <code>spec.accessRules</code> with a list of allowed  <code>ClusterTemplateChain</code> object names and their namespaces. For more information, see Template Life Cycle Management.</p> <p>Note</p> <p>Support for displaying all available Cluster Templates for updates in the <code>ClusterDeployment</code> status is planned.</p>"},{"location":"admin/hosted-control-plane/","title":"Deploying a Hosted Control Plane","text":"<p>Under normal circumstances, k0rdent deploys a child cluster as a complete, self-contained unit. That is, the controllers and workers are all part of the <code>ClusterDeployment</code>. There are serious advantages to deploying a cluster this way, not the least of which is the fact that the cluster is essentially independent of the Management cluster, in that if the Management cluster becomes inaccessible for any reason the child cluster can continue on as though nothing has happened. This is known as a \"standalone\" deployment.</p> <p>On the other hand, for many Kubernetes clusters, the controllers are either too busy -- that is, they are constantly being scaled up and down -- or they're not busy enough -- that is, they're taking up server resources that aren't being fully used.</p> <p>For either of these cases, a better choice may be a \"hosted control plane\". </p> <p>A hosted control plane is a Kubernetes setup in which the control plane components (such as the API server,  etcd, and controllers) run as pods inside the management cluster instead of separate controller nodes. This  architecture centralizes control plane management and improves scalability by sharing resources in the management cluster. Need more controllers? Spin up another pod. Need fewer controllers? Remove some pods.</p> <p>It's important to remember that if a child cluster's control plane goes down, workloads will continue on indefinitely, but you won't be able to manage the cluster. So be sure to back up your management cluster after deploying important child clusters!</p> <p>Hosted control planes are managed by k0smotron, which makes it possible for Kubernetes controller nodes and worker nodes to reside not only in different clusters, but even in different clouds.</p> <p>Instructions for setting up a hosted control plane vary slighting depending on the provider.</p> <ul> <li>AWS</li> <li>Azure</li> <li>OpenStack</li> <li>VMware</li> <li>GCP</li> </ul>"},{"location":"admin/hosted-control-plane/hcp-aws/","title":"AWS Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on AWS:</p> <ol> <li> <p>Prerequisites</p> <p>Before proceeding, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28 or later) deployed on AWS with k0rdent installed.</li> <li>A default storage class configured on the management cluster to support Persistent Volumes.</li> <li>The VPC ID where the worker nodes will be deployed.</li> <li>The Subnet ID and Availability Zone (AZ) for the worker nodes.</li> <li>The AMI ID for the worker nodes (Amazon Machine Image ID for the desired OS and Kubernetes version).</li> </ul> <p>Important</p> <p>All control plane components for your hosted cluster will reside in the management cluster, and the management cluster must have sufficient resources to handle these additional workloads.</p> </li> <li> <p>Networking</p> <p>To deploy a hosted control plane, the necessary AWS networking resources must already exist or be created. If you're using the same VPC and subnets as your management cluster, you can reuse these resources.</p> <p>If your management cluster was deployed using the Cluster API Provider AWS (CAPA), you can gather the required networking details using the following commands:</p> <p>Retrieve the VPC ID:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre> <p>Retrieve Subnet ID:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre> <p>Retrieve Availability Zone:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre> <p>Retrieve Security Group:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre> <p>Retrieve AMI ID:</p> <pre><code>kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre> <p>Tip</p> <p>If you want to use different VPCs or regions for your management and hosted clusters, you\u2019ll need to configure additional networking, such as VPC peering, to allow communication between them.</p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>Once you've collected all the necessary data, you can create the <code>ClusterDeployment</code> manifest. This file tells k0rdent how to deploy and manage the hosted control plane. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp-1-0-16\n  credential: aws-credential\n  config:\n    managementClusterName: aws\n    clusterLabels: {}\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: true\n        natGatewayID: xxxxxx\n        routeTableId: xxxxxx\n      - id: subnet-1aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: false\n        routeTableId: xxxxxx\n    instanceType: t3.medium\n    rootVolumeSize: 32\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note</p> <p>The example above uses the <code>us-west-1</code> region, but you should use the region of your VPC.</p> <p>Alternatively, you can generate the <code>ClusterDeployment</code> manifest.</p> <p>If the management cluster you prepared in the first step was deployed using k0rdent or the CAPI AWS provider, you can simplify the creation of a <code>ClusterDeployment</code> manifest, and use the following template, which dynamically inserts the appropriate values:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp-1-0-16\n  credential: aws-credential\n  config:\n    managementClusterName: \"{{.metadata.name}}\"\n    clusterLabels: {}\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n    {{- range $subnet := .spec.network.subnets }}\n      - id: \"{{ $subnet.resourceID }}\"\n        availabilityZone: \"{{ $subnet.availabilityZone }}\"\n        isPublic: {{ $subnet.isPublic }}\n        {{- if $subnet.isPublic }}\n        natGatewayId: \"{{ $subnet.natGatewayId }}\"\n        {{- end }}\n        routeTableId: \"{{ $subnet.routeTableId }}\"\n    {{- end }}\n    instanceType: t3.medium\n    rootVolumeSize: 32\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre> <p>Save this template as <code>clusterdeployment.yaml.tpl</code>, then generate your manifest using the following command:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre> <p>Or if the management cluster is deployed on EKS, use the following command:</p> <pre><code>kubectl get awsmanagedcontrolplane &lt;cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre> <p>Note</p> <p> For EKS management clusters, update the <code>spec.config.managementClusterName</code> parameter as described in Setting the managementClusterName parameter.</p> </li> <li> <p>Apply the <code>ClusterTemplate</code></p> <p>Nothing actually happens until you apply the <code>ClusterDeployment</code> manifest to create a new cluster deployment:</p> <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-aws/#deployment-tips","title":"Deployment Tips","text":"<p>Here are some additional tips to help with deployment:</p> <ol> <li> <p>Controller and Template Availability:</p> <p>Make sure the KCM controller image and templates are available in a public or accessible repository.</p> </li> <li> <p>Install Charts and Templates:</p> <p>If you're using a custom repository, run the following commands with the appropriate <code>kubeconfig</code>:</p> <pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/k0rdent/kcm/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/k0rdent/kcm/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> </li> <li> <p>Mark the Infrastructure as Ready:</p> <p>To scale up the <code>MachineDeployment</code>, manually mark the infrastructure as ready:</p> <pre><code>kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}' -n kcm-system\n</code></pre> <p>For more details on why this is necessary, click here.</p> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-azure/","title":"Azure Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on Azure:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on Azure with k0rdent installed.</li> <li>A default storage class configured   on the management cluster to support Persistent Volumes.</li> </ul> <p>Note</p> <p>All control plane components for managed clusters will run in the management cluster. Make sure the management cluster   has sufficient CPU, memory, and storage to handle the additional workload.</p> </li> <li> <p>Gather Pre-existing Resources</p> <p>In a hosted control plane setup, some Azure resources must exist before deployment and must be explicitly provided in the <code>ClusterDeployment</code> configuration. These resources can also be reused by the management cluster.</p> <p>If you deployed your Azure Kubernetes cluster using the Cluster API Provider for Azure (CAPZ), you can retrieve the required information using the following commands:</p> <p>Location:</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre> <p>Subscription ID:</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre> <p>Resource Group:</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre> <p>VNet Name:</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre> <p>Subnet Name:</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre> <p>Route Table Name:</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre> <p>Security Group Name:</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>After collecting the required data, create a <code>ClusterDeployment</code> manifest to configure the hosted control plane. It should look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-1-0-19\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"westus\"\n    subscriptionID: ceb131c7-a917-439f-8e19-cd59fe247e03\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> <p>Alternatively, you can generate the <code>ClusterDeployment</code> manifest.</p> <p>If the management cluster you prepared in the first step was deployed using k0rdent or the CAPI AWS provider, you can simplify the creation of a <code>ClusterDeployment</code> manifest, and use the following template, which dynamically inserts the appropriate values:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-1-0-19\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> <p>Save this YAML as <code>clusterdeployment.yaml.tpl</code> and render the manifest with the following command:</p> <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code></p> <p>To actually create the cluster, apply the <code>ClusterDeployment</code> manifest to the management cluster, as in:</p> <pre><code>kubectl apply clusterdeployment.yaml -n kcm-system\n</code></pre> </li> <li> <p>Manually update the <code>AzureCluster</code> object</p> <p>Due to a limitation in k0smotron, (see k0sproject/k0smotron#668), after applying the <code>ClusterDeployment</code> manifest, you must manually update the status of the <code>AzureCluster</code> object.</p> <p>Use the following command to set the <code>AzureCluster</code> object status to <code>Ready</code>:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}'\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-azure/#important-notes-on-cluster-deletion","title":"Important Notes on Cluster Deletion","text":"<p>Due to these same k0smotron limitations, you must take some manual steps in order to delete a cluster properly:</p> <ol> <li> <p>Add a Custom Finalizer to the AzureCluster Object:</p> <p>To prevent the <code>AzureCluster</code> object from being deleted too early, add a custom finalizer:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": [\"manual\"]}}'\n</code></pre> </li> <li> <p>Delete the ClusterDeployment:</p> <p>After adding the finalizer, delete the <code>ClusterDeployment</code> object as usual. Confirm that all <code>AzureMachines</code> objects have been deleted successfully.</p> </li> <li> <p>Remove Finalizers from Orphaned AzureMachines:</p> <p>If any <code>AzureMachines</code> are left orphaned, delete their finalizers manually after confirming no VMs remain in Azure. Use this command to remove the finalizer:</p> <pre><code>kubectl patch azuremachine &lt;machine-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": []}}'\n</code></pre> </li> <li> <p>Allowing Updates to Orphaned Objects:</p> <p>If Azure admission controls prevent updates to orphaned objects, you must disable the associated <code>MutatingWebhookConfiguration</code> by deleting it:</p> <pre><code>kubectl delete mutatingwebhookconfiguration &lt;webhook-name&gt;\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-gcp/","title":"GCP Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on Google Cloud:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on GCP with k0rdent installed.</li> <li>A GCP Cloud Controller Manager installed to manage Services with the <code>Load Balancer</code> type.</li> <li>A default storage class configured   on the management cluster to support Persistent Volumes.</li> </ul> <p>Note</p> <p>All control plane components for managed clusters will run in the management cluster. Make sure the management cluster has sufficient CPU, memory, and storage to handle the additional workload.</p> </li> <li> <p>Create the <code>ClusterDeployment</code> manifest</p> <p>The <code>ClusterDeployment</code> manifest for a GCP-hosted control plane is similar to those for standalone control plane deployments. For a detailed list of parameters, refer to the Template Reference Guide, but here is an example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: gcp-hosted-cp\nspec:\n  template: gcp-hosted-cp-1-0-16\n  credential: gcp-credential\n  config:\n    project: \"PROJECT_NAME\"\n    region: \"us-east4\"\n    network:\n      name: default # Select your desired network name (select new network name to create or find it via `gcloud compute networks list --format=\"value(name)\"`)\n    worker:\n      instanceType: n1-standard-2 # Select your desired instance type (find it via `gcloud compute machine-types list | grep REGION`)\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213 # Select image (find it via `gcloud compute images list --uri`)\n</code></pre> <p>Apply the <code>ClusterDeployment</code> manifest to the management cluster, as in:</p> <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-openstack/","title":"OpenStack Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on OpenStack:</p> <ol> <li> <p>Prerequisites</p> <p>Before proceeding, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28 or later) deployed on OpenStack with k0rdent installed.</li> <li>An OpenStack Cloud Controller Manager installed to manage Services with the <code>Load Balancer</code> type.</li> <li>A default storage class configured on the management cluster to support Persistent Volumes.</li> <li>An OpenStack Credential object and resource-template ConfigMap must exist in the namespace where the cluster   will be deployed. Refer to steps 4\u20136 in the OpenStack management cluster preparation guide.</li> <li>The name of the existing network and subnet where the worker nodes will be deployed.</li> <li>The name of the existing router that will be used to deploy the worker nodes.</li> <li>(Optional) The name of the existing security group to attach to the worker nodes.</li> </ul> <p>Important</p> <p>All control plane components for your hosted cluster will reside in the management cluster, and the management cluster must have sufficient resources to handle these additional workloads.</p> </li> <li> <p>Networking</p> <p>To deploy a hosted control plane, the necessary OpenStack networking resources must already exist or be created. If you're using the same network, subnet and router as your management cluster, you can reuse these resources.</p> <p>If your management cluster was deployed using the Cluster API Provider OpenStack (CAPO), you can gather the required networking details using the following commands:</p> <p>Retrieve the network name:</p> <pre><code>kubectl -n &lt;cluster-namespace&gt; get openstackcluster &lt;cluster-name&gt; -o go-template='{{.status.network.name}}'\n</code></pre> <p>Retrieve the subnet name:</p> <pre><code>kubectl -n &lt;cluster-namespace&gt; get openstackcluster &lt;cluster-name&gt; -o go-template='{{(index .status.network.subnets 0).name}}'\n</code></pre> <p>Retrieve the router name:</p> <pre><code>kubectl -n &lt;cluster-namespace&gt; get openstackcluster &lt;cluster-name&gt; -o go-template='{{.status.router.name}}'\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code> manifest</p> <p>Once you've collected all the necessary data, you can create the <code>ClusterDeployment</code> manifest. This file tells k0rdent how to deploy and manage the hosted control plane. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: openstack-hosted-cp\nspec:\n  template: openstack-hosted-cp-1-0-7\n  credential: openstack-credential\nconfig:\n  workersNumber: 2\n  flavor: m1.medium\n  image:\n    filter:\n      name: ubuntu-20.04\n  externalNetwork:\n    filter:\n      name: \"public\"\n  identityRef:\n    name: \"openstack-cloud-config\"\n    cloudName: \"openstack\"\n    region: ${OS_REGION_NAME}\n\n  network:\n    filter:\n      name: ${NETWORK_NAME}\n  router:\n    filter:\n      name: ${ROUTER_NAME}\n  subnets:\n  - filter:\n      name: ${SUBNET_NAME}\n  ports:\n  - network:\n      filter:\n        name: ${NETWORK_NAME}\n</code></pre> <p>Note</p> <p> When deploying clusters with <code>openstack-hosted-cp</code> template version <code>1-0-2</code> or newer, the <code>identityRef.name</code> parameter is ignored and can be omitted. For older template versions, this parameter is required and must match the name of the <code>Secret</code> containing the <code>clouds.yaml</code> configuration.</p> <p>You can adjust <code>flavor</code>, <code>image name</code>, <code>region name</code>, <code>network</code>, <code>subnet</code> and <code>router</code> configuration to match your OpenStack environment. For more information about the configuration options, see the Template reference for openstack</p> <p>Alternatively, you can generate the <code>ClusterDeployment</code> manifest.</p> <p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dynamically inserts the appropriate values:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: openstack-hosted\nspec:\n  template: openstack-hosted-cp-1-0-7\n  credential: openstack-credential\n  config:\n    workersNumber: 2\n    flavor: m1.medium\n    image:\n      filter:\n        name: ubuntu-20.04\n    externalNetwork:\n      filter:\n        name: \"public\"\n    identityRef:\n      name: \"openstack-cloud-config\"\n      cloudName: \"openstack\"\n      region: \"{{.spec.identityRef.region}}\"\n\n    network:\n      filter:\n        name: \"{{.status.network.name}}\"\n    router:\n      filter:\n        name: \"{{.status.router.name}}\"\n    subnets:\n    - filter:\n        name: \"{{(index .status.network.subnets 0).name}}\"\n    ports:\n    - network:\n        filter:\n          name: \"{{.status.network.name}}\"\n</code></pre> <p>For more information on these and other available parameters, see the Template reference for openstack.</p> <p>Save this template as <code>clusterdeployment.yaml.tpl</code>, then generate your manifest using the following command:</p> <pre><code>kubectl -n &lt;cluster-namespace&gt; get openstackcluster &lt;cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre> </li> <li> <p>Apply the template</p> <p>Nothing actually happens until you apply the <code>ClusterDeployment</code> manifest to create a new cluster deployment:</p> <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-vmware/","title":"vSphere Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on vSphere.</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on vSphere with k0rdent installed.</li> </ul> <p>All control plane components for managed clusters will reside in the management cluster, so make sure the management cluster has sufficient resources (CPU, memory, and storage) to handle these workloads.</p> </li> <li> <p>Create the <code>ClusterDeployment</code> Manifest</p> <p>The <code>ClusterDeployment</code> manifest for vSphere-hosted control planes is similar to standalone control plane deployments. For a detailed list of parameters, refer to our discussion of Template parameters for vSphere.</p> <p>Important</p> <p> The vSphere provider requires you to specify the control plane endpoint IP before deploying the cluster. This IP address must match the one assigned to the k0smotron load balancer (LB) service. Use an annotation supported by your load balancer provider to assign the control plane endpoint IP to the k0smotron service. For example, the manifest below includes a <code>kube-vip</code> annotation.</p> <p><code>ClusterDeployment</code> objects for vSphere-based clusters include a <code>.spec.config.vsphere</code> object that contains vSphere-specific parameters. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-1-0-15\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"172.16.0.10\"\n</code></pre> </li> </ol> <p>For more information on these parameters, see the Template reference for vsphere.</p>"},{"location":"admin/installation/","title":"Installing k0rdent","text":"<p>This installation chapter guides you through the essential steps to set up k0rdent, beginning with the creation of a dedicated management cluster that serves as the control plane. You'll then install the actual k0rdent software, followed by verifying that it's installed successfully. From there, you'll prepare the management cluster to provision and manage child clusters, laying the groundwork for deploying and scaling your Kubernetes workloads across different infrastructures.</p> <ul> <li>Creating the management cluster</li> <li>Installing k0rdent</li> <li>Verifying the k0rdent installation</li> <li>Preparing k0rdent to create child clusters</li> </ul>"},{"location":"admin/installation/install-k0rdent/","title":"Install k0rdent","text":"<p>This section assumes that you already have a kubernetes cluster installed. If you need to setup a cluster you can follow the Create and prepare a Kubernetes cluster with k0s to create a test cluster, or Create and prepare a production grade Kubernetes cluster with EKS to create something more substantial. </p> <p>The actual management cluster is a Kubernetes cluster with the k0rdent application installed. The simplest way to install k0rdent is through its Helm chart.  You can find the latest release here, and from there you can deploy the Helm chart, as in:</p> <p><pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 1.5.0 -n kcm-system --create-namespace\n</code></pre> <pre><code>Pulled: ghcr.io/k0rdent/kcm/charts/kcm:1.5.0\nDigest: sha256:382291ed68a79eb8e03a5b2f2b7d06b56e3fb798158c519433e254b695955391\nNAME: kcm\nLAST DEPLOYED: Thu Oct 30 12:54:32 2025\nNAMESPACE: kcm-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre></p> <p>Note</p> <p> Make sure to specify the correct release version number.</p> <p>The helm chart deploys the KCM operator and prepares the environment, and KCM then proceeds to deploy the various subcomponents, including CAPI. The entire process takes a few minutes.</p>"},{"location":"admin/installation/install-k0rdent/#cleanup-uninstall-k0rdent","title":"Cleanup: Uninstall k0rdent","text":"<p>And of course when you need to clean up, you can use helm as well. Follow these steps:</p> <ol> <li> <p>Remove any <code>ClusterDeployment</code> objects in the cluster.</p> </li> <li> <p>Delete the <code>Management</code> object:</p> <pre><code>kubectl delete management.k0rdent kcm\n</code></pre> </li> <li> <p>Remove the kcm Helm release:</p> <pre><code>helm uninstall kcm -n kcm-system\n</code></pre> </li> <li> <p>Finally, remove the kcm-system namespace:</p> <pre><code>kubectl delete ns kcm-system\n</code></pre> </li> </ol>"},{"location":"admin/installation/verify-install/","title":"Confirming the deployment","text":"<p>Note</p> <p> After running the helm install command, please wait 5 to 10 minutes for the deployment to stabilize.</p> <p>To understand whether installation is complete, start by making sure all pods are ready in the <code>kcm-system</code> namespace. There should be 21 pod entries:</p> <pre><code>kubectl get pods -n kcm-system\n</code></pre> <pre><code>NAME                                                           READY   STATUS    RESTARTS   AGE\nazureserviceoperator-controller-manager-6b4dd86894-4dpfc       1/1     Running   0          7m15s\ncapa-controller-manager-64bbcb9f8-ltj6z                        1/1     Running   0          6m58s\ncapd-controller-manager-7586b6577c-6w4wq                       1/1     Running   0          7m11s\ncapg-controller-manager-774958b9b9-hvxvv                       1/1     Running   0          6m54s\ncapi-controller-manager-5b67d4fc7-dhk7p                        1/1     Running   0          8m49s\ncapo-controller-manager-6f98bb68cd-vsc24                       1/1     Running   0          6m33s\ncapv-controller-manager-69f7fc65d8-jx8s4                       1/1     Running   0          6m40s\ncapz-controller-manager-5b87fdf745-mzm7x                       1/1     Running   0          7m15s\nhelm-controller-746d7db585-x64ld                               1/1     Running   0          11m\nk0smotron-controller-manager-bootstrap-67dd88d848-wfrhm        2/2     Running   0          8m32s\nk0smotron-controller-manager-control-plane-657f5578d4-7vb8t    2/2     Running   0          8m24s\nk0smotron-controller-manager-infrastructure-5867d575f9-t28hp   2/2     Running   0          6m46s\nkcm-cert-manager-6979c67bc4-b6s4w                              1/1     Running   0          11m\nkcm-cert-manager-cainjector-5b97c84fdb-kdsw5                   1/1     Running   0          11m\nkcm-cert-manager-webhook-755796f599-q6727                      1/1     Running   0          11m\nkcm-cluster-api-operator-65c8f75569-rfsp2                      1/1     Running   0          9m30s\nkcm-controller-manager-68b56bff85-6fmsp                        1/1     Running   0          9m30s\nkcm-velero-67bf545995-x6784                                    1/1     Running   0          11m\nsource-controller-74b597b995-kkqqw                             1/1     Running   0          11m\n</code></pre> <pre><code>kubectl get pods -n kcm-system --no-headers | wc -l\n</code></pre> <pre><code>21\n</code></pre> <p>State management is handled by Project Sveltos, so you'll want to make sure that all 10 pods are running/completed in the <code>projectsveltos</code> namespace:</p> <pre><code>kubectl get pods -n projectsveltos\n</code></pre> <pre><code>NAME                                      READY   STATUS    RESTARTS   AGE\naccess-manager-6696df779-pnxjx            1/1     Running   0          10m\naddon-controller-6cb6c5f6df-zmfch         1/1     Running   0          10m\nclassifier-manager-5b47b66fc9-5mtwl       1/1     Running   0          10m\nevent-manager-564d6644b4-wr9cq            1/1     Running   0          10m\nhc-manager-7c56c59d9c-w5gds               1/1     Running   0          10m\nsc-manager-6798cd9d4d-r7z9j               1/1     Running   0          10m\nshard-controller-797965bb58-65lmp         1/1     Running   0          10m\nsveltos-agent-manager-5445f6f57c-wxw2s    1/1     Running   0          10m\ntechsupport-controller-5b666d6884-jfqnp   1/1     Running   0          10m\n</code></pre> <pre><code>kubectl get pods -n projectsveltos --no-headers | wc -l\n</code></pre> <pre><code>9\n</code></pre> <p>If any of these pods are missing, simply give k0rdent more time. If there's a problem, you'll see pods crashing and restarting, and you can see what's happening by describing the pod, as in:</p> <pre><code>kubectl describe pod classifieclassifier-manager-5b47b66fc9-5mtwl -n projectsveltos\n</code></pre> <p>As long as you're not seeing pod restarts, you just need to wait a few minutes.</p>"},{"location":"admin/installation/verify-install/#verify-that-k0rdent-itself-is-ready","title":"Verify that k0rdent itself is ready","text":"<p>The actual measure of whether k0rdent is ready is the state of the <code>Management</code> object. To check, issue this command:</p> <pre><code>kubectl get Management -n kcm-system\n</code></pre> <pre><code>NAME   READY   RELEASE     AGE\nkcm    True    kcm-1-5-0   9m\n</code></pre>"},{"location":"admin/installation/verify-install/#verify-the-templates","title":"Verify the templates","text":"<p>Next verify whether the KCM templates have been successfully installed and reconciled. Start with the <code>ProviderTemplate</code> objects:</p> <p><pre><code>kubectl get providertemplate -n kcm-system\n</code></pre> <pre><code>NAME                                   VALID\ncluster-api-1-0-7                                 true\ncluster-api-provider-aws-1-0-7                    true\ncluster-api-provider-azure-1-0-9                  true\ncluster-api-provider-docker-1-0-5                 true\ncluster-api-provider-gcp-1-0-6                    true\ncluster-api-provider-infoblox-1-0-2               true\ncluster-api-provider-ipam-1-0-3                   true\ncluster-api-provider-k0sproject-k0smotron-1-0-11   true\ncluster-api-provider-openstack-1-0-10              true\ncluster-api-provider-vsphere-1-0-6                true\nk0smotron-1-0-11                                   true\nkcm-1-5-0                                         true\nkcm-regional-1-0-5                                true\nprojectsveltos-1-1-1                             true\n</code></pre></p> <p>Make sure that all templates are not just installed, but valid. Again, this may take a few minutes.</p> <p>You'll also want to make sure the <code>ClusterTemplate</code> objects are installed and valid:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                             VALID\nadopted-cluster-1-0-7            true\naws-eks-1-0-3                    true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                  true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\ndocker-hosted-cp-1-0-19          true\ngcp-gke-1-0-3                    true\ngcp-hosted-cp-1-0-16             true\ngcp-standalone-cp-1-0-16         true\nopenstack-hosted-cp-1-0-17       true\nopenstack-standalone-cp-1-0-17   true\nremote-cluster-1-0-15            true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\n</code></pre></p>"},{"location":"admin/installation/verify-install/#verify-k0rdent-status","title":"Verify k0rdent status","text":"<p>The final test of whether k0rdent installation is installed is making sure the status of the <code>Management</code> object itself is <code>True</code>:</p> <p><pre><code>kubectl get management -n kcm-system\n</code></pre> <pre><code>NAME   READY   RELEASE         AGE\nkcm    True    kcm-1-5-0   18m\n</code></pre></p>"},{"location":"admin/installation/auth/","title":"OpenID Connect (OIDC)","text":"<p>OpenID Connect (OIDC) is an identity layer built on top of the OAuth 2.0 authorization framework. It streamlines the authentication process by enabling applications to verify a user's identity and retrieve essential profile information in a standardized and secure manner. k0rdent integrates with multiple OIDC providers.</p>"},{"location":"admin/installation/auth/#core-concepts-and-components","title":"Core Concepts and Components","text":"<p>OIDC enables k0rdent to provide a standardized way to handle authentication. </p> <ul> <li> <p>Authentication vs. Authorization:  It's easy to conflate authetication and authorization, but while they are related, they are not the same. Authentication establishes the identity of an individual or application, while authorization defines what that identity has permission to do.  While OAuth 2.0 is focused on authorization, granting applications access to resources, OIDC specifically addresses the authentication piece of the puzzle. It enables applications to confirm that a user is who they claim to be before allowing access.</p> </li> <li> <p>ID Tokens and JWTs:   At the heart of OIDC is the ID token, a JSON Web Token (JWT) that carries critical information (claims) about the user, such as a unique identifier, name, email, and the token\u2019s expiration. These tokens are digitally signed (often with RS256), which means they can be validated by any service that has access to the public key, eliminating the need for constant re-validation of credentials.</p> </li> <li> <p>Discovery and Dynamic Configuration:   OIDC features a discovery mechanism via the <code>/.well-known/openid-configuration</code> endpoint. This endpoint automatically provides metadata about the identity provider, including available authentication endpoints, supported scopes, and the necessary public keys for token verification. This simplifies setup and integration, reducing the need for manual configuration.</p> </li> <li> <p>Scopes and Claims:   The protocol defines standard scopes\u2014such as <code>openid</code>, <code>profile</code>, and <code>email</code>\u2014that specify what user data the client wishes to access. In response, the ID token will include corresponding claims, delivering verified user information in a secure and structured format. Custom scopes and claims can also be configured to meet specific application needs.</p> </li> <li> <p>Flexible Authentication Flows:   OIDC supports multiple flows tailored to different application types:</p> </li> <li>Authorization Code Flow: Ideal for server-side applications, where the client exchanges an authorization code for tokens in a secure manner.</li> <li>Implicit Flow: Suited for single-page or mobile applications, delivering tokens directly through the browser or app interface.</li> <li>Hybrid Flow: Combines elements of both the authorization code and implicit flows, balancing security and performance.</li> </ul>"},{"location":"admin/installation/auth/#oidc-in-the-context-of-kubernetes","title":"OIDC in the Context of Kubernetes","text":"<p>Integrating OIDC with Kubernetes enables the cluster to delegate authentication to an external identity provider. Here\u2019s how it works:</p> <ul> <li> <p>Delegated Authentication:   The Kubernetes API server is configured with OIDC parameters (like <code>--oidc-issuer-url</code> and <code>--oidc-client-id</code>). (In the context of k0rdent these are configured with templates.) When a user makes a request (for example, via <code>kubectl</code>), the API server validates the accompanying OIDC token by checking its signature, issuer, and expiration date. This offloads the responsibility of authentication to a trusted external provider.</p> </li> <li> <p>User Identity and RBAC:   Upon successful token validation, Kubernetes extracts user details (such as username or group memberships) from the token\u2019s claims. These details are then used to enforce Role-Based Access Control (RBAC) policies, ensuring  users have the proper permissions to access resources within the cluster.</p> </li> <li> <p>Simplified and Centralized User Management:   By integrating with a centralized OIDC provider, Kubernetes can manage user identities and access rights more efficiently across multiple clusters. This not only enhances security but also streamlines administrative tasks.</p> </li> </ul>"},{"location":"admin/installation/auth/#oidc-in-k0rdent","title":"OIDC in k0rdent","text":"<p>k0rdent supports multiple OIDC providers to accommodate various organizational needs. These options are:</p> <ul> <li>Okta</li> <li>Entra-ID</li> </ul>"},{"location":"admin/installation/auth/entra-id/","title":"Microsoft Entra ID","text":"<p>This section explains how to configure k0rdent to use Microsoft Entra ID as an OIDC provider for authentication. While the examples use KinD (Kubernetes in Docker) for demonstration purposes, the concepts and procedures are fully applicable to any k0rdent management cluster (for example, Minikube, MicroK8s, or a cloud-based cluster) that meets the minimum requirements for k0rdent.</p>"},{"location":"admin/installation/auth/entra-id/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that your environment meets the following prerequisites.</p>"},{"location":"admin/installation/auth/entra-id/#1-required-software","title":"1. Required Software","text":"<p>Make sure your development machine has the following installed:</p> <ul> <li>Docker: Container runtime to build and run containerized applications.</li> <li>k0rdent Management Cluster: Although KinD is used in this guide, you may use any k0rdent management cluster that supports deploying k0rdent.</li> <li>Helm: A package manager for Kubernetes to install and manage applications.</li> <li>Coreutils: Standard UNIX utilities for various file operations.</li> <li>jq: A lightweight and flexible command-line JSON processor.</li> <li>jwt: A CLI tool to decode and inspect JSON Web Tokens.</li> </ul>"},{"location":"admin/installation/auth/entra-id/#2-microsoft-entra-id-setup","title":"2. Microsoft Entra ID Setup","text":"<p>Prepare your Microsoft Entra ID environment by completing the following steps:</p> <ul> <li> <p>Register an OIDC-Enabled Application:   In the Microsoft Entra ID (formerly Azure AD) portal, register an application with OIDC enabled. Ensure that you configure the appropriate redirect URIs (for example, <code>http://localhost:8000</code>).</p> </li> <li> <p>Assign Users to an Entra ID Group:   Ensure that the users who will authenticate are assigned to an Entra ID group that you will use for Kubernetes RBAC.</p> </li> <li> <p>Configure Claims:   Verify that the OIDC application returns the necessary claims\u2014preferred_username (or email/upn if preferred), groups, and profile.  </p> <p>Note: Some Entra ID configurations might not return the <code>email</code> claim. In such cases, the <code>preferred_username</code> claim is used as the username.</p> </li> <li> <p>Obtain Entra ID Credentials:   After registering your application, note the following:</p> </li> <li>Your Tenant ID.</li> <li>The Authorization Server URL (for example, <code>https://login.microsoftonline.com/&lt;tenant-id&gt;/v2.0</code>).</li> <li>The Client ID and, if applicable, the Client Secret generated for your Kubernetes application.</li> </ul>"},{"location":"admin/installation/auth/entra-id/#installation-steps","title":"Installation Steps","text":"<p>Follow these steps to deploy Microsoft Entra ID as your OIDC provider.</p>"},{"location":"admin/installation/auth/entra-id/#1-install-krew-kubectl-plugin-manager","title":"1. Install Krew (Kubectl Plugin Manager)","text":"<p>Krew is a package manager for kubectl plugins. Installing it ensures you can manage the OIDC login plugin easily.</p> <p>You can install Krew by running the following command in your terminal. This script detects your OS and architecture, downloads the latest Krew release, extracts it, and installs it:</p> <pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/arm.*$/arm/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> <p>For additional details, troubleshooting tips, and instructions for Windows installations, see the official Krew Installation Guide.</p>"},{"location":"admin/installation/auth/entra-id/#2-install-the-oidc-login-plugin","title":"2. Install the OIDC Login Plugin","text":"<p>The OIDC login plugin for kubectl simplifies the process of obtaining and refreshing tokens from your Entra ID instance.</p> <pre><code>kubectl krew update\nkubectl krew install oidc-login\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#3-create-the-structured-authentication-configuration","title":"3. Create the Structured Authentication Configuration","text":"<p>This configuration file tells your Kubernetes API server how to validate JWT tokens issued by Entra ID. Create a file named <code>authentication-config.yaml</code> with the following content:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt:\n  - issuer:\n      url: \"https://login.microsoftonline.com/&lt;tenant-id&gt;/v2.0\"\n      audiences:\n        - \"&lt;client-id&gt;\"\n    claimMappings:\n      username:\n        claim: preferred_username  # Use 'email' or 'upn' if preferred and available\n        prefix: \"\"\n      groups:\n        claim: groups\n        prefix: \"\"\n    claimValidationRules:\n      - expression: \"has(claims.preferred_username)\"\n        message: \"preferred_username claim must be present\"\n      - expression: \"claims.preferred_username != ''\"\n        message: \"preferred_username claim must be non-empty\"\n      - expression: \"has(claims.groups)\"\n        message: \"groups claim must be present\"\n      - expression: \"type(claims.groups) == list ? size(claims.groups) &gt; 0 : true\"\n        message: \"groups list must be non-empty\"\n      - expression: \"type(claims.groups) == string ? claims.groups.size() &gt; 0 : true\"\n        message: \"groups string must be non-empty\"\n    userValidationRules:\n      - expression: \"!user.username.startsWith('system:')\"\n        message: \"username cannot use reserved system: prefix\"\n      - expression: \"user.groups.all(group, !group.startsWith('system:'))\"\n        message: \"groups cannot use reserved system: prefix\"\n</code></pre> <p>Note: Replace <code>&lt;tenant-id&gt;</code> and <code>&lt;client-id&gt;</code> with the actual values from your Microsoft Entra ID application registration. Adjust the claim mappings if your token uses different claim names (for example, <code>email</code> or <code>upn</code>).</p>"},{"location":"admin/installation/auth/entra-id/#4-configure-your-kubernetes-cluster","title":"4. Configure Your Kubernetes Cluster","text":"<p>Below is an example KinD cluster configuration that mounts the authentication configuration file. Adapt these instructions if you are using another Kubernetes system.</p>"},{"location":"admin/installation/auth/entra-id/#create-the-kind-cluster-configuration","title":"Create the KinD Cluster Configuration","text":"<p>Create a file named <code>kind-config.yaml</code>:</p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nfeatureGates:\n  \"StructuredAuthenticationConfiguration\": true\nnodes:\n  - role: control-plane\n    kubeadmConfigPatches:\n      - |\n        kind: ClusterConfiguration\n        apiServer:\n          extraArgs:\n            authentication-config: /etc/kubernetes/authentication-config.yaml\n          extraVolumes:\n            - name: authentication-config\n              hostPath: /etc/kubernetes/authentication-config.yaml\n              mountPath: /etc/kubernetes/authentication-config.yaml\n              readOnly: true\n              pathType: File\n    extraMounts:\n      - hostPath: ./authentication-config.yaml\n        containerPath: /etc/kubernetes/authentication-config.yaml\n        readOnly: true\n</code></pre> <p>For other Kubernetes distributions, the concept remains the same\u2014you need to configure your API server to load the <code>authentication-config.yaml</code> file. Consult your cluster\u2019s documentation for mounting configuration files and setting extra API server arguments.</p>"},{"location":"admin/installation/auth/entra-id/#5-cluster-management-with-kind-example","title":"5. Cluster Management with KinD (Example)","text":"<p>If you are using KinD, execute the following commands. Otherwise, adjust these steps to match your Kubernetes provider\u2019s procedures.</p>"},{"location":"admin/installation/auth/entra-id/#create-the-kind-cluster","title":"Create the KinD Cluster","text":"<pre><code>kind create cluster --verbosity 99 --config kind-config.yaml --retain\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#retrieve-api-server-pod-information","title":"Retrieve API Server Pod Information","text":"<p>To inspect the API server configuration, use:</p> <pre><code>kubectl describe pod -n kube-system kube-apiserver-$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#debugging-the-control-plane","title":"Debugging the Control Plane","text":"<p>For troubleshooting, you can view logs and container status:</p> <pre><code>docker exec kind-control-plane ls /var/log/containers/\ndocker exec kind-control-plane crictl ps\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#rbac-configuration","title":"RBAC Configuration","text":"<p>For k0rdent to work properly with OIDC-authenticated users, configure RBAC policies. Below is a sample RoleBinding configuration that grants permissions to a specific group.</p> <p>Create a file named <code>rolebinding.yaml</code>:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kcm-ns-viewer\n  namespace: kcm-system\nsubjects:\n  - kind: Group\n    name: \"&lt;group-id-from-token&gt;\"\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kcm-namespace-viewer-role\n</code></pre> <p>Note: Replace <code>&lt;group-id-from-token&gt;</code> with the group identifier returned in the Entra ID token that should have access to the <code>kcm-system</code> namespace.</p> <p>Apply this configuration using:</p> <pre><code>kubectl apply -f rolebinding.yaml\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#token-management","title":"Token Management","text":""},{"location":"admin/installation/auth/entra-id/#obtaining-a-microsoft-entra-id-oidc-token","title":"Obtaining a Microsoft Entra ID OIDC Token","text":"<p>Use the <code>kubectl oidc-login</code> plugin to retrieve a token from your Entra ID instance. The token is used for authenticating with your Kubernetes API server.</p> <pre><code>export K8S_TOKEN=$(kubectl oidc-login get-token \\\n  --oidc-issuer-url=https://login.microsoftonline.com/&lt;tenant-id&gt;/v2.0 \\\n  --oidc-client-id=&lt;client-id&gt; \\\n  --oidc-client-secret=&lt;client-secret&gt; \\\n  --oidc-redirect-url-hostname=localhost \\\n  --listen-address=localhost:8000 \\\n  --skip-open-browser=true \\\n  --oidc-extra-scope=\"email profile openid\" \\\n  --force-refresh | jq -r '.status.token' \\\n) &amp;&amp; echo $K8S_TOKEN | jwt decode -\n</code></pre> <p>Tip: Replace <code>&lt;tenant-id&gt;</code>, <code>&lt;client-id&gt;</code>, and <code>&lt;client-secret&gt;</code> with the actual values from your Entra ID application registration. The <code>--oidc-redirect-url-hostname</code> should match the redirect URI configured in Entra ID.</p>"},{"location":"admin/installation/auth/entra-id/#debug-token-validation","title":"Debug Token Validation","text":"<p>Validate the token by performing a simple API call that includes a higher verbosity level for debugging:</p> <pre><code>kubectl --token=$K8S_TOKEN get secrets -n kcm-system -v=9\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#configuring-kubernetes-cli","title":"Configuring Kubernetes CLI","text":"<p>After obtaining your token, update your kubectl configuration to use these OIDC credentials.</p>"},{"location":"admin/installation/auth/entra-id/#1-set-user-credentials","title":"1. Set User Credentials","text":"<pre><code>kubectl config set-credentials user --token=$K8S_TOKEN\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#2-create-a-new-context","title":"2. Create a New Context","text":"<p>Set up a context that references your cluster and the new user credentials. For KinD, you might use:</p> <pre><code>kubectl config set-context user --cluster=\"kind-$(kind get clusters | head -1)\" --user=user --namespace=kcm-system\n</code></pre> <p>For other Kubernetes clusters, replace the cluster name appropriately.</p>"},{"location":"admin/installation/auth/entra-id/#3-verify-access","title":"3. Verify Access","text":"<p>Confirm that your OIDC credentials provide the necessary access:</p> <pre><code>kubectl --context=user auth can-i get namespaces\nkubectl --context=user auth can-i get secrets -n kcm-system\nkubectl --context=user auth can-i get pods -n kcm-system\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#4-switch-contexts","title":"4. Switch Contexts","text":"<p>Switch to the OIDC context when needed:</p> <pre><code>kubectl config use-context user\n</code></pre> <p>To revert to your default context, use the standard context name (for example, the KinD default):</p> <pre><code>kubectl config use-context \"kind-$(kind get clusters | head -1)\"\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#5-view-kubeconfig-details","title":"5. View Kubeconfig Details","text":"<p>Inspect your current kubeconfig to confirm the setup:</p> <pre><code>kubectl config view --context=user\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#6-debug-inspect-api-server-logs","title":"6. [DEBUG] Inspect API Server Logs","text":"<p>For further troubleshooting, review the API server logs:</p> <pre><code>kubectl --context=\"kind-$(kind get clusters | head -1)\" logs -n kube-system kube-apiserver-kind-control-plane | grep authentication.go\n</code></pre>"},{"location":"admin/installation/auth/okta/","title":"Kubernetes OIDC Authentication Setup Guide for Okta","text":"<p>This section explains how to configure k0rdent to use Okta as an OIDC provider for authentication. While the examples use k0s for demonstration purposes, the concepts and procedures are fully applicable to any Kubernetes environment that meets the minimum requirements for k0rdent.</p>"},{"location":"admin/installation/auth/okta/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that your environment meets the following prerequisites.</p>"},{"location":"admin/installation/auth/okta/#1-required-software","title":"1. Required Software","text":"<p>Make sure your development machine has the following installed:</p> <ul> <li>Docker: Container runtime to build and run containerized applications.</li> <li>k0rdent Management Cluster: Although k0s is used in this guide, you may use any k0rdent management cluster (for example, Minikube, MicroK8s, or a cloud-based cluster) that supports deploying k0rdent.</li> <li>Helm: A package manager for Kubernetes to install and manage applications. </li> <li>Coreutils: Standard UNIX utilities for various file operations. </li> <li>jq: A lightweight and flexible command-line JSON processor. </li> <li>jwt: A CLI tool to decode and inspect JSON Web Tokens.</li> </ul>"},{"location":"admin/installation/auth/okta/#2-okta-setup","title":"2. Okta Setup","text":"<p>Prepare your Okta environment by completing the following steps:</p> <ul> <li> <p>Create an Okta Developer Account:   Visit the Okta Developer Signup page and create an account if you don\u2019t already have one.</p> </li> <li> <p>Review Okta Configuration Guides:   Familiarize yourself with the Okta user interface and setup procedures using guides such as the UI Guide. Although this guide focuses on Kubernetes, these resources provide valuable context on configuring your Okta organization, applications, and OIDC settings.</p> </li> <li> <p>Obtain Okta Credentials:   After setting up your Okta account and application, note the following:</p> </li> <li>Your Okta Domain (for example, <code>https://your-okta-domain.okta.com</code>)</li> <li>The Authorization Server URL (typically something like <code>https://your-okta-domain.okta.com/oauth2/default</code>)</li> <li>The Client ID and Client Secret generated when you register your Kubernetes application in Okta</li> <li>Any additional scopes or API keys as required by your integration</li> </ul>"},{"location":"admin/installation/auth/okta/#installation-steps","title":"Installation Steps","text":"<p>Follow these steps to deloy Okta.</p>"},{"location":"admin/installation/auth/okta/#1-install-krew-kubectl-plugin-manager","title":"1. Install Krew (Kubectl Plugin Manager)","text":"<p>Krew is a package manager for kubectl plugins. Installing it ensures you can manage the OIDC login plugin easily.</p> <p>You can install Krew by running the following command in your terminal. This script detects your OS and architecture, downloads the latest Krew release, extracts it, and installs it:</p> <p><pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> Add <code>krew</code> to your path: <pre><code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre></p> <p>For additional details, troubleshooting tips, and instructions for Windows installations, see the official Krew Installation Guide.</p>"},{"location":"admin/installation/auth/okta/#2-install-the-oidc-login-plugin","title":"2. Install the OIDC Login Plugin","text":"<p>The OIDC login plugin for kubectl simplifies the process of obtaining and refreshing tokens from your Okta instance.</p> <pre><code>kubectl krew update\nkubectl krew install oidc-login\n</code></pre>"},{"location":"admin/installation/auth/okta/#3-create-the-structured-authentication-configuration","title":"3. Create the Structured Authentication Configuration","text":"<p>This configuration file tells your Kubernetes API server how to validate JWT tokens issued by Okta. Create a file on the controller called <code>authentication-config.yaml</code> with the following content:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt:\n  - issuer:\n      url: \"&lt;YOUR_OKTA_DOMAIN&gt;\"\n      audiences:\n        - &lt;YOUR_OKTA_CLIENT_ID&gt;\n    claimMappings:\n      username:\n        claim: email\n        prefix: \"\"\n      groups:\n        claim: groups\n        prefix: \"\"\n    claimValidationRules:\n      - expression: \"has(claims.email)\"\n        message: \"email claim must be present\"\n      - expression: \"claims.email != ''\"\n        message: \"email claim must be non-empty\"\n      - expression: \"has(claims.groups)\"\n        message: \"groups claim must be present\"\n      - expression: \"type(claims.groups) == list ? size(claims.groups) &gt; 0 : true\"\n        message: \"groups list must be non-empty\"\n      - expression: \"type(claims.groups) == string ? claims.groups.size() &gt; 0 : true\"\n        message: \"groups string must be non-empty\"        \n    userValidationRules:\n      - expression: \"!user.username.startsWith('system:')\"\n        message: \"username cannot use reserved system: prefix\"\n      - expression: \"user.groups.all(group, !group.startsWith('system:'))\"\n        message: \"groups cannot use reserved system: prefix\"\n</code></pre> <p>Note</p> <p> Remember to replace placeholder values with the actual values from your Okta configuration.</p>"},{"location":"admin/installation/auth/okta/#4-configure-your-kubernetes-cluster","title":"4. Configure Your Kubernetes Cluster","text":"<p>Edit your k0s config (<code>/etc/k0s/config.yaml</code>) to tell the API server to load your auth config and enable the feature gate:</p> <pre><code>spec:\n  api:\n    extraArgs:\n      # enable the StructuredAuthenticationConfiguration feature\n      - --feature-gates=StructuredAuthenticationConfiguration=true\n      # point to your auth config file\n      - --authentication-config=/etc/k0s/authentication-config.yaml\n    extraVolumes:\n      - name: auth-config\n        hostPath: /etc/k0s/authentication-config.yaml\n        mountPath: /etc/k0s/authentication-config.yaml\n        readOnly: true\n        pathType: File\n</code></pre> <p>Then:</p> <pre><code># ensure your auth file is in place\nsudo mkdir -p /etc/k0s\nsudo cp authentication-config.yaml /etc/k0s/authentication-config.yaml\nsudo chmod 600 /etc/k0s/authentication-config.yaml\n\n# restart k0s so the API server picks up the new flags\nsudo systemctl restart k0scontroller\n</code></pre> <p>For other Kubernetes distributions, the concept remains the same\u2014you need to configure your API server to load the <code>authentication-config.yaml</code> file. Check your cluster\u2019s documentation for mounting configuration files and setting extra API server arguments.</p>"},{"location":"admin/installation/auth/okta/#rbac-configuration","title":"RBAC Configuration","text":"<p>For k0rdent to work properly with OIDC-authenticated users, configure RBAC policies. Below is a sample RoleBinding configuration that grants permissions to a specific group.</p> <p>Create a file named <code>rolebinding.yaml</code>:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kcm-ns-viewer\n  namespace: kcm-system\nsubjects:\n  - kind: Group\n    name: kcm-ns-viewer # Must match the Okta group claim\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kcm-namespace-viewer-role\n</code></pre> <p>Apply this configuration using:</p> <pre><code>kubectl apply -f rolebinding.yaml\n</code></pre>"},{"location":"admin/installation/auth/okta/#token-management","title":"Token Management","text":""},{"location":"admin/installation/auth/okta/#obtaining-an-okta-oidc-token","title":"Obtaining an Okta OIDC Token","text":"<p>Use the <code>kubectl oidc-login</code> plugin to retrieve a token from your Okta instance. The token is used for authenticating with your Kubernetes API server.</p> <pre><code>export K8S_TOKEN=$(kubectl oidc-login get-token \\\n  --oidc-issuer-url=&lt;YOUR_OKTA_DOMAIN&gt; \\\n  --oidc-client-id=&lt;YOUR_OKTA_CLIENT_ID&gt; \\\n  --listen-address=127.0.0.1:8000 \\\n  --skip-open-browser=true \\\n  --oidc-extra-scope=email \\\n  --force-refresh | jq -r '.status.token' \\\n) &amp;&amp; echo $K8S_TOKEN | jwt decode -\n</code></pre> <p>Tip: Replace the placeholder values with your actual Okta configuration details. The command pipes the token to <code>jwt decode</code> so you can inspect its contents.</p>"},{"location":"admin/installation/auth/okta/#debug-token-validation","title":"Debug Token Validation","text":"<p>Validate the token by performing a simple API call that includes a higher verbosity level for debugging:</p> <pre><code>kubectl --token=$K8S_TOKEN get secrets -n kcm-system -v=9\n</code></pre>"},{"location":"admin/installation/auth/okta/#configuring-the-kubernetes-cli","title":"Configuring the Kubernetes CLI","text":"<p>After obtaining your token, update your kubectl configuration to use these OIDC credentials.</p>"},{"location":"admin/installation/auth/okta/#1-set-user-credentials","title":"1. Set User Credentials","text":"<pre><code>kubectl config set-credentials user --token=$K8S_TOKEN\n</code></pre>"},{"location":"admin/installation/auth/okta/#2-create-a-new-context","title":"2. Create a New Context","text":"<p>In your <code>~/.kube/config</code>, add:</p> <pre><code>users:\n...\n- name: okta-user\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      command: kubectl\n      args:\n        - oidc-login\n        - get-token\n        - --oidc-issuer-url=&lt;YOUR_OKTA_DOMAIN&gt;\n        - --oidc-client-id=&lt;YOUR_OKTA_CLIENT_ID&gt;\n        - --oidc-client-secret=&lt;YOUR_OKTA_CLIENT_SECRET&gt;\n        - --oidc-extra-scope=email\n        - --oidc-extra-scope=groups\n</code></pre> <p>Also add a new context referencing that user, as in:</p> <pre><code>contexts:\n...\n- name: okta@k0s        \n  context:\n    cluster: k0s\n    user: okta-user\n</code></pre>"},{"location":"admin/installation/auth/okta/#3-verify-access","title":"3. Verify Access","text":"<p>Confirm that your OIDC credentials provide the necessary access:</p> <pre><code>kubectl --context=okta@k0s auth can-i get namespaces\nkubectl --context=okta@k0s auth can-i get secrets -n kcm-system\nkubectl --context=okta@k0s auth can-i get pods -n kcm-system\n</code></pre>"},{"location":"admin/installation/auth/okta/#4-switch-contexts","title":"4. Switch Contexts","text":"<p>Switch to the OIDC context when needed:</p> <pre><code>kubectl config use-context okta@k0s\n</code></pre> <p>To revert to your default context, use the standard context name:</p> <pre><code>kubectl config use-context Default\n</code></pre>"},{"location":"admin/installation/auth/okta/#5-view-kubeconfig-details","title":"5. View Kubeconfig Details","text":"<p>Inspect your current kubeconfig to confirm the setup:</p> <pre><code>kubectl config view --context=okta@k0s \n</code></pre>"},{"location":"admin/installation/auth/okta/#6-debug-inspect-api-server-logs","title":"6. [DEBUG] Inspect API Server Logs","text":"<p>For further troubleshooting, review the API server logs:</p> <pre><code>journalctl -u k0scontroller -f\n</code></pre> <p>By following these instructions, you will have a fully functional OIDC authentication system integrated with your Kubernetes cluster, regardless of whether you\u2019re using k0s or another deployment environment.</p>"},{"location":"admin/installation/create-mgmt-clusters/","title":"Creating the management cluster","text":"<p>The type of cluster you create for as a k0rdent management cluster is going to depend on how you're going to use it. For example, a simple single-node k0s install is sufficient for testing and evaluation, but you will want a multi-node, etcd-backed cluster such as one created using k0sctl or EKS for production.</p> <p>Note</p> <p> For best results use Kubernetes version 1.32. or above.</p> <p>In a production environment, you will always want to ensure that your management cluster is backed up. There are a few caveats and things you need to take into account when backing up k0rdent. More info can be found in the guide at use Velero as a backup provider.</p> <ul> <li>Create a single node k0s cluster</li> <li>Create a multi-node k0s cluster</li> <li>Create a multinode EKS cluster</li> </ul>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-eks-multi/","title":"Create and prepare a production-grade Kubernetes cluster with EKS","text":"<p>Follow these steps to install and prepare an Amazon EKS management cluster:</p> <ol> <li> <p>The basic AWS tools</p> <p>Start by installing and configuring the Amazon tools. First download and install the <code>aws</code> tool:</p> <p><pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre>  Then configure it using environment variables:</p> <p><pre><code>export AWS_ACCESS_KEY_ID=\"EXAMPLE_ACCESS_KEY_ID\"\nexport AWS_SECRET_ACCESS_KEY=\"EXAMPLE_SECRET_ACCESS_KEY\"\nexport AWS_SESSION_TOKEN=\"EXAMPLE_SESSION_TOKEN\"\naws configure\n</code></pre> <pre><code>AWS Access Key ID [EXAMPLE_ACCESS_KEY_ID]:\nAWS Secret Access Key [EXAMPLE_SECRET_ACCESS_KEY]:\nDefault region name [YOUR_AWS_REGION]:\nDefault output format [json]:\n</code></pre>  Once <code>aws</code> is installed you can install <code>eksctl</code>:</p> <p><pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\neksctl version\n</code></pre> <pre><code>0.211.0\n</code></pre></p> </li> <li> <p>Create the cluster</p> <p>You can use the eksctl tool to create the basic Kubernetes cluster: <pre><code>eksctl create cluster --name &lt;CLUSTER_NAME&gt; \\\n--version 1.31 \\\n--region &lt;YOUR_AWS_REGION&gt; \\\n--without-nodegroup\n</code></pre> <pre><code>2025-03-05 22:26:02 [\u2139]  eksctl version 0.211.0\n...\n2025-03-05 22:36:14 [\u2714]  all EKS cluster resources for \"CLUSTER_NAME\" have been created\n2025-03-05 22:36:15 [\u2139]  kubectl command should work with \"/home/username/.kube/config\", try 'kubectl get nodes'\n2025-03-05 22:36:15 [\u2714]  EKS cluster \"CLUSTER_NAME\" in \"YOUR_AWS_REGION\" region is ready\n</code></pre></p> </li> <li> <p>Add controllers</p> <p>While the cluster is now created, it doesn't actually have any nodes.  Start by adding controllers: <pre><code>eksctl create nodegroup --cluster &lt;CLUSTER_NAME&gt; \\\n--name &lt;CONTROLLER_NODE_GROUP&gt; \\\n--node-type t3.medium \\\n--nodes 3 \\\n--nodes-min 3 \\\n--nodes-max 3 \\\n--node-labels \"role=control-plane\" \n</code></pre> <pre><code>2025-03-05 22:57:15 [\u2139]  will use version 1.31 for new nodegroup(s) based on control plane version\n2025-03-05 22:57:18 [\u2139]  nodegroup \"nickchasek0rdentcontroller-group\" will use \"\" [AmazonLinux2/1.31]\n2025-03-05 22:57:19 [\u2139]  1 nodegroup (nickchasek0rdentcontroller-group) was included (based on the include/exclude rules)\n2025-03-05 22:57:19 [\u2139]  will create a CloudFormation stack for each of 1 managed nodegroups in cluster \"NickChasek0rdentControlCluster\"\n...\n2025-03-05 23:00:27 [\u2139]  all nodegroups have up-to-date cloudformation templates\n</code></pre></p> </li> <li> <p>Install kubectl</p> <p>Everything you do in k0rdent is done by creating and manipulating Kubernetes objects, so you'll need to have <code>kubectl</code> installed. You can find the full install docs here, or just follow these instructions:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p>Get the KUBECONFIG</p> <p>You don't have to access the KUBECONFIG directly; you can use the aws tool to get access to it:</p> <p><pre><code>aws eks update-kubeconfig --region &lt;YOUR_AWS_REGION&gt; --name &lt;CLUSTER_NAME&gt;\n</code></pre> <pre><code>Updated context arn:aws:eks:ca-central-1:026090528175:cluster/NickChasek0rdentControlCluster in /home/nick/.kube/config\n</code></pre></p> </li> <li> <p>Taint controllers</p> <p>To prevent workloads from being scheduled on the controllers, add the <code>node-role.kubernetes.io/control-plane=true:NoSchedule</code> taint. First list the started nodes:</p> <p><pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                              STATUS   ROLES    AGE    VERSION\nnodename1.compute.internal   Ready    &lt;none&gt;   4h1m   v1.31.7-eks-5d632ec\nnodename2.compute.internal   Ready    &lt;none&gt;   4h1m   v1.31.7-eks-5d632ec\nnodename3.compute.internal   Ready    &lt;none&gt;   4h1m   v1.31.7-eks-5d632ec\n</code></pre> For each node, go ahead and set the taint:</p> <p><pre><code>kubectl taint nodes nodename1.compute.internal node-role.kubernetes.io/control-plane=true:NoSchedule\n</code></pre> <pre><code>node/nodename1.ca-central-1.compute.internal tainted\n</code></pre> Now verify the taints: <pre><code>kubectl describe nodes | grep -A 5 \"Taints:\"\n</code></pre> <pre><code>Taints:             node-role.kubernetes.io/control-plane=true:NoSchedule\nUnschedulable:      false\nLease:\nHolderIdentity:  nodename1.compute.internal\nAcquireTime:     &lt;unset&gt;\nRenewTime:       Fri, 25 Jul 2025 21:16:56 -0500\n--\nTaints:             node-role.kubernetes.io/control-plane=true:NoSchedule\nUnschedulable:      false\nLease:\nHolderIdentity:  nodename2.compute.internal\nAcquireTime:     &lt;unset&gt;\nRenewTime:       Fri, 25 Jul 2025 21:16:56 -0500\n--\nTaints:             node-role.kubernetes.io/control-plane=true:NoSchedule\nUnschedulable:      false\nLease:\nHolderIdentity:  nodename3.ca-central-1.compute.internal\nAcquireTime:     &lt;unset&gt;\nRenewTime:       Fri, 25 Jul 2025 21:16:56 -0500\n</code></pre></p> </li> <li> <p>Add worker nodes</p> <p>Adding worker nodes is simpler than controllers:</p> <p><pre><code>eksctl create nodegroup --cluster &lt;CLUSTER_NAME&gt; \\\n--name &lt;WORKER_NODE_GROUP&gt; \\\n--node-type t3.medium \\\n--nodes 3 \\\n--nodes-min 3 \\\n--nodes-max 5 \\\n--node-labels \"role=worker\"\n</code></pre> <pre><code>2025-03-06 03:10:48 [\u2139]  will use version 1.31 for new nodegroup(s) based on control plane version\n...\n2025-03-06 03:13:38 [\u2714]  created 1 managed nodegroup(s) in cluster \"CLUSTER_NAME\"\n2025-03-06 03:13:39 [\u2139]  checking security group configuration for all nodegroups\n2025-03-06 03:13:39 [\u2139]  all nodegroups have up-to-date cloudformation templates\n</code></pre> Verify the nodes: <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                              STATUS   ROLES    AGE     VERSION\nnodename1.compute.internal   Ready    &lt;none&gt;   4h14m   v1.31.7-eks-5d632ec\nnodename4.compute.internal   Ready    &lt;none&gt;   79s     v1.31.7-eks-5d632ec\nnodename2.compute.internal   Ready    &lt;none&gt;   4h14m   v1.31.7-eks-5d632ec\nnodename5.compute.internal   Ready    &lt;none&gt;   82s     v1.31.7-eks-5d632ec\nnodename3.compute.internal   Ready    &lt;none&gt;   4h14m   v1.31.7-eks-5d632ec\nnodename6.compute.internal   Ready    &lt;none&gt;   4h14m   v1.31.7-eks-5d632ec\n</code></pre></p> </li> <li> <p>Verify pods</p> <p>Make sure pods will run properly by deploying a test pod: <pre><code>kubectl run test-pod --image=nginx --restart=Never\nkubectl get pods -o wide\n</code></pre> <pre><code>NAME       READY   STATUS    RESTARTS   AGE   IP               NODE                                              NOMINATED NODE   READINESS GATES\ntest-pod   1/1     Running   0          15s   192.168.76.104   ip-192-168-68-189.ca-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\n</code></pre> Clean up so you can start fresh: <pre><code>kubectl delete pod test-pod\n</code></pre> <pre><code>pod \"test-pod\" deleted\n</code></pre> <pre><code>kubectl get pods -o wide\n</code></pre> <pre><code>No resources found in default namespace.\n</code></pre></p> </li> <li> <p>Install Helm</p> <p>Finally, the easiest way to install k0rdent is through its Helm chart, so let's get Helm installed. You can find the full instructions here, or use these instructions:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Helm will be installed into <code>/usr/local/bin/helm</code>.</p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/","title":"Create and prepare a multinode Kubernetes cluster with k0s","text":"<p>Many features of k0rdent require a Kubernetes cluster that uses etcd for its persistence layer. While this is the case for many implementations, the single-node k0s cluster uses SQLite instead. Fortunately, multi-node k0s uses etcd by default, so you can solve this problem by creating controller and worker nodes and linking them together. </p> <p>Follow these steps to install and prepare a multinode k0s kubernetes management cluster:</p>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/#create-the-first-controller","title":"Create the first controller","text":"<ol> <li> <p>Download k0s:</p> <p>Run the k0s download script to download the latest stable version of k0s and make it executable from <code>/usr/bin/k0s</code>:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\n</code></pre> </li> <li> <p>Create a configuration file:</p> <p>Note</p> <p> Depending on your configuration, you may need to execute these commands as root rather than using <code>sudo</code>. If so, executing <code>sudo su</code> will be sufficient. Don't forget to exit the root user afterwards!</p> <pre><code>mkdir -p /etc/k0s\nk0s config create &gt; /etc/k0s/k0s.yaml\n</code></pre> </li> <li> <p>If you need to access the cluster from outside the specific node on which you're installing it, add the public hostname as the <code>spec.api.externalAddress</code> and as an instance under <code>spec.api.sans</code>, as in:</p> <pre><code>...\nspec:\n    api:\n        address: 172.31.7.199\n        externalAddress: myhost.example.com\n        ca:\n        certificatesExpireAfter: 8760h0m0s\n        expiresAfter: 87600h0m0s\n        k0sApiPort: 9443\n        port: 6443\n        sans:\n        - 172.31.7.199\n        - fe80::c4:e6ff:fecc:9739\n        - myhost.example.com\n    controllerManager: {}\n    extensions:\n        helm:\n...\n</code></pre> <p>You can also make other configuration changes. For more information see the k0s documentation.</p> </li> <li> <p>Install the controller:</p> <pre><code>sudo k0s install controller -c /etc/k0s/k0s.yaml\nsudo k0s start\n</code></pre> <p>The k0s process acts as a \"supervisor\" for all of the control plane components. In moments the control plane will be up and running, but because of the k0s architecture, you won't see any nodes running, because only worker nodes show up, and we haven't created any yet. You can, however, use the k0s-installed <code>kubectl</code> to make sure the cluster is up and running by looking for existing namespaces:</p> <p><pre><code>sudo k0s kubectl get namespaces\n</code></pre> <pre><code>NAME              STATUS   AGE\ndefault           Active   5m15s\nk0s-autopilot     Active   5m11s\nkube-node-lease   Active   5m15s\nkube-public       Active   5m15s\nkube-system       Active   5m15s\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/#add-workers-to-the-cluster","title":"Add workers to the cluster","text":"<p>Once the contoller is up, you can add a worker node by creating a \"join token\" and using it to start k0s on a new server. Follow these instructions:</p> <ol> <li> <p>Create a join token:</p> <p>To join additional nodes to the cluster, you need a join token, which is a base64-encoded <code>KUBECONFIG</code>. The token embeds information that enables mutual trust between the worker and controller(s) and allows the node to join the cluster.</p> <p>To get a worker token, run the following command on newly created existing controller node:</p> <pre><code>sudo k0s token create --role=worker &gt; token-file\n</code></pre> <p>The resulting output is a long token string, which we'll use in a moment to add a worker to the cluster.</p> <p>For enhanced security, set an expiration time for the token:</p> <pre><code>sudo k0s token create --role=worker --expiry=100h &gt; token-file\n</code></pre> </li> <li> <p>Create the new node:</p> <p>Start by instantiating the new node and downloading k0s, as before:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\n</code></pre> </li> <li> <p>Copy the token file you created in step 1 to the new server.</p> </li> <li> <p>Start the new worker: </p> <p>Now you can start the new worker, feeding it the token file so that it automatically joins the existing cluster:</p> <p>Note</p> <p> Don't forget to use the actual path to your token file.</p> <pre><code>sudo k0s install worker --token-file /path/to/token/file/token-file\nsudo k0s start\n</code></pre> </li> <li> <p>Check the node status:</p> <p>Now you can check the status of the node by going to the controller and once again checking for nodes. After a minute or two, you'll see the new node in a <code>Ready</code> state:</p> <p><pre><code>sudo k0s kubectl get nodes\n</code></pre> <pre><code>NAME              STATUS     ROLES    AGE     VERSION\nip-172-31-9-107   Ready      &lt;none&gt;   3m39s   v1.33.1+k0s\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/#add-controllers-to-the-cluster","title":"Add controllers to the cluster","text":"<p>To create a join token for the new controller, follow these steps.</p> <ol> <li> <p>Copy the <code>k0s.yaml</code> file to the new controller server at <code>/etc/k0s/k0s.yaml</code>. Each controller in the cluster must have this <code>k0s.yaml</code> file, or some cluster nodes will use default config values, which will lead to inconsistent behavior. </p> <p>If your configuration file includes IP addresses (node address, sans, etcd peerAddress), remember to update them accordingly for this specific controller node.</p> </li> <li> <p>Run the following command on an existing controller:</p> <pre><code>sudo k0s token create --role=controller --expiry=1h &gt; controller-token-file\n</code></pre> <p>Copy the <code>controller-token-file</code> file to the new controller server.</p> </li> <li> <p>As before, download and install k0s in the new controller:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\n</code></pre> </li> <li> <p>On the new controller, run:</p> <pre><code>sudo k0s install controller --token-file /path/to/token/file -c /etc/k0s/k0s.yaml\nsudo k0s start\n</code></pre> </li> <li> <p>This time, check k0s status by running this command on the new controller:</p> <p><pre><code>sudo k0s status\n</code></pre> <pre><code>Version: v1.33.1+k0s.1\nProcess ID: 1489\nRole: controller\nWorkloads: false\nSingleNode: false\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/#access-your-cluster","title":"Access your cluster","text":"<p>Use the Kubernetes 'kubectl' command-line tool that comes with k0s binary to deploy your application or check your node status:</p> <p><pre><code>sudo k0s kubectl get nodes\n</code></pre> <pre><code>NAME   STATUS   ROLES    AGE    VERSION\nk0s    Ready    &lt;none&gt;   4m6s   v1.33.1+k0s\n</code></pre></p> <p>You can also install <code>kubectl</code> directly. You can find the full install docs here, or just follow these instructions:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> <p>To use <code>kubectl</code> directly or to access the cluster from another machine, copy the <code>KUBECONFIG</code>, which is located on the controllers at:</p> <pre><code>/var/lib/k0s/pki/admin.conf\n</code></pre> <p>to your target machine.  Note that to access the cluster from an external machine you must replace <code>localhost</code> in the <code>KUBECONFIG</code> with the host IP address or hostname for your controller. Make sure to use the address you added to the <code>sans</code> field, and also that port <code>6443</code> is accessible.</p>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-single/","title":"Create and prepare a Kubernetes cluster with k0s","text":"<p>Follow these steps to install and prepare a k0s kubernetes management cluster:</p> <ol> <li> <p>Deploy a Kubernetes cluster</p> <p>The first step is to create the actual cluster itself. Again, the actual distribution used for the management cluster isn't important, as long as it's a CNCF-compliant distribution. That means you can use an existing EKS cluster, or whatever is your normal corporate standard. </p> <p>To make things simple this guide uses k0s, a small, convenient, and fully-functional distribution. For more granular instructions, including those for creating a cluster accessible from a different server, see the k0s multi-node instructions:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --enable-worker --no-taints\nsudo k0s start\n</code></pre> <p>k0s includes its own preconfigured version of <code>kubectl</code> so make sure the cluster is running:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>After 2-3 minutes you should see a single <code>control-plane</code> node with a status of <code>Ready</code>, as in:</p> <pre><code>NAME              STATUS   ROLES            AGE   VERSION\nip-172-31-29-61   Ready    control-plane    46s   v1.31.2+k0s\n</code></pre> </li> <li> <p>Install kubectl</p> <p>Everything you do in k0rdent is done by creating and manipulating Kubernetes objects, so you'll need to have <code>kubectl</code> installed. You can find the full install docs here, or just follow these instructions:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p>Get the kubeconfig</p> <p>In order to access the management cluster you will, of course, need the kubeconfig. Again, if you're using another Kubernetes distribution follow those instructions to get the kubeconfig, but for k0s, the process involves simply copying the existing file and adding it to an environment variable so <code>kubectl</code> knows where to find it.</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>Now you should be able to use the non-k0s <code>kubectl</code> to see the status of the cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Again, you should see the single k0s node, but by this time it should have had its role assigned, as in:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre> <p>Now the cluster is ready for installation, which we'll do using Helm.</p> </li> <li> <p>Install Helm</p> <p>The easiest way to install k0rdent is through its Helm chart, so let's get Helm installed. You can find the full instructions here, or use these instructions:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Helm will be installed into <code>/usr/local/bin/helm</code>.</p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-single/#access-your-cluster-from-another-machine","title":"Access your cluster from another machine","text":"<p>To use a tool like Lens or to access the cluster from another machine, copy the <code>KUBECONFIG</code>, which is located at:</p> <pre><code>/var/lib/k0s/pki/admin.conf\n</code></pre> <p>to your target machine.  Note that to access the cluster from an external machine you must replace <code>localhost</code> in the <code>KUBECONFIG</code> with the host IP address or hostname for your controller. Make sure to use the address you added to the <code>sans</code> field, and also that port <code>6443</code> is accessible.</p>"},{"location":"admin/installation/prepare-mgmt-cluster/","title":"Prepare k0rdent to create child clusters on multiple providers","text":"<p>Managed clusters can be hosted on a number of different platforms. At the time of this writing, those platforms include:</p> <ul> <li>AWS</li> <li>Azure</li> <li>OpenStack</li> <li>VMware</li> <li>Google Cloud Platform</li> </ul>"},{"location":"admin/installation/prepare-mgmt-cluster/aws/","title":"AWS","text":"<p>k0rdent can deploy managed clusters as both EC2-based Kubernetes clusters and EKS clusters. In both cases, you'll need to create the relevant credentials, and to do that you'll need to configure an IAM user. Follow these steps to make it possible to deploy to AWS:</p>"},{"location":"admin/installation/prepare-mgmt-cluster/aws/#test-user-creation","title":"Test user creation","text":"<p>Part of this process involves creating a user, which is sometimes forbidden by organizational policies. Fortunately once the user is created you don't need to do it again, so if you're not sure, either contact your administrator or run this test.</p> <ol> <li> <p>Configure AWS IAM</p> <p>Start by specifying the environment variables the AWS CLI will use:</p> <pre><code>export AWS_REGION=&lt;EXAMPLE_AWS_REGION&gt;\nexport AWS_ACCESS_KEY_ID=&lt;EXAMPLE_ACCESS_KEY_ID&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\nexport AWS_SESSION_TOKEN=&lt;YOUR_SESSION_TOKEN&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> </li> <li> <p>Install the AWS CLI</p> <p>If you haven't already, install the <code>aws</code> CLI tool:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre> <p>The tool recognizes the environment variables you created earlier, so there's no need to login.</p> </li> <li> <p>Create and delete a test user</p> <p>There are ways test the permissions assigned to your user, but the only way to ensure there are no policies preventing you from creating a user is to actually do it.  Execute these commands:</p> <pre><code>aws iam create-user --user-name test-delete-me\naws iam delete-user --user-name test-delete-me\n</code></pre> <p>If your account can create a user, you'll see output such as:</p> <pre><code>{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"test-delete-me\",\n        \"UserId\": \"EXAMPLE_USER_ID\",\n        \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/test-delete-me\",\n        \"CreateDate\": \"2025-10-05T17:02:36+00:00\"\n    }\n}\n</code></pre> <p>Note that you will NOT see any indication that the user has been deleted, even though it has. You can make sure by running:</p> <p><pre><code>aws iam get-user --user-name test-delete-me\n</code></pre> <pre><code>An error occurred (NoSuchEntity) when calling the GetUser operation: The user with name test-delete-me cannot be found.\n</code></pre></p> </li> </ol> <p>If you are unable to create a user, contact your administrator to request the user creation. (You should also show them this page to make sure you won't run into other issues.)</p>"},{"location":"admin/installation/prepare-mgmt-cluster/aws/#configure-the-system-for-aws-child-clusters","title":"Configure the system for AWS child clusters","text":"<p>Now go ahead and prepare the cluster.</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>Install <code>clusterawsadm</code></p> <p>k0rdent uses the Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. <code>clusterawsadm</code>, a CLI tool created by CAPA project, helps with AWS-specific tasks such as creating IAM roles and policies, as well as credential configuration. To install clusterawsadm on Ubuntu on x86 hardware, execute these commands:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre> </li> <li> <p>Configure AWS IAM</p> <p>Next you'll need to create the IAM policies and service account k0rdent will use to take action within the AWS infrastructure. (Note that you only need to do this once.)</p> <p>The first step is to create the IAM CloudFormation stack based on your admin user. Start by specifying the environment variables <code>clusterawsadm</code> will use as AWS credentials:</p> <pre><code>export AWS_REGION=&lt;EXAMPLE_AWS_REGION&gt;\nexport AWS_ACCESS_KEY_ID=&lt;EXAMPLE_ACCESS_KEY_ID&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\nexport AWS_SESSION_TOKEN=&lt;YOUR_SESSION_TOKEN&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> </li> <li> <p>Create the IAM CloudFormation stack</p> <p>Now use <code>clusterawsadm</code> to create the IAM CloudFormation stack:</p> <p><code>bash   clusterawsadm bootstrap iam create-cloudformation-stack</code></p> </li> <li> <p>Install the AWS CLI</p> <p>With the stack in place you can create the AWS IAM user. You can do this in the UI, but it's also possible to do it from the command line using the <code>aws</code> CLI tool.  Start by installing it, if you haven't already:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre> <p>The tool recognizes the environment variables you created earlier, so there's no need to login.</p> </li> <li> <p>Check for available IPs</p> <p>Because k0rdent has 3 availablilty zone NAT gateways, each cluster needs 3 public IPs. Unfortunately, the default <code>EC2-VPC Elastic IPs</code> quota per region is 5, so while you likely won't have issues with a first cluster, if you try to deplay a  second to the same region, you are likely to run into issues.  </p> <p>You can determine how many elastic IPs are available from the command line:</p> <p><pre><code>LIMIT=$(aws ec2 describe-account-attributes --attribute-names vpc-max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)\nUSED=$(aws ec2 describe-addresses --query 'Addresses[*].PublicIp' --output text | wc -w)\nAVAILABLE=$((LIMIT - USED))\necho \"Available Public IPs: $AVAILABLE\"\n</code></pre> <pre><code>Available Public IPs: 5\n</code></pre></p> <p>If you have less than 3 available public IPs, you can request an increase in your quota:</p> <pre><code>aws service-quotas request-service-quota-increase \\\n    --service-code ec2 \\\n    --quota-code L-0263D0A3 \\\n    --desired-value 20\n</code></pre> <p>You can check on the status of your request:</p> <p><pre><code>aws service-quotas list-requested-service-quota-change-history \\\n    --service-code ec2\n</code></pre> <pre><code>{\n    \"RequestedQuotas\": [\n        {\n            \"Id\": \"EXAMPLE_ACCESS_KEY_ID\",\n            \"ServiceCode\": \"ec2\",\n            \"ServiceName\": \"Amazon Elastic Compute Cloud (Amazon EC2)\",\n            \"QuotaCode\": \"L-0263D0A3\",\n            \"QuotaName\": \"EC2-VPC Elastic IPs\",\n            \"DesiredValue\": 20.0,\n            \"Status\": \"PENDING\",\n            \"Created\": \"2025-02-09T02:27:01.573000-05:00\",\n            \"LastUpdated\": \"2025-02-09T02:27:01.956000-05:00\",\n            \"Requester\": \"{\\\"accountId\\\":\\\"EXAMPLE_ACCESS_KEY_ID\\\",\\\"callerArn\\\":\\\"arn:aws:iam::EXAMPLE_ACCESS_KEY_ID:user/nchase\\\"}\",\n            \"QuotaArn\": \"arn:aws:servicequotas:EXAMPLE_AWS_REGION:EXAMPLE_ACCESS_KEY_ID:ec2/L-0263D0A3\",\n            \"GlobalQuota\": false,\n            \"Unit\": \"None\",\n            \"QuotaRequestedAtLevel\": \"ACCOUNT\"\n        }\n    ]\n}\n</code></pre></p> </li> <li> <p>Create the IAM user. </p> <p>The actual <code>user-name</code> parameter is arbitrary; you can specify it as anything you like:</p> <p><pre><code>aws iam create-user --user-name k0rdentUser\n</code></pre> <pre><code>{\n  \"User\": {\n    \"Path\": \"/\",\n    \"UserName\": \"k0rdentUser\",\n    \"UserId\": \"EXAMPLE_USER_ID\",\n    \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentUser\",\n    \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Assign the relevant policies</p> <p>You'll need to assign the following policies to the user you just created:</p> <p><pre><code>control-plane.cluster-api-provider-aws.sigs.k8s.io\ncontrollers.cluster-api-provider-aws.sigs.k8s.io\nnodes.cluster-api-provider-aws.sigs.k8s.io\ncontrollers-eks.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> To do that, you'll need the ARNs for each policy.  You can get them with the <code>list-policies</code> command, as in:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> <pre><code>{\n  \"Policies\": [\n    {\n      \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 2,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    },\n    {\n      \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 3,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n    },\n    {\n      \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 3,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    },\n    {\n      \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 2,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    }\n  ]\n}\n</code></pre></p> <p>Now you can add the policies using the <code>attach-user-policy</code> command and the ARNs you retrieved in the previous step:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> </li> <li> <p>Create an access key and secret</p> <p>To access AWS as this new user, you'll need to create an access key:</p> <p><pre><code>aws iam create-access-key --user-name k0rdentUser \n</code></pre> <pre><code>{\n  \"AccessKey\": {\n    \"UserName\": \"k0rdentUser\",\n    \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n    \"Status\": \"Active\",\n    \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n    \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Create the IAM Credentials <code>Secret</code> on the k0rdent Management Cluster</p> <p>Create a YAML file called <code>aws-cluster-identity-secret.yaml</code> and add the following text, including the <code>AccessKeyId</code> and <code>SecretAccessKey</code> you created in the previous step:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n  SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Apply the YAML to your cluster, making sure to add it to the namespace where the CAPA provider is running (currently <code>kcm-system</code>) so the controller can read it:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>AWSClusterStaticIdentity</code></p> <p>Create the <code>AWSClusterStaticIdentity</code> object in a file named <code>aws-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that the <code>secretRef</code> references the <code>Secret</code> you created in the previous step.</p> <p>Apply the YAML to your cluster, again adding it to the <code>kcm-system</code> namespace.</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre> </li> <li> <p>Create the Cluster Identity resource template <code>ConfigMap</code></p> <p>Now we create Cluster Identity resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>aws-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-cluster-identity-resource-template\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\n</code></pre> <p>Note that <code>ConfigMap</code> is empty, this is expected, we don't need to template any object inside child cluster(s), but we can use that object in the future if need arises.</p> <p>Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-resource-template.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>Credential</code></p> <p>Finally, create the KCM <code>Credential</code> object, making sure to reference the <code>AWSClusterStaticIdentity</code> you just created:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre> </li> <li> <p>Deploy a cluster</p> <p>Make sure everything is configured properly by creating a <code>ClusterDeployment</code>. Start with a YAML file specifying the <code>ClusterDeployment</code>, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-16\n  credential: aws-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n      rootVolumeSize: 32\n    worker:\n      instanceType: t3.small\n      rootVolumeSize: 32\n</code></pre> </p> <p>Note</p> <ul> <li>You're giving it an arbitrary name in <code>.metadata.name</code> (<code>my-aws-clusterdeployment1</code>)</li> <li>You're referencing the credential you created in the previous step, <code>aws-cluster-identity-cred</code>. This enables you to set up a system where users can take advantage of having access to the credentials to the AWS account without actually having those credentials in hand.</li> <li>You need to choose a template to use for the cluster, in this case <code>aws-standalone-cp-1-0-16</code>. You can get a list of available templates using:</li> </ul> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-5-0           true\naws-eks-1-0-3                   true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\nopenstack-standalone-cp-1-0-17   true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\n</code></pre> Apply the YAML to your management cluster: <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> As before, there will be a delay as the cluster finishes provisioning. Follow the provisioning process with: <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> When the cluster is <code>Ready</code>, you can access it via the kubeconfig, as in: <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre></p> </li> <li> <p>Cleanup</p> <p>When you've established that it's working properly, you can delete the managed cluster and its AWS objects:</p> <pre><code>kubectl delete clusterdeployments my-aws-clusterdeployment1 \n</code></pre> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/azure/","title":"Azure","text":"<p>Standalone clusters can be deployed on Azure instances. Follow these steps to make Azure clusters available to your users:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>The Azure CLI</p> <p>The Azure CLI (<code>az</code>) is required to interact with Azure resources. You can install it on Ubuntu as follows:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre> </li> <li> <p>Log in to Azure</p> <p>Run the <code>az login</code> command to authenticate your session with Azure:</p> <pre><code>az login\n</code></pre> <p>Make sure that the account you're using has at least one active subscription.</p> </li> <li> <p>Register resource providers</p> <p>In order for k0rdent to deploy and manage clusters, it needs to be able to work with Azure resources such as  compute, network, and identity. Make sure the subscription you're using has the following resource providers registered:</p> <pre><code>Microsoft.Compute\nMicrosoft.Network\nMicrosoft.ContainerService\nMicrosoft.ManagedIdentity\nMicrosoft.Authorization\n</code></pre> <p>To register these providers, run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> </li> <li> <p>Find Your Subscription ID</p> <p>Creating a child cluster requires a structure of credentials that link to user identities on the provider system without exposing the actual username and password to users. You can find more information on k0rdent  Credentials, but for Azure, this involves creating an <code>AzureClusterIdentity</code> and a  Service Principal (SP) to let CAPZ (Cluster API Azure) communicate with the cloud provider. </p> <p>On Azure, the lowest level of this hierarchy is the subscription, which ties to your billing information for Azure. Your Azure user must have at least one subscription for you to use it with k0rdent, so if you're working with a new account make sure to create a new subscription with billing information before you start.</p> <p>To get the information you need, list all your Azure subscriptions: </p> <p><pre><code>az account list -o table\n</code></pre> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID        TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre></p> <p>Make note of the <code>SubscriptionId</code> for the subscription you want to use.</p> </li> <li> <p>Create a Service Principal (SP)</p> <p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. Create the Service Principal, making sure to replace  with the <code>SubscriptionId</code> from step 1. <p><pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt;\"\n</code></pre> <pre><code>{\n\"appId\": \"SP_APP_ID_SP_APP_ID\",\n\"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n\"password\": \"SP_PASSWORD_SP_PASSWORD\",\n\"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> Note that this information provides access to your Azure account, so make sure to treat these strings  like passwords. Do not share them or check them into a repository.</p> <li> <p>Use the password to create a <code>Secret</code> object</p> <p>The <code>Secret</code> stores the <code>clientSecret</code> (password) from the Service Principal. Save the <code>Secret</code> YAML in a file called <code>azure-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: &lt;SP_PASSWORD_SP_PASSWORD&gt; # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>AzureClusterIdentity</code> objects</p> <p>The <code>AzureClusterIdentity</code> object defines the credentials CAPZ uses to manage Azure resources.  It references the <code>Secret</code> you just created, so make sure that <code>.spec.clientSecret.name</code> matches  the name of that <code>Secret</code>.</p> <p>Save the following YAML into a file named <code>azure-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n    k0rdent.mirantis.com/component: \"kcm\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: &lt;SP_APP_ID_SP_APP_ID&gt; # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: &lt;SP_TENANT_SP_TENANT&gt; # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p> </li> <li> <p>Create the k0rdent <code>Credential</code> Object</p> <p>Create the YAML for the specification of the <code>Credential</code> and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>You're referencing the <code>AzureClusterIdentity</code> object you just created, so make sure that <code>.spec.name</code> matches  <code>.metadata.name</code> of that object. Also, note that while the overall object's <code>kind</code> is <code>Credential</code>, the  <code>.spec.identityRef.kind</code> must be <code>AzureClusterIdentity</code> to match that object.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre></p> </li> <li> <p>Create the <code>ConfigMap</code> resource-template Object</p> <p>Create a YAML with the specification of our resource-template and save it as <code>azure-cluster-identity-resource-template.yaml</code></p> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: azure-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    {{- $subnetName := \"\" -}}\n    {{- $securityGroupName := \"\" -}}\n    {{- $routeTableName := \"\" -}}\n    {{- range $cluster.spec.networkSpec.subnets -}}\n      {{- if eq .role \"node\" -}}\n        {{- $subnetName = .name -}}\n        {{- $securityGroupName = .securityGroup.name -}}\n        {{- $routeTableName = .routeTable.name -}}\n        {{- break -}}\n      {{- end -}}\n    {{- end -}}\n    {{- $cloudConfig := dict\n      \"aadClientId\" $identity.spec.clientID\n      \"aadClientSecret\" (index $secret.data \"clientSecret\" | b64dec)\n      \"cloud\" $cluster.spec.azureEnvironment\n      \"loadBalancerName\" \"\"\n      \"loadBalancerSku\" \"Standard\"\n      \"location\" $cluster.spec.location\n      \"maximumLoadBalancerRuleCount\" 250\n      \"resourceGroup\" $cluster.spec.resourceGroup\n      \"routeTableName\" $routeTableName\n      \"securityGroupName\" $securityGroupName\n      \"securityGroupResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n      \"subnetName\" $subnetName\n      \"subscriptionId\" $cluster.spec.subscriptionID\n      \"tenantId\" $identity.spec.tenantID\n      \"useInstanceMetadata\" true\n      \"useManagedIdentityExtension\" false\n      \"vmType\" \"vmss\"\n      \"vnetName\" $cluster.spec.networkSpec.vnet.name\n      \"vnetResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n    -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: azure-cloud-provider\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-config: {{ $cloudConfig | toJson | b64enc }}\n</code></pre> Object name needs to be exactly <code>azure-cluster-identity-resource-template.yaml</code>, <code>AzureClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-resource-template.yaml\n</code></pre> <pre><code>configmap/azure-cluster-identity-resource-template created\n</code></pre></p> </li> <p>Now you're ready to deploy the cluster.</p> <ol> <li> <p>Create a <code>ClusterDeployment</code></p> <p>To test the configuration, deploy a child cluster by following these steps:</p> <p>First get a list of available locations/regions:</p> <p><pre><code>az account list-locations -o table\n</code></pre> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n. . .\n</code></pre></p> <p>Make note of the location you want to use, such as <code>eastus</code>.</p> <p>To create the actual child cluster, create a <code>ClusterDeployment</code> that references the appropriate template as well as the location, credentials, and <code>subscriptionId</code>.</p> <p>You can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-5-0           true\naws-eks-1-0-3                   true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\nopenstack-standalone-cp-1-0-17   true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\n</code></pre></p> <p>Create the yaml:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-1-0-17\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    subscriptionID: &lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt; # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre></p> <p>Note that although the <code>ClusterDeployment</code> object has been created, there will be a delay as actual Azure instances are provisioned and added to the cluster. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre> <p>If the provisioning process continues for a more than a few minutes, check to make sure k0rdent isn't trying to exceed your quotas. If you are near the top of your quotas, requesting an increase can \"unstick\" the provisioning process.</p> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up Azure resources, delete the child cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/gcp/","title":"GCP","text":"<p>Available starting in k0rdent 0.2.0 and later</p> <p>Standalone clusters can be deployed on GCP instances. Follow these steps to make GCP clusters available to your users:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>The gcloud CLI</p> <p>The gcloud CLI (<code>gcloud</code>) is required to interact with GCP resources. You can install it by following  the Install the gcloud CLI instructions.</p> </li> <li> <p>Log in to GCP</p> <p>Authenticate your session with GCP:</p> <pre><code>gcloud auth login\n</code></pre> </li> <li> <p>Enable the required API for your Google Cloud project (if it wasn't previously enabled)</p> <p>The proper API to enable depends on how you plan to deploy k0rdent:</p> <ul> <li>Standalone/hosted GCP clusters: Enable the <code>Compute Engine API</code>.</li> <li>GKE clusters: Enable the <code>Compute Engine API</code> and the <code>Kubernetes Engine API</code>.</li> </ul> <p>To enable <code>Compute Engine API</code> using the Google Cloud Console (UI):</p> <ol> <li>Go to the Google Cloud Console.</li> <li>Select your project. </li> <li>In the top navigation bar, click on the project selector (drop-down menu). </li> <li>Choose the project where you want to enable the <code>Compute Engine API</code>. </li> <li>Navigate to the <code>API Library</code> (click on the Navigation Menu in the upper-left corner and select <code>APIs &amp; Services</code> \u2192 <code>Library</code>). </li> <li>Search for Compute Engine API (in the API Library, type <code>Compute Engine API</code> in the search bar and press Enter).</li> <li>Enable the API. Click on <code>Compute Engine API</code> from the search results. Click the <code>Enable</code> button.</li> </ol> </li> <li> <p>Create a GCP Service Account</p> <p>Note</p> <p> Skip this step if the Service Account already configured</p> <p>Follow the GCP Service Account creation guide and create a new service account with <code>Editor</code> permissions. If you have plans to deploy <code>GKE</code>, the Service Account will also need the <code>iam.serviceAccountTokenCreator</code> role.</p> </li> <li> <p>Generate a JSON Key for the GCP Service Account</p> <p>Note</p> <p> Skip this step if you're going to use an existing key</p> <p>Follow the Create a service account key guide and create a new key with the JSON key type.</p> <p>A JSON file will automatically download to your computer. Keep it somewhere safe.</p> <p>The example of the JSON file:</p> <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"GCP_PROJECT_ID\",\n  \"private_key_id\": \"GCP_PRIVATE_KEY_ID\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nGCP_PRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"name@project_id.iam.gserviceaccount.com\",\n  \"client_id\": \"GCP_CLIENT_ID\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/user%40project_id.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre> </li> <li> <p>Create a <code>Secret</code> object</p> <p>Create a <code>Secret</code> object that stores the credentials under <code>data</code> section. Create a YAML file called <code>gcp-cluster-identity-secret.yaml</code>, as follows, inserting the base64-encoded GCP credentials (represented by the placeholder <code>GCP_B64ENCODED_CREDENTIALS</code> below) that you get on the previous step. To get base64 encoded credentials, run:</p> <pre><code>cat &lt;gcpJSONCredentialsFileName&gt; | base64 -w 0\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: gcp-cloud-sa\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ndata:\n  # the secret key should always equal `credentials`\n  credentials: GCP_B64ENCODED_CREDENTIALS\ntype: Opaque\n</code></pre> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f gcp-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the k0rdent <code>Credential</code> Object</p> <p>Create a YAML document with the specification of the <code>Credential</code> and save it as <code>gcp-cluster-identity-cred.yaml</code>.</p> <p>Note that <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>Secret</code> object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: gcp-credential\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: gcp-cloud-sa\n    namespace: kcm-system\n</code></pre> <pre><code>kubectl apply -f gcp-cluster-identity-cred.yaml\n</code></pre> <p>You should see output of:</p> <pre><code>credential.k0rdent.mirantis.com/gcp-cluster-identity-cred created\n</code></pre> </li> <li> <p>Create the <code>ConfigMap</code> resource-template Object</p> <p>Create a YAML with the specification of our resource-template and save it as <code>gcp-cloud-sa-resource-template.yaml</code></p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: gcp-cloud-sa-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $secret := (getResource \"InfrastructureProviderIdentity\") -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: gcp-cloud-sa\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-sa.json: {{ index $secret \"data\" \"credentials\" }}\n</code></pre> <p>Object name needs to be exactly <code>gcp-cloud-sa-resource-template</code> (credentials <code>Secret</code> object name + <code>-resource-template</code> string suffix).</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f gcp-cloud-sa-resource-template.yaml\n</code></pre> <pre><code>configmap/gcp-cloud-sa-resource-template created\n</code></pre></p> </li> </ol> <p>Now you're ready to deploy the cluster.</p> <ol> <li> <p>Create a <code>ClusterDeployment</code></p> <p>To test the configuration, deploy a child cluster by following these steps: First get a list of available regions:</p> <pre><code>gcloud compute regions list\n</code></pre> <p>You'll see output like this:</p> <pre><code>NAME                     CPUS    DISKS_GB  ADDRESSES  RESERVED_ADDRESSES  STATUS  TURNDOWN_DATE\nafrica-south1            0/300   0/102400  0/575      0/175               UP\nasia-east1               0/3000  0/102400  0/575      0/175               UP\nasia-east2               0/1500  0/102400  0/575      0/175               UP\nasia-northeast1          0/1500  0/102400  0/575      0/175               UP\nasia-northeast2          0/750   0/102400  0/575      0/175               UP\n...\n</code></pre> <p>Make note of the region you want to use, such as <code>us-east4</code>.</p> <p>To create the actual child cluster, create a <code>ClusterDeployment</code> that references the appropriate template  as well as the region, credentials, and cluster configuration.</p> <p>You can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-5-0           true\naws-eks-1-0-3                   true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\nopenstack-standalone-cp-1-0-17   true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\ngcp-standalone-cp-1-0-15     true\ngcp-gke-1-0-6     true\n</code></pre></p> <p>Create the YAML for the <code>ClusterDeployment</code> and save it as my-gcp-clusterdeployment1.yaml:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-gcp-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: gcp-standalone-cp-1-0-15\n  credential: gcp-credential\n  config:\n    project: PROJECT_NAME # Your project name\n    region: \"GCP_REGION\" # Select your desired GCP region (find it via `gcloud compute regions list`)\n    network:\n      name: default # Select your desired network name (select new network name to create or find it via `gcloud compute networks list --format=\"value(name)\"`)\n    controlPlane:\n      instanceType: n1-standard-2 # Select your desired instance type (find it via `gcloud compute machine-types list | grep REGION`)\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213 # Select image (find it via `gcloud compute images list --uri`)\n      publicIP: true\n    worker:\n      instanceType: n1-standard-2 \n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n</code></pre> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-gcp-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-gcp-clusterdeployment1 created\n</code></pre></p> <p>Note that although the <code>ClusterDeployment</code> object has been created, there will be a delay as actual GCP instances  are provisioned and added to the cluster. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-gcp-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-gcp-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-gcp-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-gcp-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up GCP resources, delete the child cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-gcp-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments my-gcp-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-gcp-clusterdeployment1\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/openstack/","title":"OpenStack","text":"<p>k0rdent can deploy child clusters on OpenStack virtual machines. Follow these steps to configure and deploy OpenStack clusters for your users:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>OpenStack CLI (optional)</p> <p>If you plan to access OpenStack directly, go ahead and  install the OpenStack CLI.</p> </li> <li> <p>Configure the OpenStack Application Credential</p> <p>The exported list of variables should include:</p> <pre><code>OS_AUTH_URL\nOS_APPLICATION_CREDENTIAL_ID\nOS_APPLICATION_CREDENTIAL_SECRET\nOS_REGION_NAME\nOS_INTERFACE\nOS_IDENTITY_API_VERSION\nOS_AUTH_TYPE\n</code></pre> <p>While it's possible to use a username and password instead of the Application Credential \u2014 adjust your YAML accordingly \u2014 an Application Credential is strongly recommended because it limits scope and improves security over a raw username/password approach.</p> </li> <li> <p>Create the OpenStack Credentials Secret</p> <p>Create a Kubernetes <code>Secret</code> containing the <code>clouds.yaml</code> that defines your OpenStack environment, substituting real values where appropriate. Save this as <code>openstack-cloud-config.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openstack-cloud-config\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clouds.yaml: |\n    clouds:\n      openstack:\n        auth:\n          auth_url: &lt;OS_AUTH_URL&gt;\n          application_credential_id: &lt;OS_APPLICATION_CREDENTIAL_ID&gt;\n          application_credential_secret: &lt;OS_APPLICATION_CREDENTIAL_SECRET&gt;\n        region_name: &lt;OS_REGION_NAME&gt;\n        interface: &lt;OS_INTERFACE&gt;\n        identity_api_version: &lt;OS_IDENTITY_API_VERSION&gt;\n        auth_type: &lt;OS_AUTH_TYPE&gt;\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cloud-config.yaml\n</code></pre> </li> <li> <p>Create the k0rdent Credential Object</p> <p>Next, define a <code>Credential</code> that references the <code>Secret</code> from the previous step. Save this as <code>openstack-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: openstack-cluster-identity-cred\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"  \nspec:\n  description: \"OpenStack credentials\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: openstack-cloud-config\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-cred.yaml\n</code></pre> <p>Note that <code>.spec.identityRef.name</code> must match the <code>Secret</code> you created in the previous step, and  <code>.spec.identityRef.namespace</code> must be the same as the one that includes the <code>Secret</code> (<code>kcm-system</code>).</p> </li> <li> <p>Create the ConfigMap resource-template object</p> <p>Create a YAML file with the specification of the resource-template and save it as <code>openstack-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: openstack-cloud-config-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n\n    {{- $clouds := fromYaml (index $identity \"data\" \"clouds.yaml\" | b64dec) -}}\n    {{- if not $clouds }}\n      {{ fail \"failed to decode clouds.yaml\" }}\n    {{ end -}}\n\n    {{- $openstack := index $clouds \"clouds\" \"openstack\" -}}\n\n    {{- if not (hasKey $openstack \"auth\") }}\n      {{ fail \"auth key not found in openstack config\" }}\n    {{- end }}\n    {{- $auth := index $openstack \"auth\" -}}\n\n    {{- $auth_url := index $auth \"auth_url\" -}}\n    {{- $app_cred_id := index $auth \"application_credential_id\" -}}\n    {{- $app_cred_name := index $auth \"application_credential_name\" -}}\n    {{- $app_cred_secret := index $auth \"application_credential_secret\" -}}\n\n    {{- $network_id := $cluster.status.externalNetwork.id -}}\n    {{- $network_name := $cluster.status.externalNetwork.name -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: openstack-cloud-config\n      namespace: kube-system\n    type: Opaque\n    stringData:\n      cloud.conf: |\n        [Global]\n        auth-url=\"{{ $auth_url }}\"\n\n        {{- if $app_cred_id }}\n        application-credential-id=\"{{ $app_cred_id }}\"\n        {{- end }}\n\n        {{- if $app_cred_name }}\n        application-credential-name=\"{{ $app_cred_name }}\"\n        {{- end }}\n\n        {{- if $app_cred_secret }}\n        application-credential-secret=\"{{ $app_cred_secret }}\"\n        {{- end }}\n\n        {{- if and (not $app_cred_id) (not $app_cred_secret) }}\n        username=\"{{ index $openstack \"username\" }}\"\n        password=\"{{ index $openstack \"password\" }}\"\n        {{- end }}\n        region=\"{{ index $openstack \"region_name\" }}\"\n\n        [LoadBalancer]\n        {{- if $network_id }}\n        floating-network-id=\"{{ $network_id }}\"\n        {{- end }}\n\n        [Networking]\n        {{- if $network_name }}\n        public-network-name=\"{{ $network_name }}\"\n        {{- end }}\n</code></pre> <p>Object needs to be named <code>openstack-cluster-identity-resource-template.yaml</code>, <code>OpenStackClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-resource-template.yaml\n</code></pre> </li> <li> <p>Create Your First Child Cluster</p> <p>To test the configuration, create a YAML file with the specification of your Managed Cluster and save it as <code>my-openstack-cluster-deployment.yaml</code>.  Note that you can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-5-0           true\naws-eks-1-0-3                   true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\nopenstack-standalone-cp-1-0-17   true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> should look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-1-0-17\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    externalNetwork:\n      filter:\n        name: \"public\"\n    authURL: ${OS_AUTH_URL}\n    identityRef:\n      name: \"openstack-cloud-config\"\n      cloudName: \"openstack\"\n      region: ${OS_REGION_NAME}\n</code></pre> <p>Note</p> <p> When deploying clusters with <code>openstack-standalone-cp</code> template version <code>1-0-12</code> or newer, the <code>identityRef.name</code> parameter is ignored and can be omitted. For older template versions, this parameter is required and must match the name of the <code>Secret</code> containing the <code>clouds.yaml</code> configuration.</p> <p>You can adjust <code>flavor</code>, <code>image name</code>, <code>region name</code>, and <code>authURL</code> to match your OpenStack environment. For more information about the configuration options, see the OpenStack Template Parameters Reference.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-openstack-cluster-deployment.yaml\n</code></pre> <p>This will trigger the provisioning process where k0rdent will create a bunch of OpenStack resources such as OpenStackCluster, OpenStackMachineTemplate, OpenStackMachineDeployment, etc. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-openstack-cluster-deployment --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, just like any other Kubernetes cluster:</p> <pre><code>kubectl -n kcm-system get secret my-openstack-cluster-deployment-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-openstack-cluster-deployment-kubeconfig.kubeconfig\nKUBECONFIG=\"my-openstack-cluster-deployment-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up OpenStack resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-openstack-cluster-deployment   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments my-openstack-cluster-deployment -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-openstack-cluster-deployment\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/vmware/","title":"vSphere","text":"<p>To enable users to deploy child clusers on vSphere, follow these steps:</p> <ol> <li> <p>Create a k0rdent management cluster</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running, as well as a local install of <code>kubectl</code>.</p> </li> <li> <p>Install a vSphere instance version <code>6.7.0</code> or higher.</p> </li> <li> <p>Create a vSphere account with appropriate privileges</p> <p>To function properly, the user assigned to the vSphere Provider should be able to manipulate vSphere resources. The user should have the following  required privileges:</p> <pre><code>Virtual machine: Full permissions are required\nNetwork: Assign network is sufficient\nDatastore: The user should be able to manipulate virtual machine files and metadata\n</code></pre> <p>In addition to that, specific CSI driver permissions are required. See the official doc for more information on CSI-specific permissions.</p> </li> <li> <p>Image template</p> <p>You can use pre-built image templates from the CAPV project or build your own.</p> <p>When building your own image, make sure that VMware tools and cloud-init are installed and properly configured.</p> <p>You can follow the official open-vm-tools guide on how to correctly install VMware tools.</p> <p>When setting up cloud-init, you can refer to the official docs and specifically the VMware datasource docs for extended information regarding cloud-init on vSphere.</p> </li> <li> <p>vSphere network</p> <p>When creating a network, make sure that it has the DHCP service.</p> <p>Also, ensure that part of your network is out of the DHCP range (for example, the network <code>172.16.0.0/24</code> should have a DHCP range of <code>172.16.0.100-172.16.0.254</code> only) so that LoadBalancer services will not create any IP conflicts in the network.</p> </li> <li> <p>vSphere Credentials</p> <p>To enable k0rdent to access vSphere resources, create the appropriate credentials objects. For a full explanation of how <code>Credential</code> objects work, see the main Credentials chapter, but for now, follow these steps:</p> <p>Create a <code>Secret</code> object with the username and password.</p> <p>Warning</p> <p> The <code>username</code> parameter should be set to the vCenter username along with the domain name. For example, <code>userName@domainName</code>. If you don't specify the domain name for active directory users, the vSphere Container Storage Plug-in will not function properly.</p> <p>The <code>Secret</code> stores the username and password for your vSphere instance. Save the <code>Secret</code> YAML in a file named <code>vsphere-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  username: &lt;USERNAME&gt;\n  password: &lt;PASSWORD&gt;\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>VSphereClusterIdentity</code> Object</p> <p>The <code>VSphereClusterIdentity</code> object defines the credentials CAPV will use to manage vSphere resources.</p> <p>Save the <code>VSphereClusterIdentity</code> YAML into a file named <code>vsphere-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>The <code>VSphereClusterIdentity</code> object references the <code>Secret</code> you created in the previous step, so <code>.spec.secretName</code>  needs to match the <code>.metadata.name</code> for the <code>Secret</code>.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity.yaml\n</code></pre> </li> <li> <p>Create the <code>Credential</code> Object</p> <p>Create a YAML with the specification of our credential and save it as <code>vsphere-cluster-identity-cred.yaml</code></p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: vsphere-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: VSphereClusterIdentity\n    name: vsphere-cluster-identity\n    namespace: kcm-system\n</code></pre> Again, <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>VSphereClusterIdentity</code> object you just created.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-cred.yaml\n</code></pre> </li> <li> <p>Create the <code>ConfigMap</code> resource-template Object</p> <p>Create a YAML with the specification of our resource-template and save it as <code>vsphere-cluster-identity-resource-template.yaml</code></p> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vsphere-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: vsphere-cloud-secret\n      namespace: kube-system\n    type: Opaque\n    data:\n      {{ printf \"%s.username\" $cluster.spec.server }}: {{ index $secret.data \"username\" }}\n      {{ printf \"%s.password\" $cluster.spec.server }}: {{ index $secret.data \"password\" }}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: vcenter-config-secret\n      namespace: kube-system\n    type: Opaque\n    stringData:\n      csi-vsphere.conf: |\n        [Global]\n        cluster-id = \"{{ $cluster.metadata.name }}\"\n\n        [VirtualCenter \"{{ $cluster.spec.server }}\"]\n        insecure-flag = \"true\"\n        user = \"{{ index $secret.data \"username\" | b64dec }}\"\n        password = \"{{ index $secret.data \"password\" | b64dec }}\"\n        port = \"443\"\n        datacenters = ${VSPHERE_DATACENTER}\n    ---\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: cloud-config\n      namespace: kube-system\n    data:\n      vsphere.conf: |\n        global:\n          insecureFlag: true\n          port: 443\n          secretName: vsphere-cloud-secret\n          secretNamespace: kube-system\n        labels:\n          region: k8s-region\n          zone: k8s-zone\n        vcenter:\n          {{ $cluster.spec.server }}:\n            datacenters:\n              - ${VSPHERE_DATACENTER}\n            server: {{ $cluster.spec.server }}\n</code></pre> Object name needs to be exactly <code>vsphere-cluster-identity-resource-template</code>, <code>VSphereClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-resource-template.yaml\n</code></pre> </li> <li> <p>Create your first Cluster Deployment</p> <p>Test the configuration by deploying a cluster. Create a YAML document with the specification of your Cluster Deployment and save it as <code>my-vsphere-clusterdeployment1.yaml</code>.</p> <p>You can get a list of available templates:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-5-0           true\naws-eks-1-0-3                   true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\nopenstack-standalone-cp-1-0-17   true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> YAML file should look something like this. Make sure to replace the placeholders with your specific information:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-vsphere-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: vsphere-standalone-cp-1-0-15\n  credential: vsphere-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    controlPlaneNumber: 1\n    workersNumber: 1\n    vsphere:\n      server: &lt;VSPHERE_SERVER&gt;\n      thumbprint: &lt;VSPHERE_THUMBPRINT&gt;\n      datacenter: &lt;VSPHERE_DATACENTER&gt;\n      datastore: &lt;VSPHERE_DATASTORE&gt;\n      resourcePool: &lt;VSPHERE_RESOURCEPOOL&gt;\n      folder: &lt;VSPHERE_FOLDER&gt;\n      username: ${VSPHERE_USER}\n      password: ${VSPHERE_PASSWORD}\n    controlPlaneEndpointIP: &lt;VSPHERE_CONTROL_PLANE_ENDPOINT&gt;\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      rootVolumeSize: 50\n      cpus: 4\n      memory: 4096\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      rootVolumeSize: 50\n      cpus: 4\n      memory: 4096\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n</code></pre> <p>For more information about the available configuration options, see the vSphere Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-vsphere-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-vsphere-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-vsphere-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To delete the provisioned cluster and free consumed vSphere resources run:</p> <pre><code>kubectl -n kcm-system delete cluster my-vsphere-clusterdeployment1\n</code></pre> </li> </ol>"},{"location":"admin/kof/","title":"k0rdent Observability &amp; FinOps (KOF)","text":"<p>Running multiple clusters means you end up wrestling with half a dozen monitoring and cost tools, all with their own configs and upgrade headaches. k0rdent Observability &amp; FinOps (KOF) saves you from duct-taping that stack together by bundling metrics, logs, traces, and cost tracking into one setup. It's built to roll out the same way everywhere, so you don\u2019t have to reinvent the wheel every time you spin up a new cluster.</p>"},{"location":"admin/kof/#kof-makes-sure-every-cluster-is-observable-and-accountable-for-cost","title":"KOF makes sure every cluster is observable and accountable for cost","text":"<p>k0rdent estates can be large. They consist of a management cluster, potentially running many child (workload) clusters, often spread across multiple datacenters, regions, and clouds. In sthese kinds of environments, you need to both observe what clusters and workloads are doing (using metrics, logs, traces) and be accountable for what they cost (using allocation and trends).  </p> <p>KOF gives k0rdent platform engineering teams the ability to guarantee that observability and cost management are deployed with every cluster they create, and then keep them consistent and automatically lifecycle-managed. </p>"},{"location":"admin/kof/#without-kof-you-would-be-forced-to-integrate-and-maintain-multiple-systems","title":"Without KOF you would be forced to integrate and maintain multiple systems","text":"<p>KOF is optional, but if you were to build this yourself, you would need to select, integrate, and maintain separate systems for metrics, logs, tracing, dashboards, and cost visibility. At scale, this arrangement introduces persistent difficulties:</p> <ul> <li> <p>Component selection and versioning. Each subsystem has its own CRDs, operators, and configuration. Keeping them consistent across many clusters is complex. Version skew and CRD drift cause brittle rollouts and difficult upgrades.  (See OpenTelemetry Collector on Kubernetes.)</p> </li> <li> <p>Cross-cluster aggregation. To query estate-wide, you need to build federation or remote-write topologies. These can work, but are fragile and add operational overhead. (See Prometheus federation.)</p> </li> <li> <p>Retention and placement. You need to plan where long-term metrics and logs live (per-cluster, regional, or dedicated) and engineer routing, storage, and query paths. </p> </li> <li> <p>Access, identity, and network. SSO, RBAC, mTLS, and inter-cluster routing must be implemented and maintained consistently across the estate.</p> </li> </ul> <p>KOF standardizes these decisions so they are consistent, repeatable, and auditable in k0rdent.</p>"},{"location":"admin/kof/#kof-delivers-unified-observability-and-cost-management-across-the-estate","title":"KOF delivers unified observability and cost management across the estate","text":"<p>By packaging observability and FinOps as a single subsystem, KOF connects operational signals and cost signals:</p> <ul> <li>Estate-wide visibility. Every cluster participates, and data is aggregated centrally.  </li> <li>Actionable costs. Workload and namespace spend is visible in the same dashboards as performance indicators.  </li> <li>Standardized retention. Documented options define where data lives and how long it is kept.  </li> <li>Lifecycle management. KOF is delivered as charts and operators, which can be consistently deployed and upgraded across the estate.  </li> <li>Compliance and audit. Central policy ensures retention, RBAC, and secure transport are enforced everywhere.</li> </ul> <p>Performance issues and cost spikes usually come from the same workloads, so you can't separate one from the other. When observability data and cost data live in the same place, it\u2019s easier to spot the cause and fix both at once.</p>"},{"location":"admin/kof/#kof-consists-of-well-established-open-source-components-wired-for-multi-cluster-use","title":"KOF consists of well-established open source components wired for multi-cluster use","text":"<ul> <li>Metrics: VictoriaMetrics with vmcluster and vmauth.  </li> <li>Logs: VictoriaLogs.  </li> <li>Tracing: Jaeger with OpenTelemetry.  </li> <li>Cost: OpenCost.  </li> <li>Dashboards: Grafana managed by grafana-operator.  </li> <li>Aggregation: Promxy for Prometheus-compatible fan-out.  </li> <li>Control: kof-operators for lifecycle and configuration.  </li> </ul> <p>KOF is deployed through Helm charts and MultiClusterServices: <code>kof-operators</code>, <code>kof-mothership</code>, <code>kof-storage</code>, <code>kof-collectors</code>, with optional <code>kof-istio</code>, plus per-role services such as <code>kof-child</code> and <code>kof-regional</code>. (See KOF architecture.)</p>"},{"location":"admin/kof/#kof-in-action-a-day-in-the-life-and-a-sudden-cost-spike","title":"KOF in Action: A Day in the Life (and A Sudden Cost Spike)","text":"<p>It\u2019s Monday morning. Finance notices that last week\u2019s cloud bill is 20% higher than forecast. The CFO wants answers before the leadership meeting tomorrow.</p> <p>Without KOF, this would trigger days of Slack threads and finger-pointing. With KOF, here\u2019s what happens instead:</p>"},{"location":"admin/kof/#step-1-finance-spots-the-anomaly","title":"Step 1: Finance Spots the Anomaly","text":"<p>The Finance team opens the KOF FinOps dashboard in Grafana. They see that most of the increase came from the US-East regional cluster, specifically from workloads tagged to the \u201crecommendation-engine\u201d project.</p> <p>What used to be a mystery bill is now an actionable insight.</p>"},{"location":"admin/kof/#step-2-devops-traces-the-issue","title":"Step 2: DevOps Traces the Issue","text":"<p>The DevOps team takes over. They drill into the same dashboard and pull up OpenCost metrics for that namespace. One service shows a sharp increase in CPU and memory usage starting on Friday.</p> <p>To understand why, they pivot into Jaeger traces. The culprit emerges: a new API endpoint is generating far more queries than expected.</p> <p>What used to take days of log-diving is discovered in minutes.</p>"},{"location":"admin/kof/#step-3-platform-engineers-validate-the-fix","title":"Step 3: Platform Engineers Validate the Fix","text":"<p>The Platform engineering team checks KOF\u2019s cluster overview dashboards to confirm whether this workload is affecting other services. They see elevated resource usage but no system-wide instability.</p> <p>They work with DevOps to roll out a fix via GitOps: right-sizing the deployment and setting an alert for future usage spikes.</p> <p>What used to be firefighting is now controlled, repeatable operations.</p>"},{"location":"admin/kof/#step-4-finance-closes-the-loop","title":"Step 4: Finance Closes the Loop","text":"<p>By the end of the day, Finance refreshes the chargeback dashboard. Costs for the \u201crecommendation-engine\u201d namespace are already trending down. The CFO gets a simple, confident answer for tomorrow\u2019s meeting:</p> <p>\"We found the root cause, we fixed it, and costs are back under control.\"</p>"},{"location":"admin/kof/#the-outcome","title":"The Outcome","text":"<ul> <li>Finance: Cost visibility and accountability.</li> <li>DevOps: Faster root cause analysis, no wasted hours in silos.</li> <li>Platform Engineers: A clean GitOps workflow to fix the issue.</li> <li>Leadership: Confidence that teams are in control of performance and costs.</li> </ul> <p>In a single day, KOF turned a vague cost spike into a resolved issue, with every team playing their part in one unified system.</p>"},{"location":"admin/kof/#getting-started","title":"Getting Started","text":"<p>KOF lets you get started with some quick wins. For example, you can easily:</p> <ul> <li>Spin up a single cluster and access Grafana dashboards in under 10 minutes.</li> <li>Attribute cloud costs to teams immediately using OpenCost metrics.</li> <li>Configure alerts for both performance and budget thresholds.</li> <li>Start retaining logs and metrics for compliance right out of the box.</li> </ul>"},{"location":"admin/kof/#kof-architecture-places-collectors-everywhere-and-lets-you-choose-where-to-store-and-aggregate","title":"KOF architecture places collectors everywhere and lets you choose where to store and aggregate","text":"<p>k0rdent defines a management cluster and many child clusters. KOF follows this and adds an optional regional role for storage and aggregation. Placement options are documented and configurable:</p> <ul> <li> <p>Child clusters. Always run collectors (OpenTelemetry, OpenCost) to gather metrics, logs, and traces at the source. This is the \"beach-head\" service that travels with every cluster.</p> </li> <li> <p>Management cluster. Runs Grafana, promxy, operators, and policy. It may also run VictoriaMetrics for its own telemetry and alert evaluation via the <code>kof-mothership</code> chart.</p> </li> <li> <p>Storage and aggregation. The main VictoriaMetrics, VictoriaLogs, and Jaeger stack can run in:  </p> </li> <li>the management cluster (for management data, with Grafana and VM provided by <code>kof-mothership</code>),  </li> <li>a regional KOF deployment (collecting from child clusters in that region), or  </li> <li>a third-party service for selected streams (for example, logs exported to AWS CloudWatch).   See: Storing KOF data.</li> </ul> <p>Data flows from child clusters into the chosen storage or aggregation point. On the management cluster, promxy and Grafana provide the consolidated UI.</p>"},{"location":"admin/kof/#kof-can-be-extended-with-your-own-dashboards-pipelines-and-destinations","title":"KOF can be extended with your own dashboards, pipelines, and destinations","text":"<p>KOF is fully functional out of the box, but you can also add additional capabilities through extensions.</p> <ul> <li>Dashboards and alerts. Add Grafana dashboards and VMRules in Git; k0rdent <code>MultiClusterService</code> distributes them consistently.  </li> <li>Collector pipelines. Extend OpenTelemetry collectors with custom receivers, processors, or exporters.  </li> <li>External destinations. Route specific streams to third-party systems (recipes are included for CloudWatch and others).  </li> </ul> <p>All extensions are managed through the same GitOps lifecycle as the rest of the platform.</p>"},{"location":"admin/kof/#managing-kof-as-code","title":"Managing KOF as Code","text":"<p>We recommend keeping a dedicated Git repository for your KOF deployment, separate from the k0rdent repo itself and separate from application workload repos.</p> <p>That repository should contain the Helm chart directories such as:</p> <pre><code>charts/\n  kof-storage/\n  kof-operators/\n  kof-dashboards/\n</code></pre> <p>With this setup you can: - Track all KOF configuration changes in Git. - Use CI/CD pipelines to run <code>helm upgrade --install</code> against your management and regional clusters. - Manage dashboards, retention policies, and observability settings as code for consistency across environments.</p> <p>Other sections in this documentation, such as Dashboard Lifecycle and Retention assume you have such a repo and a CI/CD pipeline in place.</p>"},{"location":"admin/kof/#diy-stack-vs-kof","title":"DIY Stack vs. KOF","text":"Dimension DIY Stack (Prometheus, Grafana, OpenCost, Jaeger, etc.) KOF Setup &amp; Integration Assemble 4\u20136 separate projects, each with its own configs, upgrades, and quirks. One integrated platform: metrics, logs, traces, and costs shipped together. Scale &amp; Performance Prometheus federation slows at millions of samples; log systems require heavy tuning. VictoriaMetrics/Logs designed for millions of samples/sec, fast queries, and long-term storage. Consistency Dashboards, alerts, and retention vary per cluster; drift happens quickly. GitOps-native: dashboards, alerts, and policies stored in Git and deployed via CI/CD. Troubleshooting Metrics in one tool, logs in another, traces in a third. Correlation is manual. Unified Grafana dashboards: metrics, logs, and traces correlated automatically. Cost Management OpenCost must be bolted on; engineers rarely look at it. Costs embedded in Grafana dashboards engineers already use; supports chargeback/showback. Multi-Cluster Support Each cluster runs its own observability stack; federation is fragile. Single control plane for management, regional, and child clusters. Compliance &amp; Retention Long-term retention requires custom S3/Elasticsearch setups, costly and brittle. Policy-driven retention (30\u2013365+ days) and replication built in. Security &amp; Governance Role-based access is piecemeal, TLS often manual. RBAC and secure communication enforced by default. Upgrades Upgrade each component independently; breakage risk is high. Guided version-to-version upgrades with clear migration paths. Business Impact Tools work, but require constant integration effort; finance rarely gets visibility. Engineers debug faster, finance gets cost accountability, compliance gets retention \u2014 in one system."},{"location":"admin/kof/#guides","title":"Guides","text":"<p>Get started with the basic documentation:</p> <ul> <li>Architecture</li> <li>Installing KOF</li> <li>Verifying the KOF installation</li> <li>Storing KOF data</li> <li>Using KOF</li> <li>KOF Alerts</li> <li>KOF Tracing</li> <li>Retention and Replication </li> <li>Limits</li> <li>Scaling KOF</li> <li>Maintaining KOF</li> <li>Upgrading KOF</li> <li>FAQ</li> </ul> <p>Once you have KOF up and running, check k0rdent/kof/docs for advanced guides.</p>"},{"location":"admin/kof/kof-alerts/","title":"KOF Alerts","text":""},{"location":"admin/kof/kof-alerts/#summary","title":"Summary","text":"<p>At this point you have metrics collected and visualized. It is important to check them manually, but it is even better to automate detection and notification about the issues found in the data.</p> <p>We believe the rules should be configured using YAML IaC (Infrastructure as Code), while you can perform temporary management such as Silences using the UI.</p> <p>Alerting rules and recording rules in KOF are based on the PrometheusRules from the kube-prometheus-stack chart with per-cluster customization options.</p> <p>KOF uses the data source managed rules to store and execute recording rules in regional clusters closer to the source data, and to reduce the load on Grafana, even for alerting rules executed by Promxy in the management cluster.</p> <p>Promxy is used as a data source and executor of alerting rules instead of VMAlert because:</p> <ul> <li> <p>As the Promxy FAQ says, \"for example, if you wanted to know that the global error rate was &lt; 10%,     this would be impossible on the individual prometheus hosts     (without federation, or re-scraping) but trivial in promxy.\"</p> </li> <li> <p>It fixes the \"See graph\" button in the Grafana Alerting rules UI,     as Grafana gets the metrics from all regional clusters via Promxy.</p> </li> </ul> <p>VMAlertManager aggregates and sends alerts to various receivers like Slack with advanced routing options.</p> <p>Let's start with the demo of an alert sent and received.</p>"},{"location":"admin/kof/kof-alerts/#alertmanager-demo","title":"Alertmanager Demo","text":"<ol> <li> <p>Open the https://webhook.site/ and save \"Your unique URL\"     for the next step.</p> </li> <li> <p>Add the following to the <code>mothership-values.yaml</code> file, replacing <code>$WEBHOOK_URL</code> with the URL from step 1:     <pre><code>victoriametrics:\n  vmalert:\n    vmalertmanager:\n      config: |\n        route:\n          receiver: webhook\n        receivers:\n          - name: webhook\n            webhook_configs:\n              - url: $WEBHOOK_URL\n</code></pre></p> </li> <li> <p>Apply the <code>mothership-values.yaml</code> file as described in the Management Cluster section.</p> </li> <li> <p>Wait until the https://webhook.site/     shows the <code>Watchdog</code> alert, as in:     <pre><code>{\n  \"receiver\": \"webhook\",\n  \"status\": \"firing\",\n  \"alerts\": [\n    {\n      \"status\": \"firing\",\n      \"labels\": {\n        \"alertgroup\": \"general.rules\",\n        \"alertname\": \"Watchdog\",\n        \"severity\": \"none\",\n        \"source\": \"promxy\"\n      },\n      \"annotations\": {\n        \"description\": \"This is an alert meant to ensure that the entire alerting pipeline is functional...\",\n        \"runbook_url\": \"https://runbooks.prometheus-operator.dev/runbooks/general/watchdog\",\n        \"summary\": \"An alert that should always be firing to certify that Alertmanager is working properly.\"\n      },\n      \"startsAt\": \"2025-06-02T10:27:29.14Z\",\n      \"endsAt\": \"0001-01-01T00:00:00Z\",\n      \"generatorURL\": \"http://127.0.0.1:8082/...\",\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-alerts/#advanced-routing","title":"Advanced Routing","text":"<p>The configuration of the Alertmanager Demo is very basic.</p> <p>Please use these guides to apply advanced routing options:</p> <ul> <li> <p>Prometheus Alertmanager configuration reference - all possible options.</p> </li> <li> <p>VMAlertManager Slack example -     a multichannel notification system to ensure that critical alerts     are promptly delivered to the responsible teams.</p> </li> <li> <p>Matchers -     configurable routing rules that determine where and how alerts are directed     (for example, email, Slack, PagerDuty) based on severity, source, or other attributes.</p> </li> <li> <p>Grouping and     example from Prometheus     with <code>group_by: [cluster, alertname]</code> -     you may want to use <code>group_by: [alertgroup, alertname]</code> instead     for alert correlation across clusters to identify systemic issues and reduce noise     when the same alert fires in multiple clusters.</p> </li> </ul>"},{"location":"admin/kof/kof-alerts/#alertmanager-ui","title":"Alertmanager UI","text":"<p>To access the Alertmanager UI:</p> <ol> <li> <p>In the management cluster, forward the alertmanager port:     <pre><code>kubectl port-forward -n kof svc/vmalertmanager-cluster 9093:9093\n</code></pre></p> </li> <li> <p>Open http://127.0.0.1:9093/     and check tabs such as \"Alerts\" and \"Silences\".</p> </li> </ol> <p>See the demo in the Grafana Alerting UI section where Alertmanager UI shows the same data.</p>"},{"location":"admin/kof/kof-alerts/#grafana-alerting-ui","title":"Grafana Alerting UI","text":"<p>To access the Grafana Alerting UI:</p> <ol> <li> <p>Apply the Access to Grafana step.</p> </li> <li> <p>Open: Grafana - Alerting - and then \"Alert rules\" or \"Silences\", like this:</p> </li> </ol> <p></p>"},{"location":"admin/kof/kof-alerts/#prometheus-ui","title":"Prometheus UI","text":"<p>There are few places where you can find the graph of the firing alert:</p> <ol> <li> <p>Grafana - Alerting - Alert rules - rule - See graph.</p> <p>This shows the graph in Grafana UI, as in the Grafana Alerting UI demo above.</p> </li> <li> <p>Grafana - Alerting - Groups - group - alert - See source - Graph.</p> <p>This shows the graph in Prometheus UI.</p> </li> <li> <p>The same Prometheus UI link is sent to receiver like Slack in <code>generatorURL</code> field,     as shown in the Alertmanager Demo.</p> </li> </ol> <p>The Prometheus UI looks like this:</p> <p></p> <p>To enable Promxy Prometheus UI, please run this command in the management cluster:</p> <pre><code>kubectl port-forward -n kof svc/kof-mothership-promxy 8082:8082\n</code></pre> <p>If you expose the Prometheus UI with some external domain, please set <code>promxy.extraArgs.\"web.external-url\"</code> in <code>mothership-values.yaml</code> file and reapply it as described in the Management Cluster section.</p>"},{"location":"admin/kof/kof-alerts/#custom-rules","title":"Custom rules","text":"<p>You can update or create rules for all or specific clusters in a centralized way, passing values to the <code>kof-mothership</code> chart installed in the management cluster.</p> <p>For example, let's update the <code>CPUThrottlingHigh</code> alert in the <code>kubernetes-resources</code> group:</p> <ol> <li> <p>Note the original alert     in the <code>PrometheusRule</code> has the threshold <code>&gt; ( 25 / 100 )</code>.</p> </li> <li> <p>Add this cluster-specific patch to the <code>mothership-values.yaml</code> file:     <pre><code>clusterAlertRules:\n  cluster1:\n    kubernetes-resources:\n      CPUThrottlingHigh:\n        expr: |-\n          sum(increase(container_cpu_cfs_throttled_periods_total{cluster=\"cluster1\", container!=\"\"}[5m])) without (id, metrics_path, name, image, endpoint, job, node)\n            / on (cluster, namespace, pod, container, instance) group_left\n          sum(increase(container_cpu_cfs_periods_total{cluster=\"cluster1\"}[5m])) without (id, metrics_path, name, image, endpoint, job, node)\n            &gt; ( 42 / 100 )\n</code></pre>     Note the <code>cluster=\"cluster1\"</code> filters and the <code>&gt; ( 42 / 100 )</code> threshold.</p> </li> <li> <p>Add a similar patch for <code>cluster10</code> to the same <code>clusterAlertRules</code>.</p> </li> <li> <p>Now that we have special <code>CPUThrottlingHigh</code> alerts for <code>cluster1</code> and <code>cluster10</code>,     we want to exclude these clusters from the default <code>CPUThrottlingHigh</code> alert     to avoid the ambiguity of which threshold fires this alert in each cluster.</p> <p>Add this patch to the same file: <pre><code>defaultAlertRules:\n  kubernetes-resources:\n    CPUThrottlingHigh:\n      expr: |-\n        sum(increase(container_cpu_cfs_throttled_periods_total{cluster!~\"^cluster1$|^cluster10$\", container!=\"\"}[5m])) without (id, metrics_path, name, image, endpoint, job, node)\n          / on (cluster, namespace, pod, container, instance) group_left\n        sum(increase(container_cpu_cfs_periods_total{cluster!~\"^cluster1$|^cluster10$\"}[5m])) without (id, metrics_path, name, image, endpoint, job, node)\n          &gt; ( 25 / 100 )\n</code></pre> Note the <code>cluster!~\"^cluster1$|^cluster10$\"</code> filters and the default threshold.</p> </li> <li> <p>You can also update or create recording rules in the same way,     but the whole rule group should be redefined, because the <code>record</code> field is not unique.</p> </li> <li> <p>You may update or create more rules,     like the <code>ContainerHighMemoryUsage</code> alert that was added on demand     from the awesome-prometheus-alerts collection.</p> </li> <li> <p>Apply the <code>mothership-values.yaml</code> file as described in the Management Cluster section.</p> </li> </ol>"},{"location":"admin/kof/kof-alerts/#generation-of-rules","title":"Generation of rules","text":"<p>The next steps are automated:</p> <pre><code>graph TB\n  KPSRF[rules files and values&lt;br&gt;copied from kube-prometheus-stack&lt;br&gt;to kof-mothership] --&gt;\n  PR[PrometheusRules]\n\n  ARV[kof-mothership values:&lt;br&gt;defaultAlertRules,&lt;br&gt;clusterAlertRules] --&gt;\n  ARCM[input ConfigMaps:&lt;br&gt;k-m-promxy-rules-default,&lt;br&gt;k-m-promxy-rules-cluster-*] --kof-operator:&lt;br&gt;configmap_controller&lt;br&gt;updates--&gt;\n  KMPR[output ConfigMap:&lt;br&gt;k-m-promxy-rules]\n  PR --&gt; KMPR ---&gt;\n  EPR[Alerting /etc/promxy/rules]\n\n  RRV[kof-mothership values:&lt;br&gt;defaultRecordRules&lt;br&gt;clusterRecordRules] --&gt;\n  RRCM[input ConfigMaps:&lt;br&gt;kof-record-rules-default,&lt;br&gt;kof-record-rules-cluster-*]\n  PR --&gt; KRVM[output ConfigMap:&lt;br&gt;kof-record-vmrules-*]\n  RRCM --&gt; KRVM\n\n  KRVM --\"Management special case:&lt;br&gt;helm upgrade -i kof-storage&lt;br&gt;-f vmrules.yaml\"--&gt; KSV\n  KRVM --Regional MCS/ClusterProfile&lt;br&gt;valuesFrom: ConfigMap--&gt;\n  KSV[kof-storage values:&lt;br&gt;vmrules: groups: ...] --&gt;\n  VMR[Recording VMRules]\n\n  RCD[Regional ClusterDeployment] --\"kof-operator:&lt;br&gt;clusterdeployment_controller&lt;br&gt;creates empty\"--&gt; KRVM</code></pre> <ul> <li> <p>Rules patches (empty by default) are rendered from <code>kof-mothership</code> values     to the input <code>ConfigMaps</code>, which are merged with upstream <code>PrometheusRules</code>,     generating     the output <code>ConfigMaps</code>.</p> <ul> <li>If you want to protect some output <code>ConfigMap</code> from automatic changes,     set its label <code>k0rdent.mirantis.com/kof-generated: \"false\"</code></li> </ul> </li> <li> <p>Alerting rules are mounted to Promxy in the management cluster as <code>/etc/promxy/rules</code>.</p> </li> <li> <p>Recording rules are passed via <code>MultiClusterService</code> (or <code>ClusterProfile</code> for <code>istio</code> case)     to each regional cluster where <code>kof-storage</code> chart renders them to <code>VMRules</code>.</p> </li> </ul>"},{"location":"admin/kof/kof-alerts/#mothership-recording-rules","title":"Mothership recording rules","text":"<p>If you've selected to store KOF data of the management cluster in the same management cluster, then:</p> <ol> <li> <p>Copy the generated mothership recording rules from the output <code>ConfigMap</code> to a YAML file:     <pre><code>kubectl get cm -n kof kof-record-vmrules-mothership -o yaml \\\n| yq -r .data.values &gt; vmrules.yaml\n</code></pre></p> </li> <li> <p>Add <code>-f vmrules.yaml</code> to the <code>helm upgrade ... kof-storage</code> command     described in the From Management to Management section     and apply it.</p> </li> </ol>"},{"location":"admin/kof/kof-alerts/#execution-of-rules","title":"Execution of rules","text":"<p>Details of where and how the recording and alerting rules are executed:</p> <pre><code>sequenceDiagram\n    box rgba(0, 0, 255, 0.2) Regional kof-storage\n        participant VMR as Recording VMRules\n        participant VMA as VMAlert\n        participant VMS as VMStorage\n    end\n\n    box rgba(255, 0, 0, 0.2) Management kof-mothership\n        participant MP as Promxy\n        participant MVMS as VMStorage\n        participant VMAM as VMAlertManager\n    end\n\n    VMA-&gt;&gt;VMR: execute\n    VMA-&gt;&gt;VMS: read \"expr\" metrics\n    VMA-&gt;&gt;VMS: write \"record\" metrics\n\n    note over MP: execute&lt;br&gt;Alerting /etc/promxy/rules\n    MP-&gt;&gt;VMS: read \"expr\" metrics\n    MP-&gt;&gt;MVMS: write \"ALERTS\" metrics\n    MP-&gt;&gt;VMAM: Notify about alert</code></pre> <ul> <li> <p>Recording <code>VMRules</code> are executed by <code>VMAlert</code>, reading and writing to <code>VMStorage</code> -     all this happens in <code>kof-storage</code> in each regional cluster.</p> <p>The From Management to Management case is special: <code>VMRules</code> are provided by <code>kof-storage</code> chart in the management cluster, while <code>VMAlert</code> and <code>VMStorage</code> are provided by <code>kof-mothership</code> - to avoid having two VictoriaMetrics engines in the same cluster.</p> </li> <li> <p>Alerting rules are executed by Promxy in <code>kof-mothership</code> in the management cluster,     reading metrics from all regional <code>VMStorages</code>,     writing to the management <code>VMStorage</code>,     and notifying <code>VMAlertManager</code> in the management cluster.</p> </li> </ul>"},{"location":"admin/kof/kof-architecture/","title":"Architecture","text":""},{"location":"admin/kof/kof-architecture/#high-level","title":"High-level","text":"<p>From a high-level perspective, KOF consists of three layers:</p> <ul> <li>the Collection layer, where the statistics and events are gathered,</li> <li>the Regional layer, which includes storage to keep track of those statistics and events,</li> <li>and the Management layer, where you interact through the UI.</li> </ul> <pre><code>flowchart TD\n    A[Management UI, promxy] \n    A --&gt; C[Storage Region 1]\n    A --&gt; D[Storage Region 2]\n    C --&gt; E[Collect Child 1]\n    C --&gt; F[Collect Child 2]\n    D ==&gt; G[...]</code></pre>"},{"location":"admin/kof/kof-architecture/#mid-level","title":"Mid-level","text":"<p>Getting a little bit more detailed, it's important to undrestand that data flows upwards, from observed objects to centralized Grafana on the Management layer:</p> Management Cluster      kof-operators chart            grafana-operator             opentelemetry-operator             prometheus-operator-crds           kof-mothership chart            victoria-metrics-operator             cluster-api-visualizer             sveltos-dashboard             dex             k0rdent service templates             kof-dashboards             kof-operator             promxy           kof-collectors chart            opencost             opentelemetry-kube-stack           Either kof-istio            Certificates             ClusterProfiles           Or kof-regional and kof-child            MultiClusterServices         Cloud 1..N        Region 1..M      Regional Cluster          kof-operators chart                    grafana-operator                     opentelemetry-operator                     prometheus-operator-crds                   kof-storage chart                    victoria-metrics-operator                     victoria-logs-cluster                     external-dns                     jaeger-operator                     dex                     kof-dashboards                   kof-collectors chart                    opencost                     opentelemetry-kube-stack                   cert-manager                 ingress-nginx                 istio/gateway                 kof-istio chart                    cert-manager-istio-csr                     istio/base                     istiod          Child Cluster 1          cert-manager                 Optional kof-istio                 kof-operators chart                    Disabled grafana-operator                     opentelemetry-operator                     prometheus-operator-crds                   kof-collectors chart                    opencost                     opentelemetry-kube-stack                   observed objects"},{"location":"admin/kof/kof-architecture/#helm-charts","title":"Helm Charts","text":"<p>KOF is deployed as a series of Helm charts at various levels.</p>"},{"location":"admin/kof/kof-architecture/#kof-operators","title":"kof-operators","text":"<ul> <li>Grafana dashboards platform, managed by grafana-operator</li> <li>OpenTelemetry collectors below, managed by opentelemetry-operator</li> <li>prometheus-operator-crds required to create OpenTelemetry collectors, also required to monitor <code>kof-mothership</code> itself</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-mothership","title":"kof-mothership","text":"<ul> <li>Local VictoriaMetrics storage for alerting rules only, managed by victoria-metrics-operator</li> <li>cluster-api-visualizer for insight into multicluster configuration</li> <li>Sveltos dashboard, automatic secret distribution</li> <li>Dex SSO chart</li> <li>k0rdent service templates used by <code>kof-regional</code> and <code>kof-child</code> charts</li> <li>kof-dashboards for Grafana</li> <li>kof-operator (don't confuse it with the <code>kof-operators</code> chart) for auto-configuration</li> <li>Promxy for aggregating Prometheus metrics from regional clusters</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-regional","title":"kof-regional","text":"<ul> <li>MultiClusterService   which configures and installs <code>kof-storage</code> and other charts to regional clusters</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-child","title":"kof-child","text":"<ul> <li>MultiClusterService   which configures and installs <code>kof-collectors</code> and other charts to child clusters</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-istio","title":"kof-istio","text":"<ul> <li>Optional Istio support for secure connectivity between clusters without external DNS</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-storage","title":"kof-storage","text":"<ul> <li>Regional VictoriaMetrics storage with main data, managed by victoria-metrics-operator</li> <li>vmauth entrypoint proxy for VictoriaMetrics components</li> <li>vmcluster for high-available fault-tolerant version of VictoriaMetrics database</li> <li>victoria-logs-cluster for high-performance, cost-effective, scalable logs storage</li> <li>external-dns to communicate with other clusters</li> <li>Jaeger tracing platform, managed by jaeger-operator</li> <li>Dex SSO chart</li> <li>kof-dashboards for Grafana</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-collectors","title":"kof-collectors","text":"<ul> <li>opentelemetry-kube-stack for hardware, OS, and Kubernetes metrics</li> <li>OpenCost \"shines a light into the black box of Kubernetes spend\"</li> </ul>"},{"location":"admin/kof/kof-architecture/#deployment-scenarios","title":"Deployment Scenarios","text":"<p>KOF supports two topologies:</p>"},{"location":"admin/kof/kof-architecture/#production-regional-clusters","title":"Production (Regional Clusters)","text":"<ul> <li>Management-cluster telemetry is stored locally on the k0rdent management cluster.</li> <li>Child workloads send telemetry to their regional cluster, supporting data sovereignty and isolation.</li> </ul>"},{"location":"admin/kof/kof-architecture/#development-qa-regionless","title":"Development / QA (Regionless)","text":"<ul> <li>No regions are defined. All telemetry (management  child) is stored on the k0rdent management cluster.</li> </ul>"},{"location":"admin/kof/kof-architecture/#component-roles-rationale","title":"Component Roles &amp; Rationale","text":"Component Role Notes k0rdent Orchestration Multi-cluster lifecycle  service templates OpenTelemetry Collection Metrics, logs, traces; auto-instrumentation options Promxy Query Federation Cross-cluster PromQL  alert rule evaluation at management VictoriaMetrics Metrics Storage Scalable TSDB; selected over Prometheus for clustering  efficiency VictoriaLogs Log Storage Scalable log TSDB with retention controls Jaeger Tracing Trace store/visualization; regional awareness Grafana Visualization Unified dashboards; SSO/RBAC Dex SSO OIDC provider for Grafana OpenCost FinOps Cost allocation and efficiency ratios"},{"location":"admin/kof/kof-architecture/#dex-integration","title":"Dex Integration","text":"<p>KOF uses Dex as an identity provider to enable Single Sign\u2011On (SSO) with OAuth2 and OIDC.</p> <ul> <li>Authentication flow: Dex issues ID tokens to Grafana and other clients after authenticating against an upstream identity provider (IdP).</li> <li>External IdP integration: Dex can delegate to providers such as Okta, Entra ID, GitHub, or LDAP.</li> <li>Group membership mapping: Dex propagates group membership claims, which KOF uses to enforce RBAC. Grafana dashboards and KOF namespaces can be restricted based on these groups.</li> </ul> <p>This model centralizes authentication, while authorization remains controlled via Kubernetes RBAC and Grafana roles.</p>"},{"location":"admin/kof/kof-faq/","title":"KOF FAQ &amp; Scenarios","text":""},{"location":"admin/kof/kof-faq/#what-is-fullstack-observability-in-kof","title":"What is full\u2011stack observability in KOF?","text":"<p>OpenTelemetry collects metrics, logs, and traces; data is stored in VictoriaMetrics, VictoriaLogs, and Jaeger, and visualized in Grafana.</p>"},{"location":"admin/kof/kof-faq/#how-do-i-collect-telemetry-from-a-new-service","title":"How do I collect telemetry from a new service?","text":"<p>There are two ways to collect telemetry from a new service:</p> <ul> <li>Prometheus scrape annotations: Add scrape annotations to your pods so metrics are collected automatically.</li> <li>Auto-instrumentation: Let KOF inject the OpenTelemetry language agent into your pods at runtime.</li> </ul> <p>For example, let's say you were adding auto-instrumentation to a Java Spring Boot service.  You'd follow these steps:</p> <ol> <li>Enable instrumentation in your application's Helm chart values:    <pre><code>kof:\n  instrumentation:\n    enabled: true\n    language: java\n</code></pre></li> <li>The Helm flag <code>kof.instrumentation.enabled=true</code> makes sure the necessary CRDs from the OpenTelemetry Operator are present in the cluster, but you still need to configure the actual application. When you deploy, add the appropriate annotation:    <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-java-service\n  namespace: demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-java-service\n  template:\n    metadata:\n      labels:\n        app: my-java-service\n      annotations:\n        instrumentation.opentelemetry.io/inject-java: \"true\"\n    spec:\n      containers:\n      - name: app\n        image: myregistry/my-java-service:1.0.0\n        ports:\n        - containerPort: 8080\n</code></pre></li> </ol> <p>With this annotation, KOF automatically injects the OpenTelemetry Java agent. The service then exports metrics, traces, and logs through the configured OpenTelemetry Collector.</p> <ol> <li>After deploying, you should see:</li> <li>Traces in Jaeger</li> <li>Metrics in VictoriaMetrics (via Grafana dashboards)  </li> <li>Logs in VictoriaLogs</li> </ol> <p>For other languages, use the appropriate annotation, as in:</p> <ul> <li>Python: <code>instrumentation.opentelemetry.io/inject-python: \"true\"</code></li> <li>Node.js: <code>instrumentation.opentelemetry.io/inject-nodejs: \"true\"</code></li> </ul>"},{"location":"admin/kof/kof-faq/#how-should-i-manage-dashboards","title":"How should I manage dashboards?","text":"<p>Treat dashboards as code. Edit YAML under <code>charts/kof-dashboards/files/dashboards/*</code> and deploy via Helm/CI/CD. Avoid editing in the Grafana UI, as those changes will be overwritten.</p>"},{"location":"admin/kof/kof-faq/#how-do-i-avoid-commingling-tenant-data","title":"How do I avoid commingling tenant data?","text":"<p>Deploy collectors/storage per tenant namespace and restrict access with RBAC. This ensures separation of data paths per tenant.</p>"},{"location":"admin/kof/kof-faq/#how-do-i-control-retention-policies","title":"How do I control retention policies?","text":"<p>Configure the <code>retentionPeriod</code> for VictoriaMetrics and VictoriaLogs. You can find more information in KOF Retention.</p>"},{"location":"admin/kof/kof-install/","title":"Installing k0rdent Observability and FinOps","text":""},{"location":"admin/kof/kof-install/#options","title":"Options","text":"<p>KOF may be installed using different options:</p> <pre><code>flowchart TD\n    S((Start))\n    S --&gt; AWS[AWS]\n    S --&gt; AZ[Azure]\n    S --&gt; OS[OpenStack&lt;br&gt;and others]\n\n    AWS -.-&gt; AG[**Aig Gap**]\n    AZ -.-&gt; AG\n    OS --&gt; AG\n\n    AWS --&gt; NAG[No Air Gap]\n    AZ --&gt; NAG\n    OS --&gt; NAG\n\n    AG --&gt; I[**Istio**]\n    AG --&gt; MD[Manual DNS]\n    AG -.-&gt; AD[Auto DNS]\n\n    NAG --&gt; I\n    NAG --&gt; MD\n    NAG --&gt; AD\n\n    AD --&gt; R[Regional...]\n    I --&gt; R\n    MD --&gt; R\n\n    R --&gt; CD[...ClusterDeployment]\n    R --&gt; CM[...ConfigMap]\n    R --&gt; RIM[...in Management]\n\n    CD --&gt; M2[Store KOF data&lt;br&gt;from Management...]\n    CM --&gt; M2\n    RIM --&gt; M2\n\n    M2 --&gt; M2M[**...to Management**]\n    M2 --&gt; M2R[...to Regional&lt;br&gt;with Istio&lt;br&gt;or without]\n    M2 --&gt; M2TP[...to Third-party]</code></pre> <p>Opinionated default installation we plan to simplify in the next release is shown in bold style.</p>"},{"location":"admin/kof/kof-install/#prerequisites","title":"Prerequisites","text":"<p>Before beginning KOF installation, you should have the following components in place:</p> <ul> <li>A k0rdent management cluster - You can get instructions to create one in the quickstart guide.</li> <li>You will also need your infrastructure provider credentials, such as those shown in the guide for AWS.<ul> <li>Note that you should skip the \"Create your <code>ClusterDeployment</code>\" and later sections.</li> </ul> </li> <li>Finally, select one of the options:<ul> <li>DNS auto-config to automate the process for all regional clusters.</li> <li>Manual DNS config is applied later   for each regional cluster separately and manually.</li> <li>Istio service mesh for secure connectivity between clusters.   This is the only option which does not need access to create external DNS records   for service endpoints such as <code>kof.example.com</code>.</li> </ul> </li> </ul> <p>For for information on long-term storage planning, review the KOF Retention page, which covers retention and replication strategies.</p>"},{"location":"admin/kof/kof-install/#dns-auto-config","title":"DNS auto-config","text":"<p>To avoid manual configuration of DNS records for service endpoints later, you can automate the process now using external-dns.</p>"},{"location":"admin/kof/kof-install/#aws","title":"AWS","text":"<p>For AWS in production, use the Node IAM Role or IRSA methods in production.</p> <p>Just for the sake of this demo based on the <code>aws-standalone</code> template, however, you can use the most straightforward (though less secure) static credentials method:</p> <ol> <li>Create an <code>external-dns</code> IAM user with this policy.</li> <li>Create an access key and <code>external-dns-aws-credentials</code> file, as in:     <pre><code>[default]\naws_access_key_id = &lt;EXAMPLE_ACCESS_KEY_ID&gt;\naws_secret_access_key = &lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\n</code></pre></li> <li>Create the <code>external-dns-aws-credentials</code> secret in the <code>kof</code> namespace:     <pre><code>kubectl create namespace kof\nkubectl create secret generic \\\n  -n kof external-dns-aws-credentials \\\n  --from-file external-dns-aws-credentials\n</code></pre></li> </ol>"},{"location":"admin/kof/kof-install/#azure","title":"Azure","text":"<p>To enable DNS auto-config on Azure, use DNS Zone Contributor.</p> <ol> <li> <p>Create an Azure service principal with the DNS Zone Contributor permissions. You can find an example here.</p> </li> <li> <p>Create the <code>azure.json</code> text file containing the service principal configuration data:     <pre><code>{\n  \"tenantId\": \"&lt;EXAMPLE_TENANT_ID&gt;\",\n  \"subscriptionId\": \"&lt;EXAMPLE_SUBSCRIPTION_ID&gt;\",\n  \"resourceGroup\": \"&lt;EXAMPLE_RESOURCE_GROUP&gt;\",\n  \"aadClientId\": \"&lt;EXAMPLE_SP_APP_ID&gt;\",\n  \"aadClientSecret\": \"&lt;EXAMPLE_SP_PASSWORD&gt;\"\n}\n</code></pre></p> </li> <li> <p>Create the <code>external-dns-azure-credentials</code> secret in the <code>kof</code> namespace:     <pre><code>kubectl create namespace kof\nkubectl create secret generic \\\n  -n kof external-dns-azure-credentials \\\n  --from-file azure.json\n</code></pre> See external-dns Azure documentation for more details.</p> </li> </ol>"},{"location":"admin/kof/kof-install/#openstack-and-others","title":"OpenStack and others","text":"<p>Please check <code>external-dns</code> tutorials and new providers to find instructions on how to create a secret.</p> <p>For example the <code>external-dns-openstack-credentials</code> could be created by applying the external-dns-openstack-webhook docs.</p>"},{"location":"admin/kof/kof-install/#istio","title":"Istio","text":"<p>If you've selected to skip both DNS auto-config now and Manual DNS config later, you can apply these steps to enable the Istio service mesh:</p> <ol> <li> <p>Check the overview in the kof/docs/istio.md     and this video:</p> <p> </p> </li> <li> <p>Create and label the <code>kof</code> namespace to allow Istio to inject its sidecars:     <pre><code>kubectl create namespace kof\nkubectl label namespace kof istio-injection=enabled\n</code></pre></p> </li> <li> <p>Install the <code>kof-istio</code> chart to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait \\\n  --create-namespace -n istio-system kof-istio \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-istio --version 1.4.0\n</code></pre>     You may want to customize the collectors     by passing values to the kof-istio-child     profile for all child clusters at once now,     or for each child cluster later. You can also opt for using the default values.</p> </li> </ol> <p>Notice</p> <p> Currently the <code>kof-istio</code> chart uses <code>ClusterProfiles</code> kof-istio-regional and kof-istio-child. In the future, however, the KOF team plans to replace them with <code>MultiClusterService</code> objects in the kof-regional and kof-child charts.</p> <p>If you selected the Istio option, use <code>kof-istio</code>.  Otherwise use <code>kof-regional</code> with <code>kof-child</code>.  Do not use all 3 charts at the same time.</p>"},{"location":"admin/kof/kof-install/#management-cluster","title":"Management Cluster","text":"<p>To install KOF on the management cluster, look through the default values of the kof-mothership and kof-operators charts, and apply this example, or use it as a reference:</p> <ol> <li> <p>Install <code>kof-operators</code> as required by <code>kof-mothership</code>:     <pre><code>helm upgrade -i --reset-values --wait \\\n  --create-namespace -n kof kof-operators \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-operators --version 1.4.0\n</code></pre></p> </li> <li> <p>Create an empty <code>mothership-values.yaml</code> file.</p> <p>If you have not applied the Istio section, add to this file: <pre><code>kcm:\n  installTemplates: true\n</code></pre> This enables installation of <code>ServiceTemplates</code> such as <code>cert-manager</code> and <code>kof-collectors</code>, making it possible to reference them from the regional and child <code>MultiClusterService</code> objects.</p> </li> <li> <p>If you want to use a default storage class,     but <code>kubectl get sc</code> shows no <code>(default)</code>, create it.     Otherwise you can use a non-default storage class in the <code>mothership-values.yaml</code> file:     <pre><code>global:\n  storageClass: &lt;EXAMPLE_STORAGE_CLASS&gt;\n</code></pre></p> <p>If <code>kubectl get sc</code> shows nothing   or just <code>kubernetes.io/no-provisioner</code> in the <code>PROVISIONER</code> column,   apply OpenEBS or similar.</p> </li> <li> <p>If you've applied the DNS auto-config section,     add its information to the <code>kcm:</code> object in the <code>mothership-values.yaml</code> file.</p> <p>For AWS, add:</p> <pre><code>  kof:\n    clusterProfiles:\n      kof-aws-dns-secrets:\n        matchLabels:\n          k0rdent.mirantis.com/kof-aws-dns-secrets: \"true\"\n        secrets:\n          - external-dns-aws-credentials\n</code></pre> <p>For Azure, add:</p> <pre><code>  kof:\n    clusterProfiles:\n      kof-azure-dns-secrets:\n        matchLabels:\n          k0rdent.mirantis.com/kof-azure-dns-secrets: \"true\"\n        secrets:\n          - external-dns-azure-credentials\n</code></pre> <p>For OpenStack, add:</p> <pre><code>  kof:\n    clusterProfiles:\n      kof-openstack-dns-secrets:\n        matchLabels:\n          k0rdent.mirantis.com/kof-openstack-dns-secrets: \"true\"\n        secrets:\n          - external-dns-openstack-credentials\n</code></pre> <p>This enables Sveltos to auto-distribute the DNS secret to regional clusters.</p> </li> <li> <p>Examples of <code>ClusterDeployments</code> in Regional Cluster     and Child Cluster sections are both using <code>namespace: kcm-system</code>     for a child cluster to connect to a regional cluster easily.</p> <p>If you plan to have multiple tenants/namespaces in the management cluster, please note they are isolated by default: a child cluster in <code>namespace: tenantA</code> will be able to connect to a regional cluster in <code>namespace: tenantA</code> only.</p> <p>If you want to allow a child cluster in one namespace to connect to a regional cluster in another namespace, enable the <code>crossNamespace</code> value in the <code>mothership-values.yaml</code> file:</p> <pre><code>kcm:\n  kof:\n    operator:\n      crossNamespace: true\n</code></pre> </li> <li> <p>Install <code>kof-mothership</code>:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-mothership \\\n  -f mothership-values.yaml \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-mothership --version 1.4.0\n</code></pre></p> </li> <li> <p>If you're upgrading KOF from an earlier version, apply the Upgrading KOF guide.</p> </li> <li> <p>If you have not applied the Istio section:</p> <p>Notice</p> <p> Currently the <code>kof-istio</code> chart uses <code>ClusterProfiles</code> kof-istio-regional and kof-istio-child. In the future, however, the KOF team plans to replace them with <code>MultiClusterService</code> objects in the kof-regional and kof-child charts.</p> <p>If you selected the Istio option, use <code>kof-istio</code>.  Otherwise use <code>kof-regional</code> with <code>kof-child</code>.  Do not use all 3 charts at the same time.</p> <ul> <li>Wait until the value of <code>VALID</code> changes to <code>true</code> for all <code>ServiceTemplate</code> objects:     <pre><code>kubectl get svctmpl -A\n</code></pre></li> <li>Look through the <code>MultiClusterService</code> objects in the kof-regional     and kof-child charts.</li> <li>If your regional clusters already have <code>ingress-nginx</code> and <code>cert-manager</code> services,     you can ask the <code>kof-regional</code> chart to not install them by setting Helm values     <code>ingress-nginx.enabled</code> and <code>cert-manager.enabled</code> to <code>false</code>.</li> <li>You may want to customize collectors     for all child clusters at once now, or for each child cluster later, or just use the default values.</li> <li>If all your regional clusters will use OpenStack,     you may set the OpenStack-specific values described in step 10 of the Regional Cluster section     as a values file with <code>storage:</code> key passed to <code>kof-regional</code> chart here.</li> <li>Install these charts into the management cluster with default or custom values:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-regional \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-regional --version 1.4.0\n\nhelm upgrade -i --reset-values --wait -n kof kof-child \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-child --version 1.4.0\n</code></pre></li> </ul> </li> <li> <p>Wait for all pods to show that they're <code>Running</code>:     <pre><code>kubectl get pod -n kof\n</code></pre></p> </li> <li> <p>Check options to store metrics of the management cluster in the Storing KOF data guide.</p> </li> </ol>"},{"location":"admin/kof/kof-install/#regional-cluster","title":"Regional Cluster","text":"<p>To install KOF on the regional cluster, look through the default values of the kof-storage chart, and apply this example for AWS, or use it as a reference:</p> <ol> <li> <p>Set your KOF variables using your own values:     <pre><code>REGION=us-east-2\nREGIONAL_CLUSTER_NAME=aws-$REGION\nADMIN_EMAIL=$(git config user.email)\necho \"$REGION, $REGIONAL_CLUSTER_NAME, $ADMIN_EMAIL\"\n</code></pre></p> <p>If you have not applied the Istio section, set the <code>REGIONAL_DOMAIN</code> too, using your own domain: <pre><code>REGIONAL_DOMAIN=$REGIONAL_CLUSTER_NAME.kof.example.com\necho \"$REGIONAL_DOMAIN\"\n</code></pre></p> </li> <li> <p>Use the up-to-date <code>ClusterTemplate</code>, as in:     <pre><code>kubectl get clustertemplate -n kcm-system | grep aws\nTEMPLATE=aws-standalone-cp-1-0-16\n</code></pre></p> </li> <li> <p>Compose the regional <code>ClusterDeployment</code>:</p> <p>For AWS:</p> <pre><code>cat &gt;regional-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $REGIONAL_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n    k0rdent.mirantis.com/kof-cluster-role: regional\nspec:\n  template: $TEMPLATE\n  credential: aws-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    clusterAnnotations:\n      k0rdent.mirantis.com/kof-regional-domain: $REGIONAL_DOMAIN\n      k0rdent.mirantis.com/kof-cert-email: $ADMIN_EMAIL\n    region: $REGION\n    controlPlaneNumber: 1\n    controlPlane:\n      instanceType: t3.large\n    workersNumber: 3\n    worker:\n      instanceType: t3.xlarge\nEOF\n</code></pre> <p>For Azure:</p> <pre><code>AZURE_SUBSCRIPTION_ID=$(az account show --query id -o tsv)\necho \"AZURE_SUBSCRIPTION_ID=$AZURE_SUBSCRIPTION_ID\"\n\ncat &gt;regional-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $REGIONAL_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n    k0rdent.mirantis.com/kof-cluster-role: regional\nspec:\n  template: $TEMPLATE\n  credential: azure-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: azure-cluster-identity\n      namespace: kcm-system\n    clusterAnnotations:\n      k0rdent.mirantis.com/kof-regional-domain: $REGIONAL_DOMAIN\n      k0rdent.mirantis.com/kof-cert-email: $ADMIN_EMAIL\n    subscriptionID: $AZURE_SUBSCRIPTION_ID\n    location: $REGION\n    controlPlaneNumber: 1\n    controlPlane:\n      vmSize: Standard_A4_v2\n    workersNumber: 3\n    worker:\n      vmSize: Standard_A4_v2\nEOF\n</code></pre> </li> <li> <p>If you've applied the DNS auto-config section,     add it to the <code>.metadata.labels</code> in the <code>regional-cluster.yaml</code> file.</p> <p>For AWS, add:</p> <pre><code>k0rdent.mirantis.com/kof-aws-dns-secrets: \"true\"\n</code></pre> <p>For Azure, add:</p> <pre><code>k0rdent.mirantis.com/kof-azure-dns-secrets: \"true\"\n</code></pre> <p>For OpenStack, add:</p> <pre><code>k0rdent.mirantis.com/kof-openstack-dns-secrets: \"true\"\n</code></pre> </li> <li> <p>If you've applied the Istio section, update the <code>regional-cluster.yaml</code> file:</p> <ul> <li> <p>Replace this line:   <pre><code>k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n</code></pre>   with this line:   <pre><code>k0rdent.mirantis.com/istio-role: child\n</code></pre>   Note that <code>child</code> is not a typo   and the <code>kof-</code> prefix is not missing in <code>istio-role</code>.   These values make your configuration compatible with possible migration of Istio support   from KOF to the upstream k0rdent,   where <code>child</code> is a generic term described in the Cluster Deployments concept.</p> </li> <li> <p>Delete these lines:   <pre><code>k0rdent.mirantis.com/kof-regional-domain: $REGIONAL_DOMAIN\nk0rdent.mirantis.com/kof-cert-email: $ADMIN_EMAIL\n</code></pre></p> </li> </ul> </li> <li> <p>This <code>ClusterDeployment</code> uses propagation of its <code>.metadata.labels</code>     to the resulting <code>Cluster</code> because there are no <code>.spec.config.clusterLabels</code> here.     If you add them, please copy <code>.metadata.labels</code> as well.</p> </li> <li> <p>The <code>aws-standalone-cp</code> template provides the default storage class <code>ebs-csi-default-sc</code> for AWS. The k0rdent quickstart guide     provides the default storage class <code>managed-csi</code> for Azure.     If you want to use a non-default storage class,     add it to the <code>regional-cluster.yaml</code> file in the <code>.spec.config.clusterAnnotations</code>:     <pre><code>k0rdent.mirantis.com/kof-storage-class: &lt;EXAMPLE_STORAGE_CLASS&gt;\n</code></pre></p> </li> <li> <p>The <code>kof-operator</code> creates and configures <code>PromxyServerGroup</code> and <code>GrafanaDatasource</code> automatically.     It uses the endpoints listed below by default.     If you want to disable the built-in metrics, logs, and traces to use your own existing instances instead,     add custom endpoints to the <code>regional-cluster.yaml</code> file in the <code>.spec.config.clusterAnnotations</code>:     <pre><code>k0rdent.mirantis.com/kof-write-metrics-endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/insert/0/prometheus/api/v1/write\nk0rdent.mirantis.com/kof-read-metrics-endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/select/0/prometheus\nk0rdent.mirantis.com/kof-write-logs-endpoint: https://vmauth.$REGIONAL_DOMAIN/vli/insert/opentelemetry/v1/logs\nk0rdent.mirantis.com/kof-read-logs-endpoint: https://vmauth.$REGIONAL_DOMAIN/vls\nk0rdent.mirantis.com/kof-write-traces-endpoint: https://jaeger.$REGIONAL_DOMAIN/collector\n</code></pre></p> <p>If you've applied the Istio section, default endpoints are: <pre><code>k0rdent.mirantis.com/kof-write-metrics-endpoint: http://$REGIONAL_CLUSTER_NAME-vminsert:8480/insert/0/prometheus/api/v1/write\nk0rdent.mirantis.com/kof-read-metrics-endpoint: http://$REGIONAL_CLUSTER_NAME-vmselect:8481/select/0/prometheus\nk0rdent.mirantis.com/kof-write-logs-endpoint: http://$REGIONAL_CLUSTER_NAME-logs-insert:9481/insert/opentelemetry/v1/logs\nk0rdent.mirantis.com/kof-read-logs-endpoint: http://$REGIONAL_CLUSTER_NAME-logs-select:9471\nk0rdent.mirantis.com/kof-write-traces-endpoint: http://$REGIONAL_CLUSTER_NAME-jaeger-collector:4318\n</code></pre></p> <p>If you want to skip creation of the regional cluster completely, but still let child clusters discover your custom endpoints, you may create a regional <code>ConfigMap</code> instead of a regional <code>ClusterDeployment</code>: <pre><code>cat &gt;regional-configmap.yaml &lt;&lt;EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: $REGIONAL_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-cluster-role: regional\ndata:\n  regional_cluster_name: $REGIONAL_CLUSTER_NAME\n  regional_cluster_namespace: kcm-system\n  regional_cluster_cloud: aws\n  aws_region: $REGION\n  azure_location: \"\"\n  openstack_region: \"\"\n  vsphere_datacenter: \"\"\n  istio_role: \"\"\n  kof_http_config: \"\"\n  write_metrics_endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/insert/0/prometheus/api/v1/write\n  read_metrics_endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/select/0/prometheus\n  write_logs_endpoint: https://vmauth.$REGIONAL_DOMAIN/vli/insert/opentelemetry/v1/logs\n  read_logs_endpoint: https://vmauth.$REGIONAL_DOMAIN/vls\n  write_traces_endpoint: https://jaeger.$REGIONAL_DOMAIN/collector\nEOF\n\ncat regional-configmap.yaml\nkubectl apply -f regional-configmap.yaml\n</code></pre></p> <p>Note that a similar <code>ConfigMap</code> is generated automatically from a regional <code>ClusterDeployment</code> too.</p> </li> <li> <p>If you need a custom http client configuration for <code>PromxyServerGroup</code> and <code>GrafanaDatasource</code>,     add it to the <code>regional-cluster.yaml</code> file in the <code>.metadata.annotations</code>. For example:     <pre><code>k0rdent.mirantis.com/kof-http-config: '{\"dial_timeout\": \"10s\", \"tls_config\": {\"insecure_skip_verify\": true}}'\n</code></pre></p> </li> <li> <p>Either <code>MultiClusterService</code> named kof-regional-cluster     or <code>ClusterProfiles</code> named kof-istio-regional     and kof-istio-network     configure and install <code>cert-manager</code>, <code>ingress-nginx</code>, <code>istio/gateway</code>, <code>kof-istio</code>, <code>kof-operators</code>, <code>kof-storage</code>, and <code>kof-collectors</code> charts automatically.</p> <p>To pass any custom values to the <code>kof-storage</code> chart or its subcharts, such as victoria-logs-cluster, add them to the <code>regional-cluster.yaml</code> file in the <code>.spec.config.clusterAnnotations</code>. For example: <pre><code>k0rdent.mirantis.com/kof-storage-values: |\n  victoria-logs-cluster:\n    vlinsert:\n      replicaCount: 2\n</code></pre></p> <p>For OpenStack, add: <pre><code>k0rdent.mirantis.com/kof-storage-values: |\n  external-dns:\n    provider:\n      name: webhook\n      webhook:\n        image:\n          repository: ghcr.io/inovex/external-dns-openstack-webhook\n          tag: 1.1.0\n        extraVolumeMounts:\n          - name: oscloudsyaml\n            mountPath: /etc/openstack/\n    extraVolumeMounts: null\n    extraVolumes:\n      - name: oscloudsyaml\n        secret:\n          secretName: external-dns-openstack-credentials\n</code></pre></p> </li> <li> <p>Verify and apply the Regional <code>ClusterDeployment</code>:     <pre><code>cat regional-cluster.yaml\n\nkubectl apply -f regional-cluster.yaml\n</code></pre></p> </li> <li> <p>Watch how the cluster is deployed until all values of <code>READY</code> are <code>True</code>:     <pre><code>clusterctl describe cluster -n kcm-system $REGIONAL_CLUSTER_NAME \\\n  --show-conditions all\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-install/#child-cluster","title":"Child Cluster","text":"<p>To install KOF on the actual cluster to be monitored, look through the default values of the kof-operators and kof-collectors charts, and apply this example for AWS, or use it as a reference:</p> <ol> <li> <p>Ensure you still have the variables set in the Regional Cluster section,     and set your own value for the <code>CHILD_CLUSTER_NAME</code> variable:     <pre><code>CHILD_CLUSTER_NAME=$REGIONAL_CLUSTER_NAME-child1\necho \"$CHILD_CLUSTER_NAME\"\n</code></pre></p> </li> <li> <p>Use the up-to-date <code>ClusterTemplate</code>, as in:     <pre><code>kubectl get clustertemplate -n kcm-system | grep aws\nTEMPLATE=aws-standalone-cp-1-0-16\n</code></pre></p> </li> <li> <p>Compose the child <code>ClusterDeployment</code>:</p> <p>For AWS:</p> <pre><code>cat &gt;child-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $CHILD_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n    k0rdent.mirantis.com/kof-cluster-role: child\nspec:\n  template: $TEMPLATE\n  credential: aws-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    region: $REGION\n    controlPlaneNumber: 1\n    controlPlane:\n      instanceType: t3.large\n    workersNumber: 3\n    worker:\n      instanceType: t3.medium\nEOF\n</code></pre> <p>For Azure:</p> <pre><code>AZURE_SUBSCRIPTION_ID=$(az account show --query id -o tsv)\necho \"AZURE_SUBSCRIPTION_ID=$AZURE_SUBSCRIPTION_ID\"\n\ncat &gt;child-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $CHILD_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n    k0rdent.mirantis.com/kof-cluster-role: child\nspec:\n  template: $TEMPLATE\n  credential: azure-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: azure-cluster-identity\n      namespace: kcm-system\n    subscriptionID: $AZURE_SUBSCRIPTION_ID\n    location: $REGION\n    controlPlaneNumber: 1\n    controlPlane:\n      vmSize: Standard_A4_v2\n    workersNumber: 3\n    worker:\n      vmSize: Standard_A4_v2\nEOF\n</code></pre> </li> <li> <p>If you've applied the Istio section, update the <code>child-cluster.yaml</code> file:</p> <p>replace this line: <pre><code>k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n</code></pre> with this line: <pre><code>k0rdent.mirantis.com/istio-role: child\n</code></pre></p> </li> <li> <p>This <code>ClusterDeployment</code> uses propagation of its <code>.metadata.labels</code>     to the resulting <code>Cluster</code> because there are no <code>.spec.config.clusterLabels</code> here.     If you add them, please copy <code>.metadata.labels</code> as well.</p> </li> <li> <p>The <code>kof-operator</code> discovers     the regional cluster by the location of the child cluster.     If you have more than one regional cluster in the same AWS region / Azure location / etc.,     and you want to connect the child cluster to specific regional cluster,     add this regional cluster name to the <code>child-cluster.yaml</code> file in the <code>.metadata.labels</code>:     <pre><code>k0rdent.mirantis.com/kof-regional-cluster-name: $REGIONAL_CLUSTER_NAME\n</code></pre></p> <p>If you've enabled the <code>crossNamespace</code> value, specify the namespace too, for example: <pre><code>k0rdent.mirantis.com/kof-regional-cluster-namespace: kcm-system\n</code></pre></p> </li> <li> <p>Either <code>MultiClusterService</code> named kof-child-cluster     or <code>ClusterProfiles</code> named kof-istio-child     and kof-istio-network     configure and install <code>cert-manager</code>, <code>kof-istio</code>, <code>kof-operators</code>, and <code>kof-collectors</code> charts automatically.</p> <p>To pass any custom values to the <code>kof-collectors</code> chart or its subcharts, such as opencost, add them to the <code>child-cluster.yaml</code> file in the <code>.spec.config</code>. For example: <pre><code>clusterAnnotations:\n  k0rdent.mirantis.com/kof-collectors-values: |\n    opencost:\n      opencost:\n        exporter:\n          replicas: 2\n</code></pre> Note: the first <code>opencost</code> key is to reference the subchart, and the second <code>opencost</code> key is part of its values.</p> </li> <li> <p>Verify and apply the <code>ClusterDeployment</code>:     <pre><code>cat child-cluster.yaml\n\nkubectl apply -f child-cluster.yaml\n</code></pre></p> </li> <li> <p>Watch while the cluster is deployed until all values of <code>READY</code> are <code>True</code>:     <pre><code>clusterctl describe cluster -n kcm-system $CHILD_CLUSTER_NAME \\\n  --show-conditions all\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-limits/","title":"Resource Limits","text":"<p>See also: System Requirements.</p>"},{"location":"admin/kof/kof-limits/#resources-of-management-cluster","title":"Resources of Management Cluster","text":"<ul> <li> <p>promxy:   <pre><code>resources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 100m\n    memory: 128Mi\n</code></pre></p> </li> <li> <p>promxy-deployment:   <pre><code>resources:\n  requests:\n    cpu: 0.02\n    memory: 20Mi\n  limits:\n    cpu: 0.02\n    memory: 20Mi\n</code></pre></p> </li> <li> <p>promxy-operator:   <pre><code>resources:\n  limits:\n    cpu: 500m\n    memory: 128Mi\n  requests:\n    cpu: 10m\n    memory: 64Mi\n</code></pre></p> </li> </ul>"},{"location":"admin/kof/kof-limits/#resources-of-a-child-cluster","title":"Resources of a Child Cluster","text":"<ul> <li>opentelemetry:   <pre><code>resourceRequirements:\n  limits:\n    memory: 128Mi\n  requests:\n    memory: 128Mi\n</code></pre></li> </ul>"},{"location":"admin/kof/kof-limits/#why-victoriametrics-instead-of-prometheus","title":"Why VictoriaMetrics instead of Prometheus?","text":"<p>KOF uses VictoriaMetrics as the metrics backend, rather than Prometheus alone, for several reasons.</p> <ul> <li>Scalability: VictoriaMetrics supports horizontal clustering for large multi-cluster environments.</li> <li>Compression: VictoriaMetrics stores high-cardinality metrics efficiently.</li> <li>Federation: VictoriaMetrics integrates cleanly with Promxy for cross-cluster queries.</li> </ul> <p>We still use Prometheus at the collection layer, but long-term storage and query efficiency are handled by VictoriaMetrics.</p>"},{"location":"admin/kof/kof-maintainence/","title":"Maintaining KOF","text":""},{"location":"admin/kof/kof-maintainence/#backup-requirements","title":"Backup Requirements","text":"<p>Backing up KOF requires backing up the following:</p> <ul> <li>Grafana configurations</li> <li>Alert definitions</li> <li>Custom dashboards</li> <li>Retention policies</li> </ul>"},{"location":"admin/kof/kof-maintainence/#health-monitoring","title":"Health Monitoring","text":"<p>To implement health monitoring apply the steps in the Verification section.</p>"},{"location":"admin/kof/kof-maintainence/#uninstallation","title":"Uninstallation","text":"<p>To remove the demo clusters created in this section:</p> <p>Warning</p> <p> Make sure these are just your demo clusters and do not contain important data.</p> <pre><code>kubectl delete --wait --cascade=foreground -f child-cluster.yaml\nkubectl delete --wait --cascade=foreground -f regional-cluster.yaml\n</code></pre> <p>To remove KOF from the management cluster:</p> <pre><code>helm uninstall --wait --cascade foreground -n istio-system kof-istio\nhelm uninstall --wait --cascade foreground -n kof kof-child\nhelm uninstall --wait --cascade foreground -n kof kof-regional\nhelm uninstall --wait --cascade foreground -n kof kof-collectors\nhelm uninstall --wait --cascade foreground -n kof kof-storage\nhelm uninstall --wait --cascade foreground -n kof kof-mothership\nhelm uninstall --wait --cascade foreground -n kof kof-operators\nkubectl delete namespace kof --wait --cascade=foreground\n</code></pre>"},{"location":"admin/kof/kof-retention/","title":"KOF Retention and Replication","text":"<p>KOF stores metrics in VictoriaMetrics and logs in VictoriaLogs. Configure retention and replication to balance cost, durability, and compliance.</p> <p>For example, consider these recommendations:</p> <ul> <li>Management cluster: short-term retention (1\u201330 days)</li> <li>Regional clusters: long-term retention (30\u2013365 days)</li> <li>Increase replicationFactor where higher availability is required; this field enables you to determine how many copies are stored, usually on different nodes.</li> </ul> <p>Configure VictoriaMetrics and VictoriaLogs by adjusting the <code>charts/kof-storage/values.yaml</code> file to include the following parameters:</p> <pre><code>victoriametrics:\n  vmcluster:\n    spec:\n      retentionPeriod: \"30d\"\n      replicationFactor: 2\n      vmstorage:\n        storage:\n          volumeClaimTemplate:\n            spec:\n              resources:\n                requests:\n                  storage: 100Gi\n\nvictoria-logs-cluster:\n  vlstorage:\n    extraArgs:\n      retentionPeriod: \"30d\"\n    persistentVolume:\n      size: \"100Gi\"\n</code></pre>"},{"location":"admin/kof/kof-scaling/","title":"Scaling Guidelines","text":"<p>The method for scaling KOF depends on the type of expansion:</p>"},{"location":"admin/kof/kof-scaling/#regional-expansion","title":"Regional Expansion","text":"<ol> <li>Deploy a regional cluster in the new region.</li> <li>Configure child clusters in this region to point to this regional cluster.</li> </ol>"},{"location":"admin/kof/kof-scaling/#adding-a-new-child-cluster","title":"Adding a New Child Cluster","text":"<ol> <li>Apply templates, as in the child cluster section.</li> <li>Verify the data flow.</li> <li>Configure any custom dashboards.</li> </ol>"},{"location":"admin/kof/kof-scaling/#you-must-construct-additional-pylons","title":"You Must Construct Additional Pylons","text":"<ol> <li>Change the <code>replicaCount</code> of components like <code>victoria-logs-cluster</code>     as documented in the regional cluster section.</li> <li>Change the <code>replicas</code> number of components like <code>opencost</code>     as documented in the child cluster section.</li> </ol>"},{"location":"admin/kof/kof-storing/","title":"Storing KOF data","text":""},{"location":"admin/kof/kof-storing/#overview","title":"Overview","text":"<p>KOF data (metrics, logs, traces) can be collected from each cluster and stored in specific places:</p> <pre><code>sequenceDiagram\n    Child cluster-&gt;&gt;Regional cluster: KOF data of the&lt;br&gt;child cluster&lt;br&gt;is stored in the&lt;br&gt;regional cluster.\n    Regional cluster-&gt;&gt;Regional cluster: KOF data of the&lt;br&gt;regional cluster&lt;br&gt;is stored in the same&lt;br&gt;regional cluster.\n    Management cluster-&gt;&gt;Management cluster: KOF data of the&lt;br&gt;management cluster&lt;br&gt;can be stored in:&lt;br&gt;&lt;br&gt;the same management cluster,\n    Management cluster-&gt;&gt;Regional cluster: the regional cluster,\n    Management cluster-&gt;&gt;Third-party storage: a third-party storage,&lt;br&gt;e.g. AWS CloudWatch.</code></pre>"},{"location":"admin/kof/kof-storing/#from-child-and-regional","title":"From Child and Regional","text":"<p>KOF data collected from the child and regional clusters is routed out-of-the box. No additional steps are required here.</p>"},{"location":"admin/kof/kof-storing/#from-management-to-management","title":"From Management to Management","text":"<p>This option stores KOF data of the management cluster in the same management cluster.</p> <ul> <li>Grafana and VictoriaMetrics are provided by the <code>kof-mothership</code> chart,   hence disabled in the <code>kof-storage</code> chart.</li> <li>PromxyServerGroup, VictoriaLogs, and Jaeger are provided by the <code>kof-storage</code> chart.</li> </ul> <p>To apply this option:</p> <ol> <li> <p>Create the <code>storage-values.yaml</code> file:     <pre><code>grafana:\n  enabled: false\n  security:\n    create_secret: false\nvictoria-metrics-operator:\n  enabled: false\nvictoriametrics:\n  enabled: false\npromxy:\n  enabled: true\n</code></pre></p> <p>If you want to use a non-default storage class, add to the <code>storage-values.yaml</code> file: <pre><code>victoria-logs-cluster:\n  vlstorage:\n    persistentVolume:\n      storageClassName: &lt;EXAMPLE_STORAGE_CLASS&gt;\n</code></pre></p> </li> <li> <p>Create the <code>collectors-values.yaml</code> file:     <pre><code>kcm:\n  monitoring: true\nopentelemetry-kube-stack:\n  clusterName: mothership\n  defaultCRConfig:\n    config:\n      processors:\n        resource/k8sclustername:\n          attributes:\n            - action: insert\n              key: k8s.cluster.name\n              value: mothership\n            - action: insert\n              key: k8s.cluster.namespace\n              value: kcm-system\n      exporters:\n        prometheusremotewrite:\n          external_labels:\n            cluster: mothership\n            clusterNamespace: kcm-system\n</code></pre></p> </li> <li> <p>Install the <code>kof-storage</code> and <code>kof-collectors</code> charts to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-storage \\\n  -f storage-values.yaml \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-storage --version 1.4.0\n\nhelm upgrade -i --reset-values --wait -n kof kof-collectors \\\n  -f collectors-values.yaml \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-collectors --version 1.4.0\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-storing/#from-management-to-regional","title":"From Management to Regional","text":"<p>This option stores KOF data of the management cluster in the regional cluster.</p> <p>It assumes that:</p> <ul> <li>You did not enable Istio.</li> <li>You have a regional cluster with the <code>REGIONAL_DOMAIN</code> configured here.</li> </ul> <p>To apply this option:</p> <ol> <li> <p>Create the <code>collectors-values.yaml</code> file:     <pre><code>cat &gt;collectors-values.yaml &lt;&lt;EOF\nkcm:\n  monitoring: true\nopentelemetry-kube-stack:\n  clusterName: mothership\n  defaultCRConfig:\n    env:\n      - name: KOF_VM_USER\n        valueFrom:\n          secretKeyRef:\n            key: username\n            name: storage-vmuser-credentials\n      - name: KOF_VM_PASSWORD\n        valueFrom:\n          secretKeyRef:\n            key: password\n            name: storage-vmuser-credentials\n      - name: KOF_JAEGER_USER\n        valueFrom:\n          secretKeyRef:\n            key: username\n            name: jaeger-admin-credentials\n      - name: KOF_JAEGER_PASSWORD\n        valueFrom:\n          secretKeyRef:\n            key: password\n            name: jaeger-admin-credentials\n    config:\n      processors:\n        resource/k8sclustername:\n          attributes:\n            - action: insert\n              key: k8s.cluster.name\n              value: mothership\n            - action: insert\n              key: k8s.cluster.namespace\n              value: kcm-system\n      extensions:\n        basicauth/metrics:\n          client_auth:\n            username: \\${env:KOF_VM_USER}\n            password: \\${env:KOF_VM_PASSWORD}\n        basicauth/logs:\n          client_auth:\n            username: \\${env:KOF_VM_USER}\n            password: \\${env:KOF_VM_PASSWORD}\n        basicauth/traces:\n          client_auth:\n            username: \\${env:KOF_JAEGER_USER}\n            password: \\${env:KOF_JAEGER_PASSWORD}\n      exporters:\n        prometheusremotewrite:\n          endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/insert/0/prometheus/api/v1/write\n          auth:\n            authenticator: basicauth/metrics\n          external_labels:\n            cluster: mothership\n            clusterNamespace: kcm-system\n        otlphttp/logs:\n          logs_endpoint: https://vmauth.$REGIONAL_DOMAIN/vli/insert/opentelemetry/v1/logs\n          auth:\n            authenticator: basicauth/logs\n        otlphttp/traces:\n          endpoint: https://jaeger.$REGIONAL_DOMAIN/collector\n          auth:\n            authenticator: basicauth/traces\n      service:\n        extensions:\n          - basicauth/metrics\n          - basicauth/logs\n          - basicauth/traces\nopencost:\n  opencost:\n    prometheus:\n      external:\n        url: https://vmauth.$REGIONAL_DOMAIN/vm/select/0/prometheus\nEOF\n</code></pre></p> </li> <li> <p>Install the <code>kof-collectors</code> chart to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-collectors \\\n  -f collectors-values.yaml \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-collectors --version 1.4.0\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-storing/#from-management-to-regional-with-istio","title":"From Management to Regional with Istio","text":"<p>This option stores KOF data of the management cluster in the regional cluster using Istio.</p> <p>It assumes that:</p> <ul> <li>You have Istio enabled.</li> <li>You have a regional cluster with the <code>REGIONAL_CLUSTER_NAME</code> configured here.</li> </ul> <p>To apply this option:</p> <ol> <li> <p>Create the <code>collectors-values.yaml</code> file:     <pre><code>cat &gt;collectors-values.yaml &lt;&lt;EOF\nkcm:\n  monitoring: true\nkof:\n  basic_auth: false\nopentelemetry-kube-stack:\n  clusterName: mothership\n  defaultCRConfig:\n    config:\n      processors:\n        resource/k8sclustername:\n          attributes:\n            - action: insert\n              key: k8s.cluster.name\n              value: mothership\n            - action: insert\n              key: k8s.cluster.namespace\n              value: kcm-system\n      exporters:\n        prometheusremotewrite:\n          endpoint: http://$REGIONAL_CLUSTER_NAME-vminsert:8480/insert/0/prometheus/api/v1/write\n          external_labels:\n            cluster: mothership\n            clusterNamespace: kcm-system\n        otlphttp/logs:\n          logs_endpoint: http://$REGIONAL_CLUSTER_NAME-logs-insert:9481/insert/opentelemetry/v1/logs\n        otlphttp/traces:\n          endpoint: http://$REGIONAL_CLUSTER_NAME-jaeger-collector:4318\nopencost:\n  opencost:\n    prometheus:\n      existingSecretName: \"\"\n      external:\n        url: http://$REGIONAL_CLUSTER_NAME-vmselect:8481/select/0/prometheus\nEOF\n</code></pre></p> </li> <li> <p>Install the <code>kof-collectors</code> chart to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-collectors \\\n  -f collectors-values.yaml \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-collectors --version 1.4.0\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-storing/#from-management-to-third-party","title":"From Management to Third-party","text":"<p>This option stores KOF data of the management cluster in a third-party storage, using the AWS CloudWatch Logs Exporter as an example.</p> <p>Use the most secure option to specify AWS credentials in production.</p> <p>For now, however, just for the sake of this demo, you can use the most straightforward (though less secure) static credentials method:</p> <ol> <li> <p>Create AWS IAM user with access to CloudWatch Logs,     for example, with <code>\"Action\": \"logs:*\"</code> allowed in the inline policy.</p> </li> <li> <p>Create access key and save it to the <code>cloudwatch-credentials</code> file:     <pre><code>AWS_ACCESS_KEY_ID=REDACTED\nAWS_SECRET_ACCESS_KEY=REDACTED\n</code></pre></p> </li> <li> <p>Create the <code>cloudwatch-credentials</code> secret:     <pre><code>kubectl create secret generic -n kof cloudwatch-credentials \\\n  --from-env-file=cloudwatch-credentials\n</code></pre></p> </li> <li> <p>Create the <code>collectors-values.yaml</code> file:     <pre><code>cat &gt;collectors-values.yaml &lt;&lt;EOF\nkcm:\n  monitoring: true\nopentelemetry-kube-stack:\n  clusterName: mothership\n  defaultCRConfig:\n    env:\n      - name: AWS_ACCESS_KEY_ID\n        valueFrom:\n          secretKeyRef:\n            name: cloudwatch-credentials\n            key: AWS_ACCESS_KEY_ID\n      - name: AWS_SECRET_ACCESS_KEY\n        valueFrom:\n          secretKeyRef:\n            name: cloudwatch-credentials\n            key: AWS_SECRET_ACCESS_KEY\n    config:\n      processors:\n        resource/k8sclustername:\n          attributes:\n            - action: insert\n              key: k8s.cluster.name\n              value: mothership\n            - action: insert\n              key: k8s.cluster.namespace\n              value: kcm-system\n      exporters:\n        awscloudwatchlogs:\n          region: us-east-2\n          log_group_name: management\n          log_stream_name: logs\n        prometheusremotewrite: null\n        otlphttp/logs: null\n        otlphttp/traces: null\n      service:\n        pipelines:\n          logs:\n            exporters:\n            - awscloudwatchlogs\n            - debug\n          metrics:\n            exporters:\n            - debug\n          traces:\n            exporters:\n            - debug\nEOF\n</code></pre></p> </li> <li> <p>Install the <code>kof-collectors</code> chart to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-collectors \\\n  -f collectors-values.yaml \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-collectors --version 1.4.0\n</code></pre></p> </li> <li> <p>Configure AWS CLI with the same access key, for verification:     <pre><code>aws configure\n</code></pre></p> </li> <li> <p>Verify that the management cluster logs are stored in the CloudWatch:     <pre><code>aws logs get-log-events \\\n  --region us-east-2 \\\n  --log-group-name management \\\n  --log-stream-name logs \\\n  --limit 1\n</code></pre>     Example of the output:     <pre><code>{\"events\": [{\n  \"timestamp\": 1744305535107,\n  \"message\": \"{\\\"body\\\":\\\"10.244.0.1 - - [10/Apr/2025 17:18:55] \\\\\\\"GET /-/ready HTTP/1.1 200 ...\n</code></pre></p> </li> </ol> <p>See also: KOF Retention for details on configuring retention periods and replication factors for VictoriaMetrics and VictoriaLogs.</p>"},{"location":"admin/kof/kof-tracing/","title":"KOF Tracing","text":"<p>KOF integrates Jaeger for distributed tracing via OpenTelemetry.. By default, Jaeger runs with in-memory storage and a maximum of 100,000 traces. When the limit is reached, it evicts the oldest traces (FIFO). Note that in-memory traces are lost on pod restart.</p> <p>To solve this problem in production, you canuse a persistent backend (e.g., Cassandra, Elasticsearch, or a compatible VictoriaMetrics-Jaeger deployment) and set retention according to your requirements.</p> <p>For example, you can tell Jaeger to use Cassandra by adding the following to the <code>charts/kof-storage/values.yaml</code> file:</p> <pre><code>jaeger:\n  enabled: true\n  spec:\n    strategy: production\n    storage:\n      type: cassandra\n      options:\n        cassandra:\n          servers: cassandra.kof.svc\n          keyspace: jaeger_v1_dc1\n          replication: \"{'class':'NetworkTopologyStrategy','dc1':3}\"\n</code></pre>"},{"location":"admin/kof/kof-upgrade/","title":"Upgrading KOF","text":""},{"location":"admin/kof/kof-upgrade/#upgrade-to-any-version","title":"Upgrade to any version","text":"<ul> <li>Create a backup of each KOF chart values in each cluster, for example:     <pre><code># Management cluster uses default KUBECONFIG=\"\"\nfor cluster in \"\" regional-kubeconfig child-kubeconfig; do\n  for namespace in kof istio-system; do\n    for chart in $(KUBECONFIG=$cluster helm list -qn $namespace); do\n      KUBECONFIG=$cluster helm get values -n $namespace $chart -o yaml \\\n        &gt; values-$cluster-$chart.bak\n    done\n  done\ndone\nls values-*.bak\n</code></pre></li> <li>Ideally, create a backup of everything including VictoriaMetrics/Logs data volumes.</li> <li>Open the latest version of the Installing KOF guide.</li> <li>Make sure you see the expected new version in the top navigation bar.</li> <li>Apply the guide step by step, but:<ul> <li>Skip unchanged credentials like <code>external-dns-aws-credentials</code>.</li> <li>Before applying new YAML files, verify what has changed, for example:     <pre><code>diff -u values--kof-mothership.bak mothership-values.yaml\n</code></pre></li> <li>Run all <code>helm upgrade</code> commands with the new <code>--version</code> and files as documented.</li> </ul> </li> <li>Do the same for other KOF guides.</li> <li>Apply each relevant \"Upgrade to\" section of this page from older to newer.<ul> <li>For example, if you're upgrading from v1.1.0 to v1.3.0,</li> <li>first apply the Upgrade to v1.2.0 section,</li> <li>then apply the Upgrade to v1.3.0 section.</li> </ul> </li> </ul>"},{"location":"admin/kof/kof-upgrade/#upgrade-to-v140","title":"Upgrade to v1.4.0","text":"<ul> <li><code>PromxyServerGroup</code> CRD was moved from the <code>crds/</code> directory to the <code>templates/</code> directory for auto-upgrade.</li> <li>Please use <code>--take-ownership</code> on upgrade of <code>kof-mothership</code> to 1.4.0:     <pre><code>helm upgrade --take-ownership \\\n  --reset-values --wait -n kof kof-mothership -f mothership-values.yaml \\\n  oci://ghcr.io/k0rdent/kof/charts/kof-mothership --version 1.4.0\n</code></pre></li> <li>This will not be required in future upgrades.</li> </ul>"},{"location":"admin/kof/kof-upgrade/#upgrade-to-v130","title":"Upgrade to v1.3.0","text":"<p>Before upgrading any helm chart:</p> <ul> <li> <p>If you have customized VMCluster or VMAlert resources     then update your resources accordingly to the new values under \"spec\".</p> </li> <li> <p>If you have customized VMCluster/VMAlert resources using the \"k0rdent.mirantis.com/kof-storage-values\" cluster annotation, then keep the old but put new values first to reconcile the new <code>ClusterDeployment</code> configuration on the current release, then run the helm charts upgrade. After regional clusters have the new <code>kof-storage</code> helm chart installed, you can remove old values from the cluster annotation.</p> </li> <li> <p>If you have set <code>storage</code> values of the <code>kof-regional</code> chart, update them in the same way.</p> </li> <li> <p>If you are not using Istio,     then on the step 8 of the Management Cluster upgrade     please apply this temporary workaround for the Reconciling MultiClusterService issue:     <pre><code>kubectl rollout restart -n kcm-system deploy/kcm-controller-manager\n</code></pre></p> </li> </ul>"},{"location":"admin/kof/kof-upgrade/#upgrade-to-v120","title":"Upgrade to v1.2.0","text":"<ul> <li>As part of the KOF 1.2.0 overhaul of metrics collection and representation, we switched from the victoria-metrics-k8s-stack metrics and dashboards to opentelemetry-kube-stack metrics and kube-prometheus-stack dashboards.</li> <li>Some of the previously collected metrics have slightly different labels.</li> <li>If consistency of timeseries labeling is important, users are advised to conduct relabeling of the corresponding timeseries in the metric storage by running a retroactive relabeling procedure of their preference.</li> <li>A possible reference solution here would be to use Rules backfilling via vmalert.</li> <li>The labels that would require renaming are these:<ul> <li>Replace <code>job=\"integrations/kubernetes/kubelet\"</code> with <code>job=\"kubelet\", metrics_path=\"/metrics\"</code>.</li> <li>Replace <code>job=\"integrations/kubernetes/cadvisor\"</code> with <code>job=\"kubelet\", metrics_path=\"/metrics/cadvisor\"</code>.</li> <li>Replace <code>job=\"prometheus-node-exporter\"</code> with <code>job=\"node-exporter\"</code>.</li> </ul> </li> </ul> <p>Also:</p> <ul> <li>To upgrade from <code>cert-manager-1-16-4</code> to <code>cert-manager-v1-16-4</code>     please apply this patch to management cluster:     <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplateChain\nmetadata:\n  name: patch-cert-manager-v1-16-4-from-1-16-4\n  namespace: kcm-system\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  supportedTemplates:\n    - name: cert-manager-v1-16-4\n    - name: cert-manager-1-16-4\n      availableUpgrades:\n        - name: cert-manager-v1-16-4\nEOF\n</code></pre></li> </ul>"},{"location":"admin/kof/kof-upgrade/#upgrade-to-v110","title":"Upgrade to v1.1.0","text":"<ul> <li>After you <code>helm upgrade</code> the <code>kof-mothership</code> chart, please run the following:     <pre><code>kubectl apply --server-side --force-conflicts \\\n-f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre></li> <li>After you get <code>regional-kubeconfig</code> file on the KOF Verification step,   please run the following for each regional cluster:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl apply --server-side --force-conflicts \\\n-f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre></li> <li>This is noted as required in the grafana-operator release notes.</li> </ul>"},{"location":"admin/kof/kof-using/","title":"Using KOF","text":"<p>Most of the time, you'll access KOF's data through Grafana.</p>"},{"location":"admin/kof/kof-using/#access-to-grafana","title":"Access to Grafana","text":"<p>To make Grafana available, start with these steps:</p> <ol> <li> <p>Get the Grafana username and password:</p> <pre><code>kubectl get secret -n kof grafana-admin-credentials -o yaml | yq '{\n  \"user\": .data.GF_SECURITY_ADMIN_USER | @base64d,\n  \"pass\": .data.GF_SECURITY_ADMIN_PASSWORD | @base64d\n}'\n</code></pre> </li> <li> <p>Forward a port to the Grafana dashboard:</p> <pre><code>kubectl port-forward -n kof svc/grafana-vm-service 3000:3000\n</code></pre> </li> <li> <p>Login to http://127.0.0.1:3000/dashboards with the username/password printed above.</p> </li> <li> <p>Open a dashboard and select any cluster:</p> </li> </ol> <p></p> <p></p>"},{"location":"admin/kof/kof-using/#single-sign-on","title":"Single Sign-On","text":"<p>Port forwarding, as described above, is a quick solution.</p> <p>Single Single-On provides better experience. If you want to enable it, please apply this advanced guide: SSO for Grafana.</p>"},{"location":"admin/kof/kof-using/#cluster-overview","title":"Cluster Overview","text":"<p>From here you can get an overview of the cluster, including:</p> <ul> <li>Health metrics</li> <li>Resource utilization</li> <li>Performance trends</li> <li>Cost analysis</li> </ul>"},{"location":"admin/kof/kof-using/#logging-interface","title":"Logging Interface","text":"<p>The logging interface will also be available, including:</p> <ul> <li>Real-time log streaming</li> <li>Full-text search</li> <li>Log aggregation</li> <li>Alert correlation</li> </ul>"},{"location":"admin/kof/kof-using/#dashboard-categories","title":"Dashboard Categories","text":"<p>KOF ships with dashboards across: * Infrastructure: Provides infrastructure-related metrics, such as kube clusters, nodes, API server, networking, storage, or GPU. * Applications: Provides metrics for applications, such as VictoriaMetrics, VictoriaLogs, Jaeger and OpenCost. * Service Mesh: Provides metrics for service mesh, such as Istio control-plane and traffic. * Platform: Provides metrics for the platform itself, including KCM, Cluster API, and Sveltos.</p>"},{"location":"admin/kof/kof-using/#dashboard-lifecycle-gitops-workflow","title":"Dashboard Lifecycle (GitOps Workflow)","text":"<p>All dashboards are managed as code to keep environments consistent. To add or change a dashboard, follow these steps:</p> <p>Add a new dashboard 1. Create a YAML file under <code>charts/kof-dashboards/files/dashboards/</code> with the new dashboard definition. 2. Commit and push the change to Git. 3. Your CI/CD pipeline applies the Helm chart to the target cluster.</p> <p>Update an existing dashboard 1. Edit the corresponding YAML file. 2. Commit and push changes. 3. CI/CD will roll out the update automatically.</p> <p>Delete a dashboard 1. Remove the YAML file. 2. Commit and push changes. 3. CI/CD pipeline removes the dashboard from Grafana.</p> <p>Warning</p> <p>Avoid editing dashboards directly in the Grafana UI. Changes will be overwritten by the next Helm release.</p>"},{"location":"admin/kof/kof-using/#cost-management-opencost","title":"Cost Management (OpenCost)","text":"<p>KOF includes OpenCost, which provides cost management features for Kubernetes clusters. Common signals available in Grafana are: * <code>node_total_hourly_cost</code> (per-node hourly cost) * Namespace and pod-level cost allocation * Historical spend trends and efficiency ratios</p> <p>Once you have this information, you can optimize your cluster. Typical optimizations include: * Identify under-utilized resources and right-size workloads * Budgeting and monitoring with Grafana alerts</p> <p>Common OpenCost metrics include:</p> Metric Description <code>node_total_hourly_cost</code> Hourly cost per node (includes CPU, memory, storage) <code>namespace_cpu_cost</code> CPU cost aggregated by namespace <code>namespace_memory_cost</code> Memory cost aggregated by namespace <code>pod_cost</code> Cost allocation at pod granularity <code>cluster_efficiency</code> Ratio of requested vs actual resource usage <p>These metrics appear in the pre-installed Grafana FinOps dashboards.</p>"},{"location":"admin/kof/kof-using/#access-to-jaeger","title":"Access to Jaeger","text":"<p>Jaeger UI of each regional cluster can be accessed by following these steps:</p> <ol> <li> <p>Ensure you have the <code>regional-kubeconfig</code> file created on the verification step.</p> </li> <li> <p>If you've applied the Istio section:</p> <ul> <li> <p>Forward a port to the Jaeger UI:</p> <pre><code>KUBECONFIG=regional-kubeconfig kubectl port-forward \\\n  -n kof svc/kof-storage-jaeger-query 16686:16686\n</code></pre> </li> <li> <p>Open the link http://127.0.0.1:16686/search   and explore the Jaeger UI.</p> </li> </ul> </li> <li> <p>If you have not applied the Istio section:</p> <ul> <li> <p>Ensure you have the <code>REGIONAL_DOMAIN</code> variable set on the installation step.</p> </li> <li> <p>Get the regional Jaeger username and password:</p> <pre><code>KUBECONFIG=regional-kubeconfig kubectl get secret \\\n  -n kof jaeger-admin-credentials -o yaml | yq '{\n  \"user\": .data.username | @base64d,\n  \"pass\": .data.password | @base64d\n}'\n</code></pre> </li> <li> <p>Get the the Jaeger UI URL, open it,     and login with the username/password printed above:</p> <pre><code>echo https://jaeger.$REGIONAL_DOMAIN\n</code></pre> </li> </ul> </li> </ol>"},{"location":"admin/kof/kof-using/#access-to-the-kof-ui","title":"Access to the KOF UI","text":"<p>When the TargetAllocator is in use, the configuration of OpenTelemetryCollectors Prometheus receivers is distributed across the cluster.</p> <p>The KOF UI collects metrics metadata from the same endpoints that are scraped by the Prometheus server:</p> <pre><code>graph TB\n    KOF_UI[KOF UI] --&gt; C1OTC11\n    KOF_UI --&gt; C1OTC1N\n    KOF_UI --&gt; C1OTC21\n    KOF_UI --&gt; C1OTC2N\n    KOF_UI --&gt; C2OTC11\n    KOF_UI --&gt; C2OTC1N\n    KOF_UI --&gt; C2OTC21\n    KOF_UI --&gt; C2OTC2N\n    subgraph Cluster1\n    subgraph C1Node1[Node 1]\n        C1OTC11[OTel Collector]\n        C1OTC1N[OTel Collector]\n    end\n    subgraph C1NodeN[Node N]\n        C1OTC21[OTel Collector]\n        C1OTC2N[OTel Collector]\n    end\n\n    C1OTC11 --PrometheusReceiver--&gt; C1TA[TargetAllocator]\n    C1OTC1N --PrometheusReceiver--&gt; C1TA\n    C1OTC21 --PrometheusReceiver--&gt; C1TA\n    C1OTC2N --PrometheusReceiver--&gt; C1TA\n    end\n    subgraph Cluster2\n    subgraph C2Node1[Node 1]\n        C2OTC11[OTel Collector]\n        C2OTC1N[OTel Collector]\n    end\n    subgraph C2NodeN[Node N]\n        C2OTC21[OTel Collector]\n        C2OTC2N[OTel Collector]\n    end\n\n    C2OTC11 --PrometheusReceiver--&gt; C2TA[TargetAllocator]\n    C2OTC1N --PrometheusReceiver--&gt; C2TA\n    C2OTC21 --PrometheusReceiver--&gt; C2TA\n    C2OTC2N --PrometheusReceiver--&gt; C2TA\n    end</code></pre> <p>You can access the KOF UI by following these steps:</p> <ol> <li> <p>Forward a port to the KOF UI:</p> <pre><code>kubectl port-forward -n kof deploy/kof-mothership-kof-operator 9090:9090\n</code></pre> </li> <li> <p>Open the link http://127.0.0.1:9090</p> </li> <li> <p>Check the state of the endpoints:</p> </li> </ol> <p></p> <p>If there is a misconfiguration in the Prometheus targets (for example, if multiple targets scrape the same URL), the UI will display an error:</p> <p></p> <p>The KOF UI also allows you to monitor internal telemetry from OpenTelemetry collectors and VictoriaMetrics/Logs, enabling comprehensive observability of their health and performance.</p> <p></p> <p>To identify and debug issues in deployed clusters, check if KOF UI shows any errors in these monitored resources:</p> <ul> <li>ClusterDeployment</li> <li>ClusterSummaries</li> <li>MultiClusterService</li> <li>ServiceSet</li> <li>StateManagementProvider</li> <li>SveltosCluster</li> </ul> <p></p>"},{"location":"admin/kof/kof-verification/","title":"KOF Verification","text":"<p>Finally, verify that KOF installed properly.</p>"},{"location":"admin/kof/kof-verification/#verification-steps","title":"Verification steps","text":"<ol> <li> <p>Wait until the value of <code>HELMCHARTS</code> and <code>POLICYREFS</code>     changes from <code>Provisioning</code> to <code>Provisioned</code>:     <pre><code>kubectl get clustersummaries -A -o wide\n</code></pre>     If you see the <code>Failed/Provisioning</code> loop, check status and logs:     <pre><code>kubectl get clustersummaries -A -o yaml \\\n  | yq '.items[].status.featureSummaries[]\n  | select(.status != \"Provisioned\")'\n\nkubectl logs -n kof deploy/kof-mothership-kof-operator | less\nkubectl logs -n projectsveltos deploy/addon-controller | less\n</code></pre></p> </li> <li> <p>Wait for all pods in the regional and child clusters to show as <code>Running</code>     in the namespaces <code>kof, kube-system, projectsveltos</code>:     <pre><code>kubectl get secret -n kcm-system $REGIONAL_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; regional-kubeconfig\n\nkubectl get secret -n kcm-system $CHILD_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; child-kubeconfig\n\nKUBECONFIG=regional-kubeconfig kubectl get pod -A\nKUBECONFIG=child-kubeconfig kubectl get pod -A\n</code></pre></p> </li> <li> <p>Wait until the value of <code>READY</code> changes to <code>True</code>     for all certificates in each cluster:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl get cert -A\nKUBECONFIG=child-kubeconfig kubectl get cert -A\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-verification/#manual-dns-config","title":"Manual DNS config","text":"<p>If you've opted out of DNS auto-config and Istio, you will need to do the following:</p> <ol> <li> <p>Get the <code>EXTERNAL-IP</code> of <code>ingress-nginx</code>:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl get svc \\\n  -n kof ingress-nginx-controller\n</code></pre>     It should look like <code>REDACTED.us-east-2.elb.amazonaws.com</code></p> </li> <li> <p>Create these DNS records of type <code>A</code>, all pointing to that <code>EXTERNAL-IP</code>:     <pre><code>echo grafana.$REGIONAL_DOMAIN\necho jaeger.$REGIONAL_DOMAIN\necho vmauth.$REGIONAL_DOMAIN\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-verification/#kof-ui","title":"KOF UI","text":"<p>You can use the KOF UI to find and debug errors or misconfigurations. Check the Access to the KOF UI section to learn how to access it and read about its features.</p>"},{"location":"admin/kof/kof-version-compat/","title":"Version Compatibility","text":"Component Version Notes k0rdent \u2265 0.0.7 Required for template support Kubernetes \u2265 1.32 Earlier versions untested OpenTelemetry \u2265 0.75 Recommended minimum VictoriaMetrics \u2265 0.40 Required for clustering <p>Detailed:</p> <ul> <li>kof-mothership</li> <li>kof-storage</li> <li>kof-operators</li> <li>kof-collectors</li> </ul>"},{"location":"admin/ksm/","title":"k0rdent State Management","text":"<p>k0rdent State Management (KSM) provides capabilities to manage workloads for k0rdent-managed child Kubernetes clusters. It enables centralized management for applications and auxiliary resources deployed to child clusters. KSM leverage ProjectSveltos as built-in CD solution. Despite of built-in provider, KSM is designed with provider-agnostic capabilities. This allows users to develop their own providers with other underlying CD solution.</p> <ul> <li> <p>Centralized Management: KSM allows to define which services would be installed to which clusters using declaratively defined resources in management cluster.</p> </li> <li> <p>Provider Agnostic: KSM allows to develop custom providers which would reconcile k0rdent API resources in user-defined manner.</p> </li> </ul>"},{"location":"admin/ksm/#guides","title":"Guides","text":"<p>Get started with the basic documentation:</p> <ul> <li>KSM Providers</li> <li>Built-In Provider</li> <li>Service Templates</li> <li>Deploy services to multiple clusters</li> <li>Deploy services to management cluster</li> </ul>"},{"location":"admin/ksm/ksm-built-in-provider/","title":"Built-In KSM Provider","text":""},{"location":"admin/ksm/ksm-built-in-provider/#overview","title":"Overview","text":"<p>k0rdent comes with a built-in KSM provider which relies on ProjectSveltos as a CD solution. This provider is enabled by default by the command-line flag <code>--enable-sveltos-ctrl</code> provided to kcm-controller-manager:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kcm-controller-manager\n  namespace: kcm-system\nspec:\n  # other fields are omitted\n  template:\n    spec:\n      containers:\n        - name: manager\n          args:\n          # other flags are omitted\n          - --enable-sveltos-ctrl=true\n</code></pre> <p>Enabling this flag along with enabling of the projectsveltos provider in <code>Management</code> object results into creation of the <code>StateManagementProvider</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: StateManagementProvider\nmetadata:\n  name: ksm-projectsveltos\nspec:\n  adapter:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: kcm-controller-manager\n    namespace: kcm-system\n    readinessRule: |-\n      self.status.availableReplicas == self.status.replicas &amp;&amp;\n      self.status.availableReplicas == self.status.updatedReplicas &amp;&amp;\n      self.status.availableReplicas == self.status.readyReplicas\n  provisioner:\n  - apiVersion: apps/v1\n    kind: Deployment\n    name: addon-controller\n    namespace: projectsveltos\n    readinessRule: |-\n      self.status.availableReplicas == self.status.replicas &amp;&amp;\n      self.status.availableReplicas == self.status.updatedReplicas &amp;&amp;\n      self.status.availableReplicas == self.status.readyReplicas\n  provisionerCRDs:\n  - group: config.projectsveltos.io\n    resources:\n    - profiles\n    - clustersummaries\n    version: v1beta1\n  selector:\n    matchLabels:\n      ksm.k0rdent.mirantis.com/adapter: kcm-controller-manager\n  suspend: false\n</code></pre> <p>Aside from <code>StateManagementProvider</code> object creation, the controller responsible for <code>ServiceSet</code> object reconciliation will be configured and added to controller-manager.</p> <p>Reconciliation of the <code>ServiceSet</code> objects will result into creation or update of ProjectSveltos-specific objects such as <code>Profile</code> and <code>ClusterProfile</code> which in turn will be reconciled by ProjectSveltos addon-controller referenced as provisioner.</p>"},{"location":"admin/ksm/ksm-built-in-provider/#provider-configuration","title":"Provider Configuration","text":"<p>Note</p> <p>In previous versions the following configuration was a part of <code>ClusterDeployment</code> and <code>MultiClusterService</code> <code>.spec.serviceSpec</code> field. For backward compatibility it is still possible to not to add <code>.spec.serviceSpec.provider</code> field to those objects. In this case the built-in provider will be used and the configuration defined in <code>.spec.serviceSpec</code> will be copied to <code>.spec.provider.config</code> in produced <code>ServiceSet</code> object.</p> <p><code>ClusterDeployment</code>, <code>MultiClusterService</code> and <code>ServiceSet</code> objects have field <code>.spec.provider.config</code> in their Spec. This field is an <code>apiextensionv1.JSON</code> field which allows to pass any supported configuration to the underlying provider-specific objects. For instance built-in adapter which relies on ProjectSveltos API, supports the following configuration which will seamlessly passed from <code>ClusterDeployment</code> or <code>MultiClusterService</code> objects to ProjectSveltos <code>Profile</code> or <code>ClusterProfile</code>:</p> <pre><code>spec:\n  provider:\n    config:\n      # \"syncMode\" specifies how features are synced in a matching workload cluster.\n      # - OneTime means, first time a workload cluster matches the ClusterProfile,\n      # features will be deployed in such cluster. Any subsequent feature configuration\n      # change won't be applied into the matching workload clusters;\n      # - Continuous mode ensures that the first time a workload cluster matches a ClusterProfile,\n      # the specified features are deployed. Subsequent changes to the feature configuration are also\n      # automatically applied to all matching workload clusters.\n      # _ SyncModeContinuousWithDriftDetection operates similarly to Continuous mode, but also monitors\n      # matching managed clusters for configuration drift. If drift is detected, a reconciliation is\n      # triggered to ensure the managed cluster's configuration aligns with the ClusterProfile.\n      # - DryRun means no change will be propagated to any matching cluster. A report\n      # instead will be generated summarizing what would happen in any matching cluster\n      # because of the changes made to ClusterProfile while in DryRun mode.\n      syncMode: OnTime | Continuous | ContinuousWithDriftDetection | DryRun\n      # By default (when \"continueOnError\" is unset or set to false), Sveltos stops deployment after\n      # encountering the first error.\n      # If set to true, Sveltos will attempt to deploy remaining resources in the ClusterProfile even\n      # if errors are detected for previous resources.\n      continueOnError: bool\n      # When \"stopOnConflict\" is set to true, Sveltos stops deployment after\n      # encountering the first conflict (e.g., another ClusterProfile already deployed the resource).\n      # If set to true, Sveltos will attempt to deploy remaining resources in the ClusterProfile even\n      # if conflicts are detected for previous resources.\n      stopOnConflict: bool\n      # \"reload\" indicates whether Deployment/StatefulSet/DaemonSet instances deployed\n      # by Sveltos and part of Profile (or ClusterProfile) need to be restarted via rolling upgrade\n      # when a ConfigMap/Secret instance mounted as volume is modified.\n      # When set to true, when any mounted ConfigMap/Secret is modified, Sveltos automatically\n      # starts a rolling upgrade for Deployment/StatefulSet/DaemonSet instances mounting it.\n      reload: bool\n      # \"priority\" sets the priority for the services defined in this spec.\n      # Higher value means higher priority and lower means lower.\n      # In case of conflict with another object managing the service,\n      # the one with higher priority will get to deploy its services.\n      priority: int32\n      # Define additional Kustomize inline \"patches\" applied for all resources on this profile\n      # Within the Patch Spec you can use templating.\n      # REF: [github.com/projectsveltos/libsveltos/api/v1beta1.Patch]\n      patches: []\n      # \"driftExclusions\" is a list of configuration drift exclusions to be applied when syncMode is\n      # set to ContinuousWithDriftDetection. Each exclusion specifies JSON6902 paths to ignore\n      # when evaluating drift, optionally targeting specific resources and features.\n      # REF: [github.com/projectsveltos/addon-controller/api/v1beta1.DriftExclusion]\n      driftExclusions: []\n      # \"driftIgnore\" is a list of fields selectors to ignore when checking for drift.\n      # REF: [github.com/projectsveltos/libsveltos/api/v1beta1.PatchSelector]\n      driftIgnore: []\n      # \"templateResourceRefs\" is a list of resources to collect from the management cluster,\n      # the values from which can be used in templates.\n      # REF: [github.com/projectsveltos/addon-controller/api/v1beta1.TemplateResourceRef]\n      templateResourceRefs: []\n      # \"policyRefs\" references all the ConfigMaps/Secrets/Flux Sources containing kubernetes resources\n      # that need to be deployed in the matching managed clusters.\n      # The values contained in those resources can be static or leverage Go templates for dynamic customization.\n      # When expressed as templates, the values are filled in using information from\n      # resources within the management cluster before deployment (Cluster and TemplateResourceRefs)\n      # REF: [github.com/projectsveltos/addon-controller/api/v1beta1.PolicyRef]\n      policyRefs: []\n</code></pre>"},{"location":"admin/ksm/ksm-multiclusterservice/","title":"Deploy services using MultiClusterService","text":"<p>The <code>MultiClusterService</code> object is used to deploy services on multiple matching clusters.</p>"},{"location":"admin/ksm/ksm-multiclusterservice/#creation","title":"Creation","text":"<p>You can create the <code>MultiClusterService</code> object with the following YAML:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  name: &lt;name&gt;\nspec:\n  clusterSelector:\n    matchLabels:\n      &lt;key1&gt;: &lt;value1&gt;\n      &lt;key2&gt;: &lt;value2&gt;\n  serviceSpec:\n    services:\n    - template: &lt;servicetemplate-1-name&gt;\n      name: &lt;release-name&gt;\n      namespace: &lt;release-namespace&gt;\n    priority: 100\n</code></pre>"},{"location":"admin/ksm/ksm-multiclusterservice/#matching-multiple-clusters","title":"Matching Multiple Clusters","text":"<p>Consider the following example where two clusters have been deployed using <code>ClusterDeployment</code> objects:</p> <p>Command:</p> <p><pre><code>kubectl get clusterdeployments.k0rdent.mirantis.com -n kcm-system\n</code></pre> <pre><code>NAME             READY   STATUS\ndev-cluster-1   True    ClusterDeployment is ready\ndev-cluster-2   True    ClusterDeployment is ready\n</code></pre></p> <p>Command: <pre><code> kubectl get cluster -n kcm-system --show-labels\n</code></pre> <pre><code>NAME           CLUSTERCLASS     PHASE         AGE     VERSION   LABELS\ndev-cluster-1                  Provisioned   2h41m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-1,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\ndev-cluster-2                  Provisioned   3h10m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-2,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\n</code></pre></p> <p>The <code>dev-cluster-1</code> <code>ClusterDeployment</code> services are specified as: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: dev-cluster-1\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 100\n  . . .\n</code></pre></p> <p>The <code>dev-cluster-2</code> <code>ClusterDeployment</code> beach-head services are specified as: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: dev-cluster-2\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 500\n  . . .\n</code></pre></p> <p>Note</p> <p> See Deploy beach-head Services using Cluster Deployment for how to use beach-head services with ClusterDeployment.</p> <p>Now create the following <code>global-ingress</code> <code>MultiClusterService</code> object: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  name: global-ingress\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    priority: 300\n</code></pre></p> <p>This MultiClusterService will match any CAPI cluster with the label <code>app.kubernetes.io/managed-by: Helm</code> and deploy chart version 4.11.3 of ingress-nginx service on it.</p>"},{"location":"admin/ksm/ksm-multiclusterservice/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Refer to \"Configuring Custom Values\" in Deploy beach-head Services using Cluster Deployment for more information on using custom values.</p>"},{"location":"admin/ksm/ksm-multiclusterservice/#templating-custom-values","title":"Templating Custom Values","text":"<p>Refer to \"Templating Custom Values\" in Deploy beach-head Services using Cluster Deployment for more information about dynamic custom values.</p>"},{"location":"admin/ksm/ksm-multiclusterservice/#service-dependencies","title":"Service Dependencies","text":"<p>Refer to \"Service Dependencies\" in Deploy beach-head Services using Cluster Deployment for more information about service depedencies. The only difference compared to a <code>ClusterDeployment</code> is that when using service dependencies in a <code>MultiClusterService</code> object, the dependencies will be evaluated separately for each of the matching clusters.</p>"},{"location":"admin/ksm/ksm-multiclusterservice/#services-priority-and-conflict","title":"Services Priority and Conflict","text":"<p>The <code>.spec.serviceSpec.priority</code> field specifies the priority for the services managed by a ClusterDeployment or MultiClusterService object.</p> <p>Considering the example above:</p> <ol> <li>ClusterDeployment <code>dev-cluster-1</code> manages deployment of kyverno (v3.2.6) and ingress-nginx (v4.11.0) with <code>priority=100</code> on its cluster.</li> <li>ClusterDeployment <code>dev-cluster-2</code> manages deployment of ingress-nginx (v4.11.0) with <code>priority=500</code> on its cluster.</li> <li>MultiClusterService <code>global-ingress</code> manages deployment of ingress-nginx (v4.11.3) with <code>priority=300</code> on both clusters.    This scenario presents a conflict on both the clusters as the MultiClusterService is attempting to deploy v4.11.3 of ingress-nginx    on both whereas the ClusterDeployment for each is attempting to deploy v4.11.0 of ingress-nginx.</li> </ol> <p>This is where <code>.spec.serviceSpec.priority</code> can be used to specify who gets the priority. Higher number means higer priority and vice versa. In this example:</p> <ol> <li>MultiClusterService \"global-ingress\" will take precedence over ClusterDeployment \"dev-cluster-1\" and ingress-nginx (v4.11.3) defined in MultiClusterService object will be deployed on the cluster.</li> <li>ClusterDeployment \"dev-cluster-2\" will take precedence over MultiClusterService \"global-ingress\" and ingress-nginx (v4.11.0) defined in ClusterDeployment object will be deployed on the cluster.</li> </ol> <p>Note</p> <p> If <code>priority</code> values are equal, the first one to reach the cluster wins and deploys its beach-head services.</p>"},{"location":"admin/ksm/ksm-multiclusterservice/#mcs-dependencies","title":"MCS Dependencies","text":"<p>Dependencies among MultiClusterServices can be defined using the <code>.spec.dependsOn[]</code> field in the <code>MultiClusterService</code> object. If <code>mcs2</code> depends on <code>mcs1</code>, then services defined by <code>mcs2</code> will not be deployed on a matching cluster until all services defined by <code>mcs1</code> have been successfully deployed on that cluster. For Example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  name: mcs1\nspec:\n  clusterSelector:\n    matchLabels:\n      owner: dev-team\n  serviceSpec:\n    services:\n      - template: cert-manager-1-18-2\n        name: cert-manager\n        namespace: cert-manager\n        values: |\n          cert-manager:\n            crds:\n              enabled: true\n---\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  name: mcs2\nspec:\n  clusterSelector:\n    matchLabels:\n      owner: dev-team\n  dependsOn:\n    - mcs1\n  serviceSpec:\n    services:\n      - template: ingress-nginx-4-13-0\n        name: ingress-nginx\n        namespace: ingress-nginx\n        values: |\n          ingress-nginx:\n            controller:\n              replicaCount: 3\n      - template: postgres-operator-1-14-0\n        name: postgres-operator\n        namespace: postgres-operator\n        dependsOn:\n          - name: ingress-nginx\n            namespace: ingress-nginx\n</code></pre> <p>In this example, for all matching clusters, <code>ingress-nginx</code> and <code>postgres-operators</code> defined by <code>mcs2</code> will not be deployed until <code>cert-manager</code> defined by <code>mcs1</code> has been successfully deployed.</p>"},{"location":"admin/ksm/ksm-multiclusterservice/#checking-status","title":"Checking Status","text":"<p>The status for the <code>MultiClusterService</code> object shows the deployment status for the beach-head services managed by it on each of the CAPI target clusters that it matches. Consider the same example where 2 ClusterDeployments and 1 MultiClusterService is deployed. The status for the <code>global-ingress</code> <code>MultiClusterService</code> appears as:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  . . .\n  name: global-ingress\n  resourceVersion: \"38146\"\n  . . .\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    . . .\n  . . .\nstatus:\n  conditions:\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: \"\"\n    reason: Succeeded\n    status: \"True\"\n    type: SveltosClusterProfileReady\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: MultiClusterService is ready\n    reason: Succeeded\n    status: \"True\"\n    type: Ready\n  observedGeneration: 1\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary p--dev-cluster-2-capi-dev-cluster-2 managing it.\n      reason: Failed\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary p--dev-cluster-2-capi-dev-cluster-2\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>The status under <code>.status.services</code> shows a conflict for <code>dev-cluster-2</code> as expected because the <code>MultiClusterService</code> has a lower priority. On the other hand, it shows provisioned for <code>dev-cluster-1</code> because the <code>MultiClusterService</code> has a higher priority.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . . \n  name: dev-cluster-1\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 100\n    . . .\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary global-ingress-capi-dev-cluster-1 managing it.\n      reason: Provisioning\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T07:44:43Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary global-ingress-capi-dev-cluster-1\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>The status under <code>.status.services</code> for the <code>ClusterDeployment</code> <code>dev-cluster-1</code> shows that it is managing Kyverno but unable to manage ingress-nginx because another object with higher priority is managing it, so it shows a conflict instead.</p> <p>On the otherhand, the <code>dev-cluster-2</code> <code>ClusterDeployment</code> has a higher priority: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  name: dev-cluster-2\n  namespace: kcm-system\n  resourceVersion: \"30889\"\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 500\n    . . .\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> for <code>ClusterDeployment</code> <code>dev-cluster-2</code> shows that it is managing ingress-nginx, as expected because it has a higher priority.</p>"},{"location":"admin/ksm/ksm-multiclusterservice/#parameter-list","title":"Parameter List","text":"<p>Refer to \"Parameter List\" in Deploy beach-head Services using Cluster Deployment for more information.</p>"},{"location":"admin/ksm/ksm-providers/","title":"KSM Providers","text":"<p>k0rdent provides API to make underlying CD software pluggable and leverage it for KSM purposes. There are two crucial API types: <code>StateManagementProvider</code> and <code>ServiceSet</code>.</p>"},{"location":"admin/ksm/ksm-providers/#statemanagementprovider","title":"StateManagementProvider","text":"<p>The goal of <code>StateManagementProvider</code> is to ensure that the <code>ServiceSet</code> objects produced by k0rdent controllers will be reconciled and transformed to provider-specific objects. Aside from that, the controller which reconciles <code>StateManagementProvider</code> objects, will validate that all components - adapter, provisioner and required custom resources - are ready to be used.  User-defined CEL rules will be used for this purpose.</p> <p>Here is an example of built-in <code>StateManagementProvider</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: StateManagementProvider\nmetadata:\n  name: ksm-projectsveltos\nspec:\n  adapter:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: kcm-controller-manager\n    namespace: kcm-system\n    readinessRule: |-\n      self.status.availableReplicas == self.status.replicas &amp;&amp;\n      self.status.availableReplicas == self.status.updatedReplicas &amp;&amp;\n      self.status.availableReplicas == self.status.readyReplicas\n  provisioner:\n  - apiVersion: apps/v1\n    kind: Deployment\n    name: addon-controller\n    namespace: projectsveltos\n    readinessRule: |-\n      self.status.availableReplicas == self.status.replicas &amp;&amp;\n      self.status.availableReplicas == self.status.updatedReplicas &amp;&amp;\n      self.status.availableReplicas == self.status.readyReplicas\n  provisionerCRDs:\n  - group: config.projectsveltos.io\n    resources:\n    - profiles\n    - clustersummaries\n    version: v1beta1\n  selector:\n    matchLabels:\n      ksm.k0rdent.mirantis.com/adapter: kcm-controller-manager\n  suspend: false\nstatus:\n  conditions:\n  - lastTransitionTime: \"2025-08-26T10:58:26Z\"\n    message: Successfully ensured RBAC\n    observedGeneration: 1\n    reason: RBACEnsuredSuccessfully\n    status: \"True\"\n    type: RBACReady\n  - lastTransitionTime: \"2025-08-27T21:29:49Z\"\n    message: Successfully ensured adapter\n    observedGeneration: 1\n    reason: AdapterEnsuredSuccessfully\n    status: \"True\"\n    type: AdapterReady\n  - lastTransitionTime: \"2025-08-26T10:59:37Z\"\n    message: Successfully ensured provisioner\n    observedGeneration: 1\n    reason: ProvisionerEnsuredSuccessfully\n    status: \"True\"\n    type: ProvisionerReady\n  - lastTransitionTime: \"2025-08-26T10:59:17Z\"\n    message: Successfully ensured provisioner CRDs\n    observedGeneration: 1\n    reason: ProvisionerCRDsEnsuredSuccessfully\n    status: \"True\"\n  ready: true\n</code></pre>"},{"location":"admin/ksm/ksm-providers/#adapter","title":"Adapter","text":"<p>This is some workload deployed in management cluster which will reconcile <code>ServiceSet</code> objects. Built-in adapter comes bundled as a part of <code>kcm-controller-manager</code> binary. Adapter is expected to convert <code>ServiceSpec</code> object being reconciled to provider-specific object and collect status of defined services.</p>"},{"location":"admin/ksm/ksm-providers/#provisioner","title":"Provisioner","text":"<p>This is a set of or a single workload which will reconcile provider-specific objects produced by the adapter. For built-in provider it's a <code>addon-controller</code> provided by ProjectSveltos.</p>"},{"location":"admin/ksm/ksm-providers/#provisioner-crds","title":"Provisioner CRDs","text":"<p>This is a set of CRDs which are required by adapter and provisioners to operate normally.</p> <p>Note</p> <p><code>StateManagementProvider</code> controller does not deploy any workloads or CRDs to the management cluster. The only goal of this controller is to ensure that all components, required to deploy services, exist and operate normally.</p>"},{"location":"admin/ksm/ksm-providers/#serviceset","title":"ServiceSet","text":"<p>The goal of <code>ServiceSet</code> is to act as a Man-in-The-Middle between k0rdent API and provider-specific objects.</p> <p>Here is an example of <code>ClusterDeployment</code> object with defined service to be deployed and the <code>ServiceSet</code> object produced out of <code>ClusterDeployment</code> spec:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: sample-cluster\n  namespace: kcm-system\nspec:\n  # spec fields related to cluster configuration are omitted\n  serviceSpec:\n    provider:\n      name: kcm-projectsveltos\n      config:\n        continueOnError: false\n        priority: 100\n        stopOnConflict: false\n        syncMode: Continuous\n    services:\n      - template: ingress-nginx-4-11-0\n        name: managed-ingress\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceSet\nmetadata:\n  name: sample-cluster\n  namespace: kcm-system\n  labels:\n    ksm.k0rdent.mirantis.com/adapter: kcm-controller-manager\nspec:\n  cluster: kcm-system\n  provider:\n    name: kcm-projectsveltos\n    config:\n      continueOnError: false\n      priority: 100\n      stopOnConflict: false\n      syncMode: Continuous\n  services:\n    - template: ingress-nginx-4-11-0\n      name: managed-ingress\n      namespace: default\n</code></pre>"},{"location":"admin/ksm/ksm-self-management/","title":"Deploy beach-head services on Management Cluster itself","text":"<p>There is a SveltosCluster object named mgmt in the mgmt namespace where k0rdent has been installed. This object represents the management cluster itself. The following command can be used to get this object along with labels:</p> <pre><code>kubectl -n mgmt get sveltoscluster mgmt --show-labels\n</code></pre> <p>The output should be similar to this:</p> <pre><code>NAME   READY   VERSION   LABELS\nmgmt   true    v1.32.2   k0rdent.mirantis.com/management-cluster=true,projectsveltos.io/k8s-version=v1.32.2,sveltos-agent=present\n</code></pre> <p>To deploy beach-head services on the management cluster, a MultiClusterService object can be created that matches the mgmt SveltosCluster using the k0rdent.mirantis.com/management-cluster=true and sveltos-agent=present labels as shown in the following YAML:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  name: mgmt-mcs\nspec:\n  clusterSelector:\n    matchLabels:\n      k0rdent.mirantis.com/management-cluster: \"true\"\n      sveltos-agent: present\n  serviceSpec:\n    provider:\n      selfManagement: true\n    services:\n      - template: ingress-nginx-4-11-3\n        name: ingress-nginx\n        namespace: ingress-nginx\n</code></pre> <p>Any number of ServiceTemplates (ingress-nginx-4-11-3 in this example) can be added to the MultiClusterService's <code>.spec.serviceSpec.services</code> field. See Using and Creating ServiceTemplates for how to create ServiceTemplates.</p> <p>To verify that the ingress-nginx-4-11-3 beach-head service was sucessfully deployed, the status of the MultiClusterService can be queried with:</p> <pre><code>kubectl get multiclusterservice mgmt -o yaml\n</code></pre> <p>The output should be similar to the following showing that ingress-nginx has been Provisioned:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\n. . .\nstatus:\n  . . .\n  services:\n  - clusterName: mgmt\n    clusterNamespace: mgmt\n    conditions:\n    - lastTransitionTime: \"2025-04-24T10:35:03Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2025-04-24T10:35:03Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>See Creating multi-cluster services for more detail on deploying beach-head services using MultiClusterService.</p>"},{"location":"admin/ksm/ksm-service-templates/","title":"Using and Creating ServiceTemplates: Creating and Deploying Applications","text":"<p>Deploying an application, like deploying a cluster, involves applying a template to the management cluster. Rather than a <code>ClusterTemplate</code>, however, applications are deployed using a <code>ServiceTemplate</code>.</p> <p>You can find more information on Bringing Your Own Templates in the Template Guide, but this section gives you an idea of how to create a <code>ServiceTemplate</code> and use it to deploy an application to a k0rdent child cluster.</p> <p><code>ServiceTemplate</code> supports the following types as a source:</p> <ul> <li><code>HelmChart</code></li> <li><code>GitRepository</code></li> <li><code>Bucket</code></li> <li><code>OCIRepository</code></li> <li><code>Secret</code></li> <li><code>ConfigMap</code></li> </ul>"},{"location":"admin/ksm/ksm-service-templates/#creating-helm-based-servicetemplate","title":"Creating Helm-based ServiceTemplate","text":"<p>Before creating a <code>ServiceTemplate</code>, the source of the Helm chart that represents the service can be created. The source object must have the label <code>k0rdent.mirantis.com/managed: \"true\"</code>. For example, this YAML describes a FluxCD source object of <code>kind</code> <code>HelmRepository</code>:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-templates-repo\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/external-templates-repo/charts\n</code></pre>"},{"location":"admin/ksm/ksm-service-templates/#create-the-servicetemplate","title":"Create the <code>ServiceTemplate</code>","text":"<p>Helm-based <code>ServiceTemplate</code> can be created in three ways:</p> <ul> <li>by defining Helm chart right in the template object</li> <li>by referring the existing Helm chart</li> <li>by defining or referring source provided by FluxCD</li> </ul> <p>Note</p> <p> <code>ServiceTemplate</code> can be defined using <code>.spec.helm.chartSpec</code> or <code>.spec.helm.chartRef</code> if only Helm chart being defined or referred is backed by <code>HelmRepository</code> or <code>GitRepository</code> object. To use FluxCD sources, <code>ServiceTemplate</code> must be defined using <code>.spec.helm.chartSource</code> field.</p> <p>FluxCD sources supported by <code>ServiceTemplate</code> are:</p> <ul> <li><code>GitRepository</code></li> <li><code>Bucket</code></li> <li><code>OCIRepository</code></li> </ul>"},{"location":"admin/ksm/ksm-service-templates/#examples","title":"Examples","text":"<ol> <li>Explicitly defining Helm chart</li> </ol> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  helm:\n    chartSpec:\n      chart: ingress-nginx\n      version: 4.11.3\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n</code></pre> <p>In this case, we're creating a <code>ServiceTemplate</code> called <code>ingress-nginx-4.11.3</code> in the    <code>my-target-namespace</code> namespace.  It references version 4.11.3 of the <code>ingress-nginx</code> chart    located in the <code>k0rdent-catalog</code> Helm repository. The <code>HelmChart</code> object will be created by the controller.</p> <ol> <li>Referring the existing Helm chart</li> </ol> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  helm:\n    chartRef:\n      kind: HelmChart\n      name: ingress-nginx-4.11.3\n</code></pre> <p>In this case, we're creating a <code>ServiceTemplate</code> called <code>ingress-nginx-4.11.3</code> in the    <code>my-target-namespace</code> namespace.  It references the existing <code>ingress-nginx-4.11.3</code> Helm chart.</p> <ol> <li>Referring existing FluxCD source</li> </ol> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  helm:\n    chartSource:\n      localSourceRef:\n        kind: GitRepository\n        name: k0rdent-catalog\n      path: \"./charts/ingress-nginx\"\n</code></pre> <p>In this case, we're creating a <code>ServiceTemplate</code> called <code>ingress-nginx-4.11.3</code> in the    <code>my-target-namespace</code> namespace.  It references the existing <code>GitRepository</code> object called    <code>k0rdent-catalog</code> and the Helm chart located in the <code>charts/ingress-nginx</code> path.</p> <ol> <li>Defining Helm chart source, which can be one of types provided by FluxCD:</li> </ol> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  helm:\n    chartSource:\n      remoteSourceSpec:\n        bucket:\n          url: s3://bucket/path/to/charts\n          interval: 10m0s\n        path: \"./ingress-nginx\"\n</code></pre> <p>In this case, we're creating a <code>ServiceTemplate</code> called <code>ingress-nginx-4.11.3</code> in the    <code>my-target-namespace</code> namespace.  It defines the source of the Helm chart located in the    <code>s3://bucket/path/to/charts</code> bucket and path where the Helm chart is located within the bucket.</p> <p>For more information on creating templates, see the Template Guide.</p>"},{"location":"admin/ksm/ksm-service-templates/#creating-kustomization-based-servicetemplate","title":"Creating Kustomization-based ServiceTemplate","text":"<p>Define the source of the Kustomization that defines the service. If the source object is one of Flux source - <code>GitRepository</code>, <code>Bucket</code> or <code>OCIRepository</code> - it must have the label <code>k0rdent.mirantis.com/managed: \"true\"</code>.</p> <p>Source object can be already created. In this case <code>.spec.kustomization.localSourceRef</code> should be used:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kustomization-app\n  namespace: kcm-system\nspec:\n  kustomize:\n    localSourceRef:\n      kind: GitRepository\n      name: project-x\n    deploymentType: Remote\n    path: \"./base\"\n</code></pre> <p>Aside from flux sources, local <code>ConfigMap</code> or <code>Secret</code> object can be used as a source of the kustomization manifests. The <code>ServiceTemplate</code> definition will not differ from one in above, except the <code>.spec.kustomize.localSourceRef.kind</code> which should be set to respective object type. However, to use <code>ConfigMap</code> or <code>Secret</code> as a source, they must embed archive with kustomization. After the archive was created, this can be done by executing the following command:</p> <pre><code>kubectl create configmap kustomization-source --from-file=/path/to/kustomization.tar.gz\n</code></pre> <p>Another option is to let the controller to create the remote source object. In this case <code>.spec.kustomization.remoteSourceSpec</code> should be used:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kustomization-app\n  namespace: kcm-system\nspec:\n  kustomize:\n    remoteSourceSpec:\n      oci:\n        url: oci://ghcr.io/org/project-x\n        reference:\n          tag: latest\n        interval: 10m\n    deploymentType: Remote\n    path: \"./overlays\"\n</code></pre> <p>When <code>.spec.kustomize.remoteSourceSpec</code> is defined, the controller will create corresponding object.</p>"},{"location":"admin/ksm/ksm-service-templates/#creating-raw-resources-based-servicetemplate","title":"Creating Raw-Resources-based ServiceTemplate","text":"<p>This type of source is quite similar to the kustomization sources, with the only exception: when using <code>ConfigMap</code> or <code>Secret</code> as a source, the field <code>.spec.resources.localSourceRef.path</code> will be ignored and the resources' manifests to be deployed must be inlined in the source's <code>data</code>.</p> <p>Example of the <code>ConfigMap</code> and corresponding <code>ServiceTemplate</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: project-cm\n  namespace: project-x\ndata:\n  namespace.yaml: |\n    apiVersion: v1\n    kind: Namespace\n    metadata:\n      name: managed-ns\n</code></pre> <p>To create such <code>ConfigMap</code> the following command can be used:</p> <pre><code>kubectl --namespace project-x create configmap project-cm --from-file=/path/to/namespace.yaml\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: managed-ns\n  namespace: project-x\nspec:\n  resources:\n    localSourceRef:\n      kind: ConfigMap\n      name: project-cm\n    deploymentType: Remote\n    path: \"\"  # will be ignored\n</code></pre> <p>Using the remote source for <code>ServiceTemplate</code> based on raw resources is similar to the kustomization-based template:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kustomization-app\n  namespace: kcm-system\nspec:\n  resources:\n    remoteSourceSpec:\n      git:\n        url: https://github.com/org/project-x.git\n        reference:\n          branch: main\n        interval: 10m\n    deploymentType: Remote\n    path: \"./overlays\"\n</code></pre>"},{"location":"admin/ksm/ksm-service-templates/#deploying-application-with-servicetemplate","title":"Deploying Application with ServiceTemplate","text":""},{"location":"admin/ksm/ksm-service-templates/#create-a-servicetemplatechain","title":"Create a <code>ServiceTemplateChain</code>","text":"<p>To let k0rdent know where this <code>ServiceTemplate</code> can and can't be used, create a <code>ServiceTemplateChain</code> object, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplateChain\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  supportedTemplates:\n    - name: project-ingress-nginx-4.11.3\n    - name: project-ingress-nginx-4.10.0\n      availableUpgrades:\n        - name: project-ingress-nginx-4.11.3\n</code></pre> <p>Here you see a template called <code>project-ingress-nginx-4.11.3</code> that is meant to be deployed in the <code>my-target-namespace</code> namespace. The <code>.spec.helm.chartSpec</code> specifies the name of the Helm chart and where to find it, as well as the version and other important information. The <code>ServiceTempateChain</code> shows that this template is also an upgrade path from version 4.10.0.</p> <p>If you wanted to deploy this as an application, you would first go ahead and add it to the cluster in which you were working, so if you were to save this YAML to a file called <code>project-ingress.yaml</code> you could run this command on the management cluster:</p> <pre><code>kubectl apply -f project-ingress.yaml -n my-target-namespace\n</code></pre>"},{"location":"admin/ksm/ksm-service-templates/#adding-a-service-to-a-clusterdeployment","title":"Adding a <code>Service</code> to a <code>ClusterDeployment</code>","text":"<p>To add the service defined by this template to a cluster, you would simply add it to the <code>ClusterDeployment</code> object when you create it, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-1-0-16\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.3\n        name: ingress-nginx\n        namespace: my-target-namespace\n    priority: 100\n</code></pre> As you can see, you're simply referencing the template in the <code>.spec.serviceSpec.services[].template</code> field of the <code>ClusterDeployment</code> to tell k0rdent that you want this service to be part of this cluster.</p> <p>If you wanted to add this service to an existing cluster, you would simply patch the definition of the <code>ClusterDeployment</code>, as in:</p> <p><pre><code>kubectl patch clusterdeployment my-cluster-deployment -n my-target-namespace --type='merge' -p '{\"spec\":{\"serviceSpec\":{\"services\":[{\"template\":\"project-ingress-nginx-4.11.3\",\"name\":\"ingress-nginx\",\"namespace\":\"my-target-namespace\"}]}}}'\n</code></pre> For more information on creating and using <code>ServiceTemplate</code> objects, see the User Guide.</p>"},{"location":"admin/regional-clusters/","title":"Regional Clusters","text":"<p>Note</p> <p> Regional clusters are available starting from version 1.4.0.</p> <p>Warning</p> <p> There are some limitations in deploying regional clusters, which will be addressed in upcoming k0rdent releases:</p> <ul> <li>Automated credential distribution is not yet supported. You must manually create the corresponding <code>ClusterIdentity</code> objects (e.g., <code>Secrets</code>, <code>AWSClusterIdentity</code>) on each regional cluster.</li> <li>The kubeconfig <code>Secret</code> must exist in the system namespace (default: <code>kcm-system</code>) when registering a new region.</li> </ul> <p>k0rdent can manage thousands of clusters, potentially over multiple clouds and infrastructure domains.  It's possible to do all of this within a single k0rdent management cluster. In that case, the management cluster works to manage child clusters across multiple clouds and infrastructures, and serves as the operator's 'single pane of glass'. It collects metrics, provides visibility, and so on. Moreover, k0rdent is designed so that a single  manager does not become a single point of failure. The management cluster can be backed up quite simply (see Backups  and restored to its prior state quickly without disrupting operations across the IT estate.</p> <p>However, k0rdent can also work in a distributed arrangement, where each infrastructure domain (for example, a  cloud provider, a region within a cloud provider, and so on) is partly controlled by a Regional Cluster. A k0rdent  Regional Cluster is not a full-featured k0rdent manager, but a drone cluster containing:</p> <ul> <li>CAPI components pertinent to the infrastructure the Regional Cluster is managing. Infrastructure operations (often time-consuming) are distributed by the manager to the Regional Cluster for execution.</li> <li>KOF components including regional metrics and logs aggregation</li> <li>Backup components such as Velero</li> <li>and so on</li> </ul> <p>When using regional clusters, the management cluster remains the single pane of glass, responsible for managing <code>ClusterDeployments</code>, <code>Credentials</code>, <code>Templates</code>, and other k0rdent resources. It then deploys the actual CAPI objects that implement those k0rdent resources on the regional cluster, a plain (non-k0rdent)  Kubernetes cluster where provider-related infrastructure (such as CAPI providers) and the objects that implement it (such as <code>Cluster</code>, and <code>Machine</code> objects) are deployed. </p> <p>Regional clusters provide several benefits:</p> <ul> <li>Cluster resources can be distributed across multiple, purpose-specific clusters.</li> <li>Improved fault isolation, because the management cluster only needs to deal with k0rdent objects.</li> <li>Geographic placement of resources.</li> </ul> <p>For more details, see:</p> <ul> <li>Understanding Regional Components Segregation</li> <li>Register Regional Cluster</li> <li>Creating Credential in Region</li> <li>Deploying Clusters in Region</li> </ul>"},{"location":"admin/regional-clusters/components-segregation/","title":"Understanding Regional Components Segregation","text":"<p>Up until 1.4.0, all k0rdent components including the API and cluster resources ran in the management cluster. With regional support, you\u2019ll now be able to register another cluster as a regional cluster. As a result:</p> <ul> <li>The management cluster can focus on control plane operations. The management cluster is a single place to manage <code>ClusterDeployments</code>, <code>Credentials</code>, <code>Backups</code>, <code>Templates</code>, and so on.</li> <li>The regional cluster becomes the place where the actual objects to implement those k0rdent concepts, such as <code>Clusters</code> and <code>Machines</code>, and the related infrastructure, actually created.</li> </ul> <p>Depending on the setup, the management cluster in k0rdent can be dedicated exclusively to control-plane operations, with no providers enabled (in <code>spec.providers</code> of the Management object). In this setup, it can serve only as a single pane of glass for managing the k0rdent platform, hosting no actual child cluster workloads, just their definitions for management.</p>"},{"location":"admin/regional-clusters/components-segregation/#regional-cluster-components","title":"Regional Cluster Components","text":"<p>A regional cluster contains the following regional core components installed by k0rdent:</p> <ol> <li>CAPI operator</li> <li>CAPI providers (enabled in the <code>spec.providers</code> of the <code>Region</code> object)</li> <li>Cert Manager</li> <li>Velero and so on.</li> </ol> <p>The following cluster-related resources are created within the regional cluster (for <code>ClusterDeployments</code> that were created in the region):</p> <ol> <li>All infrastructure objects (CAPI cluster, machines, and so on)</li> <li><code>ClusterIdentity</code> objects for accessing cloud resources</li> <li>Sveltos objects</li> </ol>"},{"location":"admin/regional-clusters/components-segregation/#management-cluster-components","title":"Management Cluster Components","text":"<p>A management cluster contains the following components:</p> <ol> <li>All regional core components (CAPI operator, Cert Manager, Velero, and so on)</li> <li>Only the CAPI providers enabled in the <code>spec.providers</code> field of the <code>Management</code> object</li> <li>The K0rdent API (<code>Credential</code>, <code>ClusterDeployment</code>, <code>Templates</code>, and so on)</li> <li>Flux's Helm and Source controllers to install and manage subcomponents and so on.</li> </ol> <p>Additionally, the management cluster hosts the cluster-related resources (<code>Cluster</code>, <code>Machine</code>, and so on) for  ClusterDeployments that were created in the management cluster (and not a regional cluster), which is the default.</p>"},{"location":"admin/regional-clusters/creating-credential-in-region/","title":"Creating a Credential in a Region","text":"<p>Once a <code>Region</code> is deployed and ready, you can proceed with <code>Credential</code> configuration.</p> <p>Credentials are required for k0rdent to communicate with the infrastructure provider (such as AWS, Azure, vSphere, and so on). They enable provisioning of resources such as virtual machines, networking components, and storage.</p> <p>The <code>Credential</code> <code>spec</code> has been extended with a <code>region</code> field, which specifies the name of the <code>Region</code> object in which the <code>Credential</code> applies. This is the only place where the region is configured. When <code>spec.region</code> is set, any <code>ClusterDeployment</code> referencing this Credential will be deployed to the corresponding regional cluster.</p> <p>Note</p> <p> Each <code>Credential</code> maps 1:1 to a Region. If <code>spec.region</code> is empty, the <code>Credential</code> is tied to the management cluster and deployment will proceed as though there are no regional clusters.</p>"},{"location":"admin/regional-clusters/creating-credential-in-region/#creating-cluster-identity-objects","title":"Creating Cluster Identity objects","text":"<p>Warning</p> <p> Starting from k0rdent v1.5.0 the ClusterIdentity resources for regional Credential are synced with the regional cluster. Which means, when the regional Credential object references some ClusterIdentity as <code>identityRef</code> (for example <code>AWSClusterStaticIdentity</code>) this identity object and all referenced objects (for example, <code>Secrets</code>) will be copied from the management to the regional cluster. To get more information follow Regional Credential Distribution.</p> <p>Before k0rdent v1.5.0 you should manually create <code>ClusterIdentity</code> resources for regional Credential in the regional cluster, and not the management cluster.</p> <p>This does not apply to the resource template <code>ConfigMap</code> which should be manually created in the regional cluster.</p> <p>Depending on the provider, you need to create <code>ClusterIdentity</code> resources to allow provider components to interact with the cloud.</p>"},{"location":"admin/regional-clusters/creating-credential-in-region/#example-aws","title":"Example: AWS","text":"<ol> <li> <p>Create the <code>Secret</code> with your AWS cloud credential</p> <p>Create a YAML file called <code>aws-cluster-identity-secret.yaml</code> and add the following text, replacing the <code>EXAMPLE_ACCESS_KEY_ID</code> and <code>EXAMPLE_SECRET_ACCESS_KEY</code> with corresponding cloud values:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n  SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system --kubeconfig &lt;path-to-management-cluster-kubeconfig&gt;\n</code></pre> </li> <li> <p>Create the <code>AWSClusterStaticIdentity</code></p> </li> </ol> <p>Create the <code>AWSClusterStaticIdentity</code> object in a file named <code>aws-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that the <code>secretRef</code> references the <code>Secret</code> you created in the previous step.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml --kubeconfig &lt;path-to-management-cluster-kubeconfig&gt;\n</code></pre> <ol> <li>Create the <code>ClusterIdentity</code> resource template <code>ConfigMap</code> in the regional cluster</li> </ol> <p>Warning</p> <p> Create <code>ClusterIdentity</code> resource template <code>ConfigMap</code> in the regional cluster, and not the management cluster.</p> <p>Now we create <code>ClusterIdentity</code> resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>aws-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\n</code></pre> <p>Note that <code>ConfigMap</code> is empty. This is expected as we don't need to template any objects inside child cluster(s) for now, but we can use that object in the future if need arises.</p> <p>Apply the YAML to your regional cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-resource-template.yaml -n kcm-system --kubeconfig &lt;path-to-regional-cluster-kubeconfig&gt;\n</code></pre>"},{"location":"admin/regional-clusters/creating-credential-in-region/#creating-the-credential","title":"Creating the Credential","text":"<p>Warning</p> <p> The <code>Credential</code> should be created in the management cluster and not the regional cluster</p> <p>After configuring the <code>ClusterIdentity</code> objects in the regional cluster, create the <code>Credential</code> in the management cluster, referencing that <code>ClusterIdentity</code>.</p> <p>For AWS, the <code>Credential</code> should be created in the same namespace as the previously created <code>ClusterIdentity</code> objects. The <code>spec.region</code> should be configured and refer the name of the <code>Region</code> object that points to the cluster where the <code>ClusterIdentity</code> resources have been created:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\n  metadata:\n    name: aws-cluster-identity-cred\n    namespace: kcm-system\n  spec:\n    region: region1\n    description: \"Credential Example\"\n    identityRef:\n      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n      kind: AWSClusterStaticIdentity\n      name: aws-cluster-identity\n</code></pre> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre>"},{"location":"admin/regional-clusters/creating-credential-in-region/#verifying-credential-status","title":"Verifying Credential Status","text":"<p>After creation, the <code>Credential</code> is validated and its status will reflect readiness. To ensure the <code>Credential</code> is ready, run:</p> <pre><code>kubectl get credential -n kcm-system aws-cluster-identity-cred\n</code></pre> <p>and check the <code>READY</code> column.</p> <p>For the detailed information about Credential readiness, run:</p> <pre><code>kubectl get credential -n kcm-system aws-cluster-identity-cred -o=yaml\n</code></pre> <p>Example of an error status:</p> <pre><code>status:\n  conditions:\n  - lastTransitionTime: \"2025-09-26T10:15:17Z\"\n    type: CredentialReady\n    status: \"False\"\n    reason: Failed\n    message: \"Failed to get ClusterIdentity object of Kind=AWSClusterStaticIdentity\n      /aws-cluster-identity: unable to retrieve the complete list of server APIs:\n      infrastructure.cluster.x-k8s.io/v1beta2: no matches for infrastructure.cluster.x-k8s.io/v1beta2,\n      Resource=\"\n   ready: false\n</code></pre> <p>This usually indicates that either the provider is not enabled in the specified <code>Region</code>, or provider deployment failed. Double-check the <code>Region</code> spec for the required provider's presence and status for any issues.</p> <p>Example of a ready status:</p> <pre><code>status:\n  conditions:\n  - lastTransitionTime: \"2025-09-26T11:15:57Z\"\n    message: Credential is ready\n    observedGeneration: 1\n    reason: Succeeded\n    status: \"True\"\n    type: CredentialReady\n  ready: true\n</code></pre> <p>To get more details about Credential usage, follow the instructions in the Credential System. For other providers, see the specific credential setup instructions for your target infrastructure.</p> <p>Once the <code>Credential</code> is ready, you can proceed with Deploying Clusters in Region.</p>"},{"location":"admin/regional-clusters/deploying-clusters-in-region/","title":"Deploying Clusters in Region","text":"<p>The process of creating a <code>ClusterDeployment</code> does not change when targeting a <code>Region</code>. If the <code>Credential</code> object defined in the <code>spec.credential</code> field of the <code>ClusterDeployment</code> specifies a region, all cluster resources will be deployed to the corresponding regional cluster.</p> <p>See:</p> <ul> <li>Creating Credential in Region for details about regional credential setup.</li> <li>Deploying a Cluster for cluster deployment instruction.</li> </ul>"},{"location":"admin/regional-clusters/regional-cluster-registration/","title":"Regional Cluster Registration","text":"<p>Note</p> <p> Regional clusters are available starting from version 1.4.0.</p> <p>To register an existing Kubernetes cluster as a regional cluster in k0rdent, you must create a <code>Region</code> object. There are two ways to register a regional cluster:</p> <ul> <li>By providing a kubeconfig <code>Secret</code> in the system namespace</li> <li>By referencing an existing <code>ClusterDeployment</code> to onboard it as a regional cluster</li> </ul>"},{"location":"admin/regional-clusters/regional-cluster-registration/#region-object-with-a-kubeconfig-secret-reference","title":"Region Object with a Kubeconfig Secret Reference","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Region\nmetadata:\n  name: region1\nspec:\n  kubeConfig:\n    name: kubeconfig-secret-name\n    key: value\n  providers:\n  - name: cluster-api-provider-k0sproject-k0smotron\n  - name: cluster-api-provider-openstack\n  - name: projectsveltos\n</code></pre>"},{"location":"admin/regional-clusters/regional-cluster-registration/#region-object-with-a-clusterdeployment-reference","title":"Region Object with a ClusterDeployment Reference","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Region\nmetadata:\n  name: region1\nspec:\n  clusterDeployment:\n    name: regional-cluster\n    namespace: test\n  providers:\n  - name: cluster-api-provider-k0sproject-k0smotron\n  - name: cluster-api-provider-aws\n  - name: projectsveltos\n</code></pre>"},{"location":"admin/regional-clusters/regional-cluster-registration/#configuration-parameters","title":"Configuration Parameters","text":"<p>Use the <code>spec</code> to configure the <code>Region</code> object. For example:</p> <ul> <li><code>spec.kubeConfig</code></li> </ul> <p>This field contains a reference to the <code>Secret</code> containing the kubeconfig of the cluster being onboarded as a regional cluster. This <code>Secret</code> must reside in the system namespace (default: <code>kcm-system</code>). The field must specify both the <code>name</code> of the <code>Secret</code> and the <code>key</code> where the kubeconfig content is stored within that <code>Secret</code>. This field is mutually exclusive with <code>spec.clusterDeployment</code>.</p> <ul> <li><code>spec.clusterDeployment</code></li> </ul> <p>Specifies a reference to an existing ClusterDeployment object to be onboarded as a regional cluster. This field must include both the <code>name</code> and the <code>namespace</code> of the ClusterDeployment object. This field is mutually exclusive with <code>spec.kubeConfig</code>.</p> <ul> <li><code>spec.core</code></li> </ul> <p>The core KCM and CAPI configuration. Allows modification of regional components (CAPI, Cluster API Operator, Velero, Cert Manager). This field is optional; if unspecified, k0rdent will apply the defaults.</p> <ul> <li><code>spec.providers</code></li> </ul> <p>This field includes a list of enabled providers to deploy on the regional cluster.</p> <p>You can use this field to override the configuration (<code>config</code>) or template (<code>template</code>) of the core regional components or providers. Example:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        key1: value1\n    capi:\n      config:\n        key1: value1\n  providers:\n  - name: &lt;provider-name&gt;\n    config:\n      key1: value1\n    template: &lt;provider-template-name&gt;\n</code></pre>"},{"location":"admin/regional-clusters/regional-cluster-registration/#overriding-core-values-with-regional-components-image-overrides","title":"Overriding Core Values with Regional Components: Image Overrides","text":"<p>You can change the images CAPI uses for various functions by specifying the <code>spec.core.kcm.config</code> field. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Region\nmetadata:\n  name: region2\nspec:\n  core:\n    kcm:\n      config:\n        cert-manager:\n          cainjector:\n            image:\n              repository: custom-registry-url.com/jetstack/cert-manager-cainjector\n          image:\n            repository: custom-registry-url.com/jetstack/cert-manager-controller\n          startupapicheck:\n            image:\n              repository: custom-registry-url.com/jetstack/cert-manager-startupapicheck\n          webhook:\n            image:\n              repository: custom-registry-url.com/jetstack/cert-manager-webhook\n        cluster-api-operator:\n          image:\n            manager:\n              repository: custom-registry-url.com/capi-operator/cluster-api-operator\n        velero:\n          image:\n            repository: custom-registry-url.com/velero/velero\n  kubeConfig:\n    name: kubeconfig-secret-name\n    key: value\n  providers:\n  - name: cluster-api-provider-k0sproject-k0smotron\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: projectsveltos\n</code></pre>"},{"location":"admin/regional-clusters/regional-cluster-registration/#after-the-region-is-created","title":"After the Region is Created","text":"<p>Once you create the <code>Region</code> object within the management cluster:</p> <ul> <li>The regional core components and enabled providers start installing on the regional cluster automatically.</li> <li>You can monitor the Region's status and readiness using <code>kubectl wait</code>. For example:</li> </ul> <pre><code>kubectl wait --for=condition=Ready=True region/&lt;region-name&gt;\n</code></pre> <p>Once the <code>Region</code> is ready, you can start creating <code>Credentials</code> and deploy clusters in that region. For more infromation, see: Creating Credential in Region.</p>"},{"location":"admin/upgrade/","title":"Upgrading k0rdent","text":"<p>Important</p> <p>In some circumstances, upgrades involve additional manual steps. Be sure to check the additional instructions for upgrading to k0rdent <code>0.2.0</code>, <code>0.3.0</code>, <code>1.0.0</code>, or <code>1.1.1</code>.</p> <p>Upgrading k0rdent involves making upgrades to the <code>Management</code> object. To do that, you must have the <code>Global Admin</code> role. (For detailed information about k0rdent RBAC roles and permissions, refer to the RBAC documentation.) Follow these steps to upgrade k0rdent:</p> <ol> <li> <p>Create a new <code>Release</code> object</p> <p>Start by creating a <code>Release</code> object in the management cluster that points to the desired version. You can see available versions at https://github.com/k0rdent/kcm/releases.  The actual <code>Release</code> object includes information on the templates and resources that are available, as well as the version of the Kubernetes Cluster API.  For example, the v1.5.0 <code>Release</code> object looks like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Release\nmetadata:\n  name: kcm-1-5-0\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  version: 1.5.0\n  kcm:\n    template: kcm-1-5-0\n  capi:\n    template: cluster-api-1-0-7\n  providers:\n    - name: cluster-api-provider-k0sproject-k0smotron\n      template: cluster-api-provider-k0sproject-k0smotron-1-0-11\n    - name: cluster-api-provider-azure\n      template: cluster-api-provider-azure-1-0-9\n    - name: cluster-api-provider-vsphere\n      template: cluster-api-provider-vsphere-1-0-6\n    - name: cluster-api-provider-aws\n      template: cluster-api-provider-aws-1-0-7\n    - name: cluster-api-provider-openstack\n      template: cluster-api-provider-openstack-1-0-10\n    - name: cluster-api-provider-docker\n      template: cluster-api-provider-docker-1-0-5\n    - name: cluster-api-provider-gcp\n      template: cluster-api-provider-gcp-1-0-6\n    - name: cluster-api-provider-ipam\n      template: cluster-api-provider-ipam-1-0-3\n    - name: cluster-api-provider-infoblox\n      template: cluster-api-provider-infoblox-1-0-2                    \n    - name: projectsveltos\n      template: projectsveltos-1-1-1\n</code></pre> <p>Thankfully, you don't have to build these YAML files yourself. Once you've chosen a release, you can go ahead and create the release object by referencing the YAML file online, as in:</p> <p><pre><code>VERSION=v1.5.0\nkubectl create -f https://github.com/k0rdent/kcm/releases/download/${VERSION}/release.yaml\n</code></pre> <pre><code>release.k0rdent.mirantis.com/kcm-1-5-0 created\n</code></pre></p> </li> <li> <p>List Available <code>Releases</code></p> <p>Once you've created the new <code>Release</code> you need to update the <code>Management</code> object to use it. Start by viewing all available <code>Release</code>s:</p> <pre><code>kubectl get releases\n</code></pre> <pre><code>NAME        AGE\nkcm-0-2-0   6d9h\nkcm-1-5-0   12m\n</code></pre> </li> <li> <p>Patch the <code>Management</code> object with the new <code>Release</code></p> <p>Update the <code>spec.release</code> field in the <code>Management</code> object to point to the new release. Replace <code>&lt;release-name&gt;</code> with the name of your desired release:</p> <pre><code>RELEASE_NAME=kcm-1-5-0\nkubectl patch managements.k0rdent.mirantis.com kcm --patch \"{\\\"spec\\\":{\\\"release\\\":\\\"${RELEASE_NAME}\\\"}}\" --type=merge\n</code></pre> </li> <li> <p>Verify the Upgrade</p> <p>Although the change will be made immediately, it will take some time for k0rdent to update the components it should be using. Monitor the readiness of the <code>Management</code> object to ensure the upgrade was successful. For example:</p> <p><pre><code>kubectl get managements.k0rdent.mirantis.com kcm\n</code></pre> <pre><code>NAME   READY   RELEASE     AGE\nkcm    True    kcm-1-5-0   4m34s\n</code></pre></p> </li> </ol>"},{"location":"admin/upgrade/upgrade-to-0-2-0/","title":"Upgrading to k0rdent 0.2.0","text":"<p>In k0rdent 0.2.0, the <code>k0smotron</code> management component has been renamed to <code>cluster-api-provider-k0sproject-k0smotron</code>. To safely upgrade from <code>0.1.0</code> to <code>0.2.0</code>, follow the upgrade guide and perform the additional manual steps outlined below:</p> <ol> <li> <p>Follow the upgrading guide and create a new <code>Release</code> object (steps 1-2).</p> </li> <li> <p>Verify the new <code>Release</code> status</p> <p>Wait for the new <code>Release</code> to have <code>status.ready: true</code>:</p> <pre><code>kubectl wait --for=jsonpath='{.status.ready}=true' release/kcm-0-2-0\n</code></pre> </li> <li> <p>Manually delete the <code>k0smotron</code> providers (replace <code>kcm-system</code> with your system namespace):</p> <pre><code>kubectl -n kcm-system delete infrastructureproviders.operator.cluster.x-k8s.io k0sproject-k0smotron\nkubectl -n kcm-system delete controlplaneproviders.operator.cluster.x-k8s.io k0sproject-k0smotron\nkubectl -n kcm-system delete bootstrapproviders.operator.cluster.x-k8s.io k0sproject-k0smotron\n</code></pre> </li> <li> <p>Instead of the step 3 from Upgrading guide you need to edit    the <code>Management</code> object. Run:</p> <pre><code>kubectl edit managements.k0rdent.mirantis.com kcm\n</code></pre> <p>And do the following:</p> <ul> <li>Set <code>spec.release</code> to <code>kcm-0-2-0</code></li> <li>In <code>spec.providers</code> rename the <code>k0smotron</code> provider to <code>cluster-api-provider-k0sproject-k0smotron</code>:</li> </ul> <pre><code>providers:\n- name: cluster-api-provider-k0sproject-k0smotron\n</code></pre> <p>The full example of the new Management spec:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  labels:\n    k0rdent.mirantis.com/component: kcm\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm: {}\n  providers:\n  - name: cluster-api-provider-k0sproject-k0smotron\n  - name: projectsveltos\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: cluster-api-provider-docker\n  - name: cluster-api-provider-gcp\n  - name: cluster-api-provider-openstack\n  - name: cluster-api-provider-vsphere\n  release: kcm-0-2-0\n</code></pre> </li> <li> <p>Follow the Upgrading guide and verify the upgrade (step 4).</p> </li> </ol>"},{"location":"admin/upgrade/upgrade-to-0-3-0/","title":"Upgrading to k0rdent 0.3.0","text":"<p>Several changes in 0.3.0 can require manual intervention while upgrading.</p>"},{"location":"admin/upgrade/upgrade-to-0-3-0/#defaultregistryurl-parameter-rename","title":"<code>defaultRegistryURL</code> parameter rename","text":"<p>In k0rdent 0.3.0, the KCM controller parameter <code>defaultRegistryURL</code> was renamed to <code>templatesRepoURL</code>.</p> <p>If you used a custom value for the <code>defaultRegistryURL</code> parameter, upon upgrade you'll need to add <code>templatesRepoURL</code> with the same value to the <code>spec.core.kcm.config.controller</code> of the <code>Management</code> object. After you've upgraded to v0.3.0, you can remove the <code>defaultRegistryURL</code> parameter altogether.</p>"},{"location":"admin/upgrade/upgrade-to-0-3-0/#possible-projectsveltos-upgrade-issue","title":"Possible projectsveltos upgrade issue","text":"<p>When updating the <code>projectsveltos</code> component, you may run into a problem where the upgrade is stuck, with an error referencing an immutable field when patching the <code>register-mgmt-cluster-job</code> <code>Job</code> object.</p> <p>For example:</p> <pre><code>Helm upgrade failed for release projectsveltos/projectsveltos with chart projectsveltos@0.52.2: cannot patch \"register-mgmt-cluster-job\" with kind Job: Job.batch \"register-mgmt-cluster-job\" is invalid: spec.template: Invalid value\n</code></pre> <p>To mitigate this error, follow these steps:</p> <ol> <li>Delete any jobs in the <code>projectsveltos</code> namespace:    <pre><code>kubectl -n projectsveltos delete jobs --all\n</code></pre></li> <li>Fix the <code>projectsveltos</code> helm release by executing the following command:     <pre><code>TOKEN=\"$(date +%s)\" kubectl -n kcm-system annotate hr projectsveltos \"reconcile.fluxcd.io/requestedAt=$TOKEN\" \"reconcile.fluxcd.io/forceAt=$TOKEN\"\n</code></pre></li> </ol>"},{"location":"admin/upgrade/upgrade-to-1-0-0/","title":"Upgrading to k0rdent 1.0.0","text":"<p>The actual process of upgrading k0rdent to 1.0.0 doesn't require any additional steps beyond those listed in the general upgrade instructions, but there are several changes that may affect ancilliary artifacts.</p> <p>Specifically, as part of ongoing improvements to the k0rdent API, we have upgraded the  <code>apiVersion</code> from <code>v1alpha1</code> to <code>v1beta1</code>. This change introduces enhancements in stability and forward compatibility,  but may require updates to any existing automation, scripts, or integrations built against the earlier version.</p>"},{"location":"admin/upgrade/upgrade-to-1-1-1/","title":"Upgrading to k0rdent v1.1.1","text":""},{"location":"admin/upgrade/upgrade-to-1-1-1/#api-versions-and-upgrading-from-pre-100-versions","title":"API versions and upgrading from pre-1.0.0 versions","text":"<p>Because of the change from <code>v1alpha1</code> to <code>v1beta1</code>, clusters running k0rdent versions older than v1.0.0 cannot be upgraded directly to v1.1.1. First follow the directions to upgrade to v1.0.0, and then perform a second upgrade to v1.1.1.</p>"},{"location":"admin/upgrade/upgrade-to-1-1-1/#upgrade-k0rdent-observability-and-finops-kof","title":"Upgrade k0rdent Observability and FinOps (KOF)","text":"<p>After upgrading KOF to v1.1.1, please run:</p> <pre><code>kubectl apply --server-side --force-conflicts \\\n  -f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre> <p>Also run the same for each regional cluster:</p> <pre><code>kubectl get secret -n kcm-system $REGIONAL_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; regional-kubeconfig\n\nKUBECONFIG=regional-kubeconfig kubectl apply --server-side --force-conflicts \\\n  -f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre> <p>These steps are required by <code>grafana-operator</code>, as noted in its release notes.</p>"},{"location":"admin/upgrade/upgrade-to-1-2-0/","title":"Upgrade to v1.2.0","text":"<p>This version doesn't require any special actions. Follow the main upgrade instructions here.</p>"},{"location":"admin/upgrade/upgrade-to-1-3-1/","title":"Upgrade to v1.3.1","text":"<p>This version doesn't require any special actions. Follow the main upgrade instructions here.</p>"},{"location":"admin/upgrade/upgrade-to-1-4-0/","title":"Upgrade to v1.4.0","text":"<p>This version doesn't require any special actions. Follow the main upgrade instructions here.</p>"},{"location":"appendix/","title":"Appendix","text":"<ul> <li>Extended management configuration</li> <li>Using a Private Secure Registry to deploy k0rdent</li> <li>Understanding the dry run</li> <li>Cloud provider credentials management in CAPI</li> <li>Running k0rdent on ARM64</li> <li>Glossary</li> </ul>"},{"location":"appendix/appendix-dryrun/","title":"Understanding Dry Run mode","text":"<p>The <code>ClusterDeployment</code> process includes a \"dry run\" mode, which enables you to validate your configuration without actually provisioning resources. By default, <code>.spec.dryRun</code> is set to <code>false</code>, but enabling it can help identify potential issues early.</p> <p>Note that if no configuration (<code>.spec.config</code>) is provided, default values from the selected template will populate the object, and <code>.spec.dryRun</code> will automatically be enabled.</p> <p>Example: Dry Run with default configuration:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  dryRun: true\n</code></pre> <p>After validation (this is, you see <code>TemplateReady</code> as a condition in <code>.status.conditions</code>), remove or disable <code>.spec.dryRun</code> to proceed with deployment.</p> <p>Example: Validated <code>ClusterDeployment</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    publicIP: true\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n  status:\n    conditions:\n    - type: TemplateReady\n      status: \"True\"\n      reason: Succeeded\n      message: Template is valid\n    - type: Ready\n      status: \"True\"\n      reason: Succeeded\n      message: ClusterDeployment is ready\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/","title":"Extended Management Configuration","text":"<p>k0rdent is deployed with the following default configuration, which may vary depending on the release version:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm: {}\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: cluster-api-provider-openstack\n  - name: cluster-api-provider-vsphere\n  - name: projectsveltos\n  release: kcm-0-0-7\n</code></pre> <p>As you can see, the <code>Management</code> object defines the providers that are available from within k0rdent. Some of these are providers directly used by the user, such as aws, azure, and so on, and others are used internally by k0rdent, such as Sveltos.</p> <p>To see what is included in a specific release, look at the <code>release.yaml</code> file in the tagged release. For example, here is the v0.0.7 release.yaml.</p> <p>k0rdent allows you to customize its default configuration by modifying the spec of the <code>Management</code> object. This enables you to manage the list of providers to deploy and adjust the default settings for core components.</p> <p>For detailed examples and use cases, refer to Examples and Use Cases</p>"},{"location":"appendix/appendix-extend-mgmt/#configuration-guide","title":"Configuration Guide","text":"<p>There are two options to override the default management configuration of k0rdent:</p> <ol> <li> <p>Update the <code>Management</code> object after the k0rdent installation using <code>kubectl</code>:</p> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; edit management</code></p> </li> <li> <p>Deploy k0rdent skipping the default <code>Management</code> object creation and provide your    own <code>Management</code> configuration:</p> <ul> <li> <p>Create <code>management.yaml</code> file and configure core components and providers.   For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm:\n      config:\n        controller:\n          templatesRepoURL: \"oci://ghcr.io/my-oci-registry-name/kcm/charts\"\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: projectsveltos\n  release: kcm-0-0-7\n</code></pre> <p>In the example above, the <code>Management</code> object is configured with custom registry settings for the KCM controller and a reduced list of providers.</p> </li> <li> <p>Specify <code>--create-management=false</code> controller argument and install k0rdent:   If installing using <code>helm</code> add the following parameter to the <code>helm   install</code> command:</p> <pre><code>--set=\"controller.createManagement=false\"\n</code></pre> </li> <li> <p>Create <code>kcm</code> <code>Management</code> object after k0rdent installation:</p> <pre><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; create -f management.yaml\n</code></pre> </li> </ul> </li> </ol> <p>You can customize the default configuration options for core components by updating the <code>.spec.core.&lt;core-component-name&gt;.config</code> section in the <code>Management</code> object. For example, to override the default settings for the KCM component, modify the <code>spec.core.kcm.config</code> section. To view the complete list of configuration options available for kcm, refer to: KCM Configuration Options for k0rdent v0.0.7 (Replace v0.0.7 with the relevant release tag for other k0rdent versions).</p> <p>To customize the list of providers to deploy, update the <code>.spec.providers</code> section. You can add or remove providers and configure custom templates for each provider. Each provider in the list must include the <code>name</code> field and may include the <code>template</code> and <code>config</code> fields:</p> <pre><code>- name: &lt;provider-name&gt; \n  template: &lt;provider-template&gt; # optional. If omitted, the default template from the `Release` object will be used\n  config: {} # optional provider configuration containing provider Helm Chart values in YAML format\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#examples-and-use-cases","title":"Examples and Use Cases","text":""},{"location":"appendix/appendix-extend-mgmt/#configuring-a-custom-oci-registry-for-kcm-components","title":"Configuring a Custom OCI Registry for KCM components","text":"<p>You can override the default registry settings in k0rdent by specifying the <code>templatesRepoURL</code>, <code>insecureRegistry</code>, and <code>registryCredsSecret</code> parameters under <code>spec.core.kcm.config.controller</code>.</p> <ul> <li><code>templatesRepoURL</code>: Specifies the registry URL for downloading Helm charts representing templates. Use the <code>oci://</code> prefix for OCI registries. Default: <code>oci://ghcr.io/k0rdent/kcm/charts</code>.</li> <li><code>globalRegistry</code>: Specifies the global registry. This value will be propagated to all <code>ClusterDeployment</code> objects configuration as <code>global.registry</code> (for example, it is used for pulling cluster Helm extensions, such as the Cloud Controller Manager and to download required images, such as <code>etcd</code> or <code>kube-proxy</code>).</li> <li><code>insecureRegistry</code>: Allows connecting to an HTTP registry. Default: <code>false</code>.</li> <li><code>registryCredsSecret</code>: Specifies the name of a Kubernetes <code>Secret</code> containing authentication credentials for the registry (optional). This <code>Secret</code> should exist in the system namespace (default: <code>kcm-system</code>).</li> </ul> <p>Additionally, if your templates repository (<code>templatesRepoURL</code>) and/or registry (<code>globalRegistry</code>) is private and uses a certificate signed by an unknown authority, you can make them \"trusted\" within the K0rdent system by configuring the <code>registryCertSecret</code> parameter. This parameter should reference the name of a <code>Secret</code> in the system (default: <code>kcm-system</code>) namespace that contains the root CA certificate(s) (<code>ca.crt</code>) used to verify the server certificates of the registry and/or templates repository. If the <code>templatesRepoURL</code> and <code>globalRegistry</code> refer to different endpoints, and each uses a different certificate authority, you can include both certificates concatenated in the same <code>ca.crt</code> key of the <code>Secret</code>, like this:</p> <pre><code>-----BEGIN CERTIFICATE-----\n&lt;templatesRepo CA cert&gt;\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n&lt;registry CA cert&gt;\n-----END CERTIFICATE-----\n</code></pre> <p>Note</p> <p> This is used for server certificate verification only - mutual TLS (mTLS) is not supported yet.</p> <p>Note</p> <p> If you\u2019re using a private registry signed by an unknown certificate authority, refer to Private Secure Registry Usage for the required prerequisites.</p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          templatesRepoURL: \"oci://ghcr.io/my-private-oci-registry-name/kcm/charts\"\n          globalRegistry: ghcr.io/my-private-oci-registry-name\n          insecureRegistry: false\n          registryCredsSecret: my-private-oci-registry-creds\n          registryCertSecret: registry-cert\n</code></pre> <p>Note</p> <p> Prior to K0rdent v0.3.0, the <code>templatesRepoURL</code> parameter was named <code>defaultRegistryURL</code>. (See: K0rdent v0.3.0 Release Notes).</p> <p>Example of a <code>Secret</code> with Registry Credentials:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-private-oci-registry-creds\n  namespace: kcm-system\nstringData:\n  username: \"my-user-123\"\n  password: \"my-password-123\"\n</code></pre> <p>Example of a <code>Secret</code> with Registry Certificate:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: registry-cert\n  namespace: kcm-system\nstringData:\n  ca.crt: |\n    -----BEGIN CERTIFICATE-----\n    MIIDfjCCAmagAwIBAgIUV/Ykpp7jzkOdfsZs0wwNZOS9X04wDQYJKoZIhvcNAQEL\n    ...\n    2eVUGBCoHgFcUrkjcZlxvjjdaV5L/Y6mEt6u9mIhsb1M8w==\n    -----END CERTIFICATE-----\n</code></pre> <p>The KCM controller will create the default HelmRepository using the provided configuration and fetch KCM components from this repository. For the example above, the following <code>HelmRepository</code> will be created:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\n  name: my-private-oci-registry-name\n  namespace: kcm-system\nspec:\n  insecure: false\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/my-private-oci-registry-name/kcm/charts\n  secretRef:\n    name: my-private-oci-registry-creds\n  certSecretRef:\n    name: registry-cert\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#configuring-a-global-k0s-url","title":"Configuring a global K0s URL","text":"<p>You can override the default URL from which to download the k0s binary in k0rdent by specifying the <code>globalK0sURL</code>, and optionally <code>k0sURLCertSecret</code> (if the k0s download URL is private and uses a certificate signed by an unknown authority), under <code>spec.core.kcm.config.controller</code>. This is optional and is only needed when the environment does not have access to the default upstream k0s binaries endpoint. This is required for airgapped environments.</p> <ul> <li><code>globalK0sURL</code>: Specifies the prefix of the k0s URL from which to download the k0s binary. This value will be propagated to all <code>ClusterDeployment</code> objects configuration as <code>global.k0sURL</code>.</li> <li><code>k0sURLCertSecret</code>: The name of the secret in the system (default: <code>kcm-system</code>) namespace containing the root CA certificate (<code>ca.crt</code>) for the k0s download URL.</li> </ul> <p>Note</p> <p> This is used for server certificate verification only - mutual TLS (mTLS) is not supported yet.</p> <p>Note</p> <p> If you\u2019re using a private registry signed by an unknown certificate authority, refer to Private Secure Registry Usage for the required prerequisites.</p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          globalK0sURL: https://172.19.123.4:8443\n          k0sURLCertSecret: k0s-url-cert\n</code></pre> <p>Example of a <code>Secret</code> with K0s URL Certificate:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: k0s-url-cert\n  namespace: kcm-system\nstringData:\n  ca.crt: |\n    -----BEGIN CERTIFICATE-----\n    MIIDfjCCAmagAwIBAgIUV/Ykpp7jzkOdfsZs0wwNZOS9X04wDQYJKoZIhvcNAQEL\n    ...\n    2eVUGBCoHgFcUrkjcZlxvjjdaV5L/Y6mEt6u9mIhsb1M8w==\n    -----END CERTIFICATE-----\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#configuring-a-custom-image-for-kcm-controllers","title":"Configuring a Custom Image for KCM controllers","text":"<p>You can override the default image for the KCM controllers by specifying the <code>repository</code>, <code>tag</code> and <code>pullPolicy</code> parameters under <code>spec.core.kcm.config.image</code>:</p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        image:\n          repository: ghcr.io/my-custom-repo/kcm/controller\n          tag: v0.0.7\n          pullPolicy: IfNotPresent\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#configuring-manager-settings-for-capi-providers","title":"Configuring manager settings for CAPI providers","text":"<p>Starting from <code>v0.3.0</code>, k0rdent supports configuring manager settings for CAPI providers. You can override these settings by defining the <code>spec.providers[*].config.manager</code> section. The values under the <code>manager</code> section should follow the format defined by the CAPI Operator.</p> <p>Warning</p> <p>Prior to <code>v1.2.0</code> k0rdent, this is not supported for the <code>k0sproject-k0smotron</code> provider due to a bug in the CAPI Operator: CAPI operator incorrectly finds the manager container if the number of containers is &gt;1.</p> <p>Starting <code>v1.2.0</code> k0rdent shipped with a CAPI Operator version with the issue addressed.</p> <p>For example, to override feature gates for the Cluster API Provider AWS, configure the following:</p> <pre><code>spec:\n  providers:\n  - name: cluster-api-provider-aws\n    config:\n      manager:\n        featureGates:\n          MachinePool: true\n          EKSEnableIAM: true\n          EKSAllowAddRoles: true\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#configuring-the-sveltos-stuck-tokens-controller","title":"Configuring the Sveltos Stuck Tokens Controller","text":"<p>If a management cluster has some maintenance activity or hardware issue causing it to go down for more than ~30 minutes, the token for the management <code>sveltoscluster</code> object expires. This leads to continuous errors and failure to reconcile the management <code>sveltoscluster</code> object. Related issue: KCM #995.</p> <p>A dedicated controller that automatically detects and renews stuck tokens is shipped with k0rdent starting <code>v1.1.0</code> and is disabled by default.</p> <p>To enable the controller, set:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          enableSveltosExpiredCtrl: true\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#configuring-default-timeout-for-helm-install-and-upgrade-operations","title":"Configuring Default Timeout for Helm Install and Upgrade Operations","text":"<p>In some environments, Helm chart installations or upgrades may take longer than the default timeout of 5 minutes. This hardcoded limit could cause operations to fail unexpectedly if they exceeded the timeout.</p> <p>Starting from K0rdent v1.3.0, KCM allows you to configure the default Helm timeout using the <code>defaultHelmTimeout</code> controller setting:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          defaultHelmTimeout: 20m\n</code></pre> <p>This value accepts standard duration format (e.g., 20m, 1h).</p>"},{"location":"appendix/appendix-extend-mgmt/#configuring-telemetry","title":"Configuring Telemetry","text":"<p>To configure Telemetry options via the Management object, set values under the <code>telemetry</code> block. For example, you can disable collection by setting <code>mode</code> to <code>disabled</code>:</p> <pre><code>spec:\n  core:\n    kcm:\n      regional:\n        telemetry:\n          mode: disabled\n</code></pre> <p>Follow the telemetry configuration page for all of the possible values.</p>"},{"location":"appendix/appendix-providers/","title":"Cloud Provider Credentials Management in CAPI","text":"<p>Cloud provider credentials in Cluster API (CAPI) environments are managed through objects in the management cluster. <code>Credential</code>, <code>ClusterIdentity</code>, and <code>Secret</code> (related to <code>ClusterIdentity</code>) objects handle credential storage and management, while a dedicated <code>ConfigMap</code> object is used as a template to render configuration into child clusters.</p>"},{"location":"appendix/appendix-providers/#configuration-patterns","title":"Configuration Patterns","text":"<p>The configuration follows two patterns:</p> <ol> <li>ClusterIdentity Pattern</li> <li>Uses a <code>ClusterIdentity</code> resource that defines provider identity configuration</li> <li>References a <code>Secret</code> with credentials</li> <li> <p>Used by <code>Azure</code> and <code>vSphere</code> in-tree providers</p> </li> <li> <p>Source Secret Pattern</p> </li> <li>Uses only a <code>Secret</code> without <code>ClusterIdentity</code></li> <li><code>Secret</code> contains all cloud configuration data</li> <li>Used by <code>OpenStack</code> in-tree provider</li> </ol> <p>In both cases <code>ConfigMap</code> with template code is used to render configuration into child clusters.</p>"},{"location":"appendix/appendix-providers/#credential-resource","title":"Credential Resource","text":"<p>The <code>Credential</code> resource provides an abstraction layer by either:</p> <ul> <li>Referencing a <code>ClusterIdentity</code> through <code>identityRef</code></li> <li>Directly referencing a <code>Secret</code>, depending on the pattern used</li> </ul>"},{"location":"appendix/appendix-providers/#template-configmap","title":"Template ConfigMap","text":"<ul> <li>Marked with <code>projectsveltos.io/template: \"true\"</code> annotation</li> <li>Contains Go template code for generating child cluster resources via the Sveltos templating system</li> <li>Template processing accesses cluster objects through:</li> <li>Built-in Sveltos variables (<code>Cluster</code>, <code>InfrastructureProvider</code>)</li> <li><code>getResource</code> function for additionally exposed objects (<code>InfrastructureProviderIdentity</code>, <code>InfrastructureProviderIdentitySecret</code>)</li> <li>The object name needs to follow a predictable naming pattern, such as the <code>ClusterIdentity</code> object (referenced via <code>identityRef</code> in the <code>Credential</code> resource) name + <code>-resource-template</code> suffix. It must also be placed in same Namespace as the <code>ClusterIdentity</code> object it references</li> </ul>"},{"location":"appendix/appendix-providers/#templating-system","title":"Templating System","text":"<p>The templating system leverages:</p> <ul> <li>Golang templating</li> <li>Sprig functions</li> <li>Sveltos resource manipulation functions</li> </ul>"},{"location":"appendix/appendix-providers/#examples","title":"Examples","text":"<p>Provider-specific examples are available in <code>*.credentials.yaml</code> files here.</p> <p>Let's take <code>Azure</code> provider as an example azure-credentials.yaml</p> <ul> <li><code>ClusterIdentity</code></li> <li><code>Secret</code> (related to <code>ClusterIdentity</code>)</li> <li><code>Credential</code></li> <li><code>ConfigMap</code></li> </ul> <p>Note</p> <p> These examples assume the <code>ConfigMap</code> is included in a template. To use these examples directly via <code>kubectl</code>, please replace <code>$$</code> (double dollar signs) with <code>$</code> (single dollar sign)</p>"},{"location":"appendix/appendix-providers/#provider-registration","title":"Provider Registration","text":"<p>Providers are registered through <code>ProviderInterface</code> CR, each provider Helm chart ships with corresponding <code>ProviderInterface</code> object.</p> <p>Modifications to the <code>Management</code> Spec are needed to enable newly added provider.</p> <p>For detailed information, refer to Extended Management Configuration</p>"},{"location":"appendix/appendix-providers/#configuration-examples","title":"Configuration Examples","text":"<p>As mentioned previously provider configuration examples can be found here, look for <code>ProviderInterface</code> object inside CAPI Helm charts.</p> <p>Let's take the <code>Azure</code> provider as an example interface.yaml, as seen, the CR definitions are straightforward.</p>"},{"location":"appendix/arm64/","title":"Running K0rdent on ARM64: Known Limitations","text":"<p>K0rdent can be deployed on ARM64-based infrastructure, but there are some current limitations to be aware of.</p>"},{"location":"appendix/arm64/#infoblox-capi-provider-compatibility","title":"Infoblox CAPI Provider Compatibility","text":"<p>The Infoblox Cluster API IPAM provider does not currently support the ARM64 architecture. See the upstream issue for details: Multi-arch support.</p> <p>As a result, the Infoblox provider will fail to start during the installation process, and the management object will remain in a non-ready state. This blocks the successful deployment of K0rdent on ARM64 platforms.</p>"},{"location":"appendix/arm64/#workaround","title":"Workaround","text":"<p>To install K0rdent without the Infoblox provider you should use a custom management configuration that excludes <code>cluster-api-provider-infoblox</code> from the list of enabled providers. Follow the official configuration guide here: Extended Management Configuration Guide.</p> <p>This will allow K0rdent to be deployed successfully on ARM64 infrastructure without relying on unsupported components.</p>"},{"location":"appendix/glossary/","title":"k0rdent Glossary","text":"<p>This glossary is a collection of terms related to k0rdent. It clarifies some of the unique terms and concepts we use or explains more common ones that may need a little clarity in the way we use them.</p>"},{"location":"appendix/glossary/#beach-head-services","title":"Beach-head Services","text":"<p>We use the term to refer to those Kubernetes services that need to be installed on a Kubernetes cluster to make it actually useful, for example: an ingress controller, CNI, and/or CSI. While from the perspective of how they are deployed they are no different from other Kubernetes services, we define them as distinct from the apps and services deployed as part of the applications.</p>"},{"location":"appendix/glossary/#accessmanagement-crd","title":"AccessManagement (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent used to define and manage access controls.  It typically includes specifications for <code>AccessRule</code> and <code>TargetNamespace</code> objects to control the  distribution of resources such as <code>ClusterTemplate</code>, <code>ServiceTemplate</code>, and <code>Credential</code> objects to  specific namespaces within managed clusters.</p>"},{"location":"appendix/glossary/#accessrules","title":"AccessRules","text":"<p>A component within the <code>AccessManagement</code> CRD that specifies which k0rdent  resources (such as <code>ClusterTemplateChain</code>, <code>Credential</code>, and <code>ServiceTemplateChain</code> objects) are to be  distributed to a defined set of <code>TargetNamespaces</code>.</p>"},{"location":"appendix/glossary/#cluster-api-capi","title":"Cluster API (CAPI)","text":"<p>CAPI is a Kubernetes project that provides a declarative way to manage the lifecycle of  Kubernetes clusters. It abstracts the underlying infrastructure, allowing users to  create, scale, upgrade, and delete clusters using a consistent API. CAPI is extensible  via providers that offer infrastructure-specific functionality, such as AWS, Azure, and  vSphere.</p>"},{"location":"appendix/glossary/#capi-provider-see-also-infrastructure-provider","title":"CAPI provider (see also Infrastructure provider)","text":"<p>A CAPI provider is a Kubernetes CAPI extension that allows k0rdent to manage and drive  the creation of clusters on a specific infrastructure via API calls.</p>"},{"location":"appendix/glossary/#capa","title":"CAPA","text":"<p>CAPA stands for Cluster API Provider for AWS.</p>"},{"location":"appendix/glossary/#capg","title":"CAPG","text":"<p>CAPG stands for Cluster API Provider for Google Cloud.</p>"},{"location":"appendix/glossary/#capo","title":"CAPO","text":"<p>CAPO stands for Cluster API Provider for OpenStack.</p>"},{"location":"appendix/glossary/#capv","title":"CAPV","text":"<p>CAPV stands for Cluster API Provider for vSphere.</p>"},{"location":"appendix/glossary/#capz","title":"CAPZ","text":"<p>CAPZ stands for Cluster API Provider for Azure.</p>"},{"location":"appendix/glossary/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<p>Cloud Controller Manager (CCM) is a Kubernetes component that embeds logic to manage a  specific infrastructure provider.</p>"},{"location":"appendix/glossary/#cluster-deployment","title":"Cluster Deployment","text":"<p>A Kubernetes cluster created and managed by k0rdent.</p>"},{"location":"appendix/glossary/#clusterdeployment-crd","title":"ClusterDeployment (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent that represents  the desired state and configuration of a Kubernetes cluster to be created and managed by  k0rdent. It typically includes details about the infrastructure  provider, cluster topology, version, and references to other configurations such as  <code>ClusterTemplate</code> or <code>ServiceTemplate</code> objects.</p>"},{"location":"appendix/glossary/#clusteridentity","title":"ClusterIdentity","text":"<p>ClusterIdentity is a Kubernetes object that references a Secret object containing  credentials for a specific infrastructure provider.</p>"},{"location":"appendix/glossary/#clusteripam-crd","title":"ClusterIPAM (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent responsible  for defining and managing IP address pools at a broad, cluster-aware level. It serves as  the central authority or source from which IP address ranges (CIDRs) or specific blocks  can be allocated for use by various Kubernetes clusters or their internal networks.</p>"},{"location":"appendix/glossary/#clusteripamclaim-crd","title":"ClusterIPAMClaim (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent that enables  users or automated processes to request or \"claim\" specific IP address resources (such as subnets or blocks of IPs) from a <code>ClusterIPAM</code> instance. This mechanism ensures orderly  allocation of IP addresses to individual Kubernetes clusters or other network-dependent  components.</p>"},{"location":"appendix/glossary/#clustertemplate-crd","title":"ClusterTemplate (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent that provides  a reusable blueprint for defining the configuration, components, and characteristics of a  Kubernetes cluster. <code>ClusterTemplate</code> objects are used by k0rdent to ensure consistency when provisioning or managing multiple clusters.</p>"},{"location":"appendix/glossary/#clustertemplatechain-crd","title":"ClusterTemplateChain (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent that defines an ordered sequence or a collection of <code>ClusterTemplate</code> objects. This allows for the  application of multiple layers of cluster configurations in a structured and repeatable  manner, often used to build up complex cluster environments.</p>"},{"location":"appendix/glossary/#credential","title":"Credential","text":"<p>A <code>Credential</code> is a custom resource (CR) in kcm that supplies k0rdent with the necessary  credentials to manage a specific infrastructure. The credential object references other  CRs with infrastructure-specific credentials such as access keys, passwords,  certificates, etc. This means that a credential is specific to the CAPI provider that  uses it.</p>"},{"location":"appendix/glossary/#declarative-approach","title":"Declarative approach","text":"<p>We define the declarative approach to cluster management using the Kubernetes principles  as the process where you define the state you want within custom resource objects and the  controllers or customer operators ensure that the system moves toward that desired state.</p>"},{"location":"appendix/glossary/#distributed-container-management-environment-dcme","title":"Distributed Container Management Environment (DCME)","text":"<p>An infrastructure setup focused on managing containerized applications across various,  often geographically dispersed, locations and platforms. k0rdent  is designed to provide platform engineers with the tools to build and operate a DCME.</p>"},{"location":"appendix/glossary/#dry-run","title":"Dry Run","text":"<p>A feature or operational mode within k0rdent that enables users to  simulate the outcome of an action, such as deploying or modifying a cluster or service,  without making any actual changes to the infrastructure. Running a command in <code>--dry-run</code> mode  helps in validating configurations and understanding potential impacts.</p>"},{"location":"appendix/glossary/#eks-eks-cluster","title":"EKS / EKS cluster","text":"<p>Refers to Amazon Elastic Kubernetes Service (EKS), a managed Kubernetes service by AWS.  k0rdent supports creating and managing EKS clusters, as well as using EKS to host the management cluster.</p>"},{"location":"appendix/glossary/#entra-id","title":"Entra-ID","text":"<p>Microsoft Entra ID (formerly Azure Active Directory) is a cloud-based identity and access  management service. k0rdent can integrate with Entra-ID for  authentication purposes.</p>"},{"location":"appendix/glossary/#internal-developer-platform-idp","title":"Internal Developer Platform (IDP)","text":"<p>A platform built by an organization to provide its developers with self-service capabilities  for accessing tools, services, and infrastructure required for software development and  deployment. k0rdent aims to enable platform engineers to compose  and deliver IDPs.</p>"},{"location":"appendix/glossary/#ip-address-management-ipam","title":"IP Address Management (IPAM)","text":"<p>The general concept and practice of planning, tracking, allocating, and managing IP address  space within a network. In k0rdent, this refers to the systems or  features, such as <code>ClusterIPAM</code> and <code>ClusterIPAMClaim</code>, used for managing IP addresses for  clusters and services.</p>"},{"location":"appendix/glossary/#k0rdent-cluster-manager-kcm","title":"k0rdent Cluster Manager (KCM)","text":"<p>Deployment and life-cycle management of Kubernetes clusters, including configuration,  updates, and other CRUD operations.</p>"},{"location":"appendix/glossary/#k0rdent-crds","title":"k0rdent CRDs","text":"<p>The collective term for the set of Custom Resource Definitions (CRDs) that are specific  to k0rdent. These CRDs extend the Kubernetes API to define  and manage k0rdent-specific resources, configurations, and functionalities, forming the  core of its declarative management capabilities.</p>"},{"location":"appendix/glossary/#k0rdent-observability-and-finops-kof","title":"k0rdent Observability and FinOps (KOF)","text":"<p>Cluster and beach-head services monitoring, events and log management.</p>"},{"location":"appendix/glossary/#k0rdent-state-manager-ksm","title":"k0rdent State Manager (KSM)","text":"<p>Installation and life-cycle management of deployed services.</p>"},{"location":"appendix/glossary/#k0s-k0s-cluster","title":"k0s / k0s cluster","text":"<p>Refers to k0s, an open-source, lightweight, and certified Kubernetes distribution.  k0rdent supports creating and managing k0s clusters, as well as running the Management Cluster on k0s.</p>"},{"location":"appendix/glossary/#hosted-control-plane-hcp","title":"Hosted Control Plane (HCP)","text":"<p>An HCP is a Kubernetes control plane that runs outside of the clusters it manages.  Instead of running the control plane components (like the API server, controller  manager, and etcd) within the same cluster as the worker nodes, the control plane is  hosted on a separate, often centralized, infrastructure. This approach can provide  benefits such as easier management, improved security, and better resource utilization,  as the control plane can be scaled independently of the worker nodes.</p>"},{"location":"appendix/glossary/#infrastructure-provider-see-also-capi-provider","title":"Infrastructure provider (see also CAPI provider)","text":"<p>An infrastructure provider (aka <code>InfrastructureProvider</code>) is a Kubernetes custom  resource (CR) that defines the infrastructure-specific configuration needed for managing  Kubernetes clusters. It enables Cluster API (CAPI) to provision and manage clusters on  a specific infrastructure platform (for example, AWS, Azure, VMware, OpenStack, and so on.).</p>"},{"location":"appendix/glossary/#localsourceref","title":"LocalSourceRef","text":"<p>A defined structure within k0rdent Custom Resource Definitions  that specifies a local source for configuration data, such as a kustomize manifest stored  within the management cluster (for example, in a <code>ConfigMap</code> or <code>Secret</code>).</p>"},{"location":"appendix/glossary/#management-crd","title":"Management (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent. While  \"Management Cluster\" refers to the Kubernetes cluster where k0rdent  itself is installed and operates, the <code>Management</code> CRD represents a specific  k0rdent resource type for a core management-related  configuration or entity within the ecosystem.</p>"},{"location":"appendix/glossary/#managementbackup-crd","title":"ManagementBackup (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent specifically  designed for configuring, triggering, and managing backup operations related to the management  plane components or the configuration of the management cluster itself.</p>"},{"location":"appendix/glossary/#multi-cluster-service","title":"Multi-Cluster Service","text":"<p>The <code>MultiClusterService</code> is a custom resource used to manage services' deployment across multiple clusters.</p>"},{"location":"appendix/glossary/#management-cluster","title":"Management Cluster","text":"<p>The Kubernetes cluster where k0rdent is installed and from which all other managed  clusters are managed.</p>"},{"location":"appendix/glossary/#okta","title":"Okta","text":"<p>An identity and access management (IAM) service. k0rdent can integrate  with Okta to handle user authentication and authorization for accessing k0rdent  functionality.</p>"},{"location":"appendix/glossary/#pluggableprovider-crd","title":"PluggableProvider (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent that represents a configured  instance of an external provider (e.g., infrastructure, services). It enables the integration of such  providers into the k0rdent system in a modular or \"pluggable\" fashion, specifically  by adhering to a defined <code>ProviderInterface</code>.</p>"},{"location":"appendix/glossary/#project-sveltos","title":"Project Sveltos","text":"<p>An open-source Kubernetes add-on controller that k0rdent, particularly its  k0rdent State Manager (KSM) component, leverages for managing the deployment and lifecycle of add-ons and  applications across managed Kubernetes clusters.</p>"},{"location":"appendix/glossary/#providerinterface-crd","title":"ProviderInterface (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent that defines a standardized  contract or API structure for how k0rdent interacts with various external  infrastructure or service providers (for example, cloud platforms or storage systems). This abstraction  layer enables consistent provider integration and management.</p>"},{"location":"appendix/glossary/#providertemplate-crd","title":"ProviderTemplate (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent used for creating reusable  and parameterized templates for the configuration of specific infrastructure providers (such as AWS,  Azure, GCP). These templates abstract provider-specific details and promote consistency in cluster  provisioning.</p>"},{"location":"appendix/glossary/#release-crd","title":"Release (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent primarily used to define and  manage aspects of k0rdent's own internal software lifecycle. <code>Release</code> plays  a role in tracking the versions of different components, coordinating updates or rollbacks of the  k0rdent platform itself on the management cluster, or specifying the  collection of software artifacts and configurations that constitute a particular internal k0rdent release.</p>"},{"location":"appendix/glossary/#remotesourcespec","title":"RemoteSourceSpec","text":"<p>A defined structure within k0rdent Custom Resource Definitions that specifies  a remote source for configuration data. This can include sources such as  a Git repository or an  S3-compatible object storage bucket (defined by <code>bucketName</code>, <code>endpoint</code>, etc.), often used for kustomize  manifests or Helm charts.</p>"},{"location":"appendix/glossary/#role-based-access-control-rbac","title":"Role Based Access Control (RBAC)","text":"<p>Role-Based Access Control (RBAC) defines roles, permissions, and rules governing user and system access to  k0rdent resources and the clusters it manages, ensuring secure and controlled operations.</p>"},{"location":"appendix/glossary/#service-definition-for-deployment","title":"Service (definition for deployment)","text":"<p>Within k0rdent's CRDs (for example, as part of <code>MultiClusterService</code> or templates),  this refers to a specific schema or object structure that defines a service to be deployed onto a cluster.  It typically includes attributes such as the service's name (often the chart release name), the template to  use, target namespace, Helm values, and <code>valuesFrom</code> for sourcing configuration from <code>ConfigMap</code> or <code>Secret</code> objects.</p>"},{"location":"appendix/glossary/#servicetemplate-crd","title":"ServiceTemplate (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent that provides a reusable template  for defining how a specific service, application, or set of Kubernetes resources (such as a database, monitoring  agent, or custom workload) is deployed and configured on managed Kubernetes clusters.</p>"},{"location":"appendix/glossary/#servicetemplatechain-crd","title":"ServiceTemplateChain (CRD)","text":"<p>A Custom Resource Definition (CRD) in k0rdent that defines an ordered sequence  or a collection of <code>ServiceTemplate</code> objects. This enables the orchestrated deployment of multiple services  or applications as a cohesive logical unit onto managed clusters. The <code>ServiceTemplateChain</code> also defines potential upgrade paths.</p>"},{"location":"appendix/glossary/#targetnamespaces","title":"TargetNamespaces","text":"<p>A component within the <code>AccessManagement</code> CRD that defines the specific Kubernetes namespaces within  managed clusters where selected k0rdent resources (such as <code>ClusterTemplate</code>, <code>Credential</code>, and <code>ServiceTemplate</code>  objects defined in an <code>AccessRule</code>) will be distributed or made available.</p>"},{"location":"appendix/glossary/#templateresourcerefs","title":"templateResourceRefs","text":"<p>A structure commonly found within k0rdent CRDs (Custom Resource Definitions) that  enables a template to reference existing Kubernetes resources (such as <code>Secret</code> or <code>ConfigMap</code> objects)  residing in the management cluster. These referenced resources can then be fetched and their data  injected or used during the instantiation of the k0rdent template for a target cluster or service.</p>"},{"location":"appendix/private-secure-registry/","title":"Using a Private Secure Registry to deploy k0rdent","text":""},{"location":"appendix/private-secure-registry/#prerequisites","title":"Prerequisites","text":"<p>If you are deploying k0rdent with registry overrides (see Configuring a Custom OCI Registry for KCM components), and your registry endpoint is secured with a certificate signed by an unknown Certificate Authority (CA), you must ensure that the CA certificate is trusted by your management cluster nodes before deploying k0rdent.</p> <p>To do this, add the CA certificate to the system\u2019s trust store on each management cluster node.</p>"},{"location":"appendix/private-secure-registry/#for-management-clusters-running-on-k0s","title":"For Management Clusters Running on k0s","text":"<p>Before starting the k0s controller, do the following:</p> <ol> <li> <p>Copy the CA certificate file (e.g., <code>&lt;PATH_TO_CA_CERT&gt;</code>) to the system\u2019s trusted certificate directory:</p> <pre><code>sudo cp &lt;PATH_TO_CA_CERT&gt; /usr/local/share/ca-certificates/\n</code></pre> </li> <li> <p>Update the system\u2019s trusted certificates:</p> <pre><code>sudo update-ca-certificates\n</code></pre> </li> <li> <p>Proceed to install k0s as usual.</p> </li> </ol>"},{"location":"appendix/private-secure-registry/#for-management-clusters-running-on-kind","title":"For Management Clusters Running on Kind","text":"<ol> <li> <p>Create a Kind configuration file that mounts the CA certificate into the container nodes. Suppose your CA certificate is located at <code>&lt;PATH_TO_CA_CERT&gt;</code> on your host.</p> <pre><code># kind-config.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    extraMounts:\n      - hostPath: &lt;PATH_TO_CA_CERT&gt;\n        containerPath: /usr/local/share/ca-certificates/registry-ca.crt\n</code></pre> </li> <li> <p>Create the Kind cluster using the configuration:</p> <pre><code>kind create cluster --config kind-config.yaml --name &lt;KIND_CLUSTER_NAME&gt;\n</code></pre> </li> <li> <p>Run the following command to update the trust store inside the container. Repeat this command for each node in the Kind cluster, replacing <code>&lt;KIND_CLUSTER_NAME&gt;-control-plane</code> with the name of the Docker container representing that node:</p> <pre><code>docker exec &lt;KIND_CLUSTER_NAME&gt;-control-plane update-ca-certificates\n</code></pre> </li> </ol>"},{"location":"appendix/telemetry/","title":"Telemetry introduction","text":"<p>This document explains how k0rdent collects telemetry from child (managed) clusters, what data is gathered, where it is stored for each mode, and how to configure the feature through either Helm values or the Management object.</p> <p>Why telemetry? Telemetry helps the k0rdent team understand real-world environments (cluster sizes, versions, GPU usage, etc.) to improve reliability and guide roadmap priorities. Data is scoped to operational characteristics and includes no application payloads.</p> <p>Default behavior Telemetry is enabled by default with mode <code>online</code>, which sends metrics to Segment.io CDP workspace owned by our team.</p>"},{"location":"appendix/telemetry/configuration/","title":"Configuring telemetry","text":"<p>k0rdent enables you to configure telemetry either by using the <code>--set</code> flag to set helm values during installation or by editing the Management object for an existing management cluster, as shown in the Extended Management Configuration.</p>"},{"location":"appendix/telemetry/configuration/#main-settings-block","title":"Main settings block","text":"<p>Telemetry is configured under the <code>telemetry</code> block of the <code>kcm-regional</code> chart (see Components segregation).</p> <p>In a <code>managements</code> object set the following:</p> <pre><code>regional:\n  telemetry:\n    mode: online            # disabled | local | online (default: online)\n    concurrency: 5          # scrape N clusters in parallel (1..100)\n    interval: 1h            # scrape cadence\n    jitter: 10              # % jitter (1..99)\n</code></pre> <ul> <li><code>mode</code> controls the collection and storage mechanism. <code>online</code> sends to Segment;   <code>local</code> writes files; <code>disabled</code> turns it off. (Default: <code>online</code>)</li> <li><code>concurrency</code> is how many child clusters are scraped in parallel.</li> <li><code>interval</code> is the scrape frequency.</li> <li><code>jitter</code> spreads scrapes across time to avoid thundering herds.</li> </ul> <p>In a corresponding <code>regions</code> object set the following:</p> <pre><code>telemetry:\n  mode: online            # disabled | local | online (default: online)\n  concurrency: 5          # scrape N clusters in parallel (1..100)\n  interval: 1h            # scrape cadence\n  jitter: 10              # % jitter (1..99)\n</code></pre> <p>Note, that the only change is the absence of the <code>regional</code> subpath.</p> <p>Note</p> <p> Here and after all of the examples are given without the <code>regional</code> top subpath. Set or remove it accordingly depending what resource is being edited: add it if a <code>managements</code> object is being edited, do not add it if a <code>regions</code> object is being edited.</p>"},{"location":"appendix/telemetry/configuration/#local-mode-storage-settings","title":"Local mode storage settings","text":"<p>When <code>telemetry.mode</code> is set to <code>local</code>, you can configure where and how files are stored:</p> <pre><code>telemetry:\n  mode: local\n  local:\n    baseDir: /var/lib/telemetry\n    volume:\n      source: hostPath     # pvc | existing | hostPath (default)\n      # For source: pvc (dynamic provisioning)\n      pvc:\n        storageClassName: \"\"        # default StorageClass if empty\n        accessModes: [ReadWriteOnce]\n        volumeMode: Filesystem\n        size: 200Mi\n        annotations: {}\n      # For source: existing (use pre-created PVC)\n      existingClaim: \"\"\n      # For source: hostPath (static PV + matching PVC)\n      hostPath:\n        name: \"\"                    # defaults to &lt;fullname&gt;-telemetry-data\n        reclaimPolicy: Retain\n        storageClassName: \"manual\"\n        accessModes: [ReadWriteOnce]\n        volumeMode: Filesystem\n        size: 200Mi\n        annotations: {}\n        labels: {}\n        nodeAffinity: {}            # pin PV to a node if needed\n</code></pre> <ul> <li><code>baseDir</code> is the directory inside the pod where telemetry files appear;   when <code>source: hostPath</code>, the host path on the node is the same <code>baseDir</code>.</li> <li> <p><code>volume.source</code> chooses between:</p> <ul> <li><code>pvc</code> \u2014 the chart creates a PVC (dynamic PV provisioning   must be configured prior).</li> <li><code>existing</code> \u2014 the chart uses an existing PVC you name in <code>existingClaim</code>.</li> <li><code>hostPath</code> (default) \u2014 the chart creates a static hostPath PV +   matching PVC mounted at <code>baseDir</code>.   Default PV settings include <code>persistentVolumeReclaimPolicy: Retain</code>   and <code>storageClassName: manual</code>; optional <code>nodeAffinity</code> ties   the PV to a specific node.</li> </ul> </li> </ul> <p>Recommendation</p> <p>Prefer dynamically-provisioned PVCs (<code>source: pvc</code>) or existing PVCs (<code>source: existing</code>) over <code>hostPath</code>. HostPath ties data to one node and is generally less stable/portable for controller pods that may reschedule.</p>"},{"location":"appendix/telemetry/configuration/#how-the-chart-wires-this-up","title":"How the chart wires this up","text":"<ul> <li>Only in <code>local</code> mode, an init container (<code>telemetry-data-permissions</code>)   runs as root (only the <code>CHOWN</code> capability)   to <code>chown -R 65532:65532 &lt;baseDir&gt;</code> so the main controller   (running as non-root) can write files.   The main pod runs with <code>runAsNonRoot: true</code>.</li> <li> <p>When <code>source: hostPath</code>, the chart creates:</p> <ul> <li>a PersistentVolume with <code>hostPath.path: &lt;baseDir&gt;</code>,   <code>type: DirectoryOrCreate</code>, reclaim policy Retain (default),   optional <code>nodeAffinity</code>.</li> <li>a matching PVC bound by name.</li> </ul> </li> </ul> <p>Note on a legacy flag</p> <p><code>controller.enableTelemetry</code> is deprecated in 1.4.0 and removed in 1.5.0 version in favor of the <code>telemetry</code> block. It may still appear as a container arg for backward compatibility, but you should configure telemetry via the <code>telemetry</code> values going forward.</p>"},{"location":"appendix/telemetry/configuration/#configuration-examples","title":"Configuration examples","text":"<p>Note</p> <p> Replace only the snippets you need; all other values keep their defaults.</p>"},{"location":"appendix/telemetry/configuration/#disable-telemetry-entirely","title":"Disable telemetry entirely","text":"<pre><code>telemetry:\n  mode: disabled\n</code></pre> <p>No data is collected or stored.</p>"},{"location":"appendix/telemetry/configuration/#online-telemetry-defaults","title":"Online telemetry (defaults)","text":"<pre><code>telemetry:\n  mode: online      # default\n  concurrency: 5    # default\n  interval: 1h      # default\n  jitter: 10        # default\n</code></pre> <p>Sends data to Segment.io CDP.</p>"},{"location":"appendix/telemetry/configuration/#local-telemetry-with-dynamic-provisioning","title":"Local telemetry with dynamic provisioning","text":"<p>Warning</p> <p>Ensure dynamic PV provisioning is configured prior.</p> <pre><code>telemetry:\n  mode: local\n  local:\n    baseDir: /var/lib/telemetry\n    volume:\n      source: pvc\n      pvc:\n        storageClassName: gp2\n        accessModes: [ReadWriteOnce]\n        size: 5Gi\n</code></pre> <p>Chart creates a PVC; files appear under <code>/var/lib/telemetry</code> in the pod, rotated daily.</p>"},{"location":"appendix/telemetry/configuration/#local-telemetry-with-an-existing-pvc","title":"Local telemetry with an existing PVC","text":"<pre><code>telemetry:\n  mode: local\n  local:\n    baseDir: /var/lib/telemetry\n    volume:\n      source: existing\n      existingClaim: kcm-telemetry-pvc\n</code></pre> <p>Chart uses your PVC; no PV/PVC objects are created by the chart.</p>"},{"location":"appendix/telemetry/configuration/#local-telemetry-with-hostpath-single-node-pinned-node","title":"Local telemetry with hostPath (single-node / pinned-node)","text":"<pre><code>telemetry:\n  mode: local\n  local:\n    baseDir: /var/lib/telemetry\n    volume:\n      source: hostPath\n      hostPath:\n        name: kcm-telemetry-hostpath\n        reclaimPolicy: Retain\n        storageClassName: manual\n        size: 10Gi\n        nodeAffinity:\n          required:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/hostname\n                operator: In\n                values: [\"worker-01\"]\n</code></pre> <p>Chart creates a hostPath PV at <code>/var/lib/telemetry</code> and a matching PVC; prefer PVC-backed options in HA clusters.</p>"},{"location":"appendix/telemetry/configuration/#controller-settings","title":"Controller settings","text":"<p>Note</p> <p> Added in 1.5.0 version.</p> <p>There are numerions settings to customize the dedicated telemetry controller. The list of default settings is as follows:</p> <pre><code>telemetry:\n  # Telemetry runner configuration\n  controller:\n    # Number of replicas\n    replicas: 1\n    # Image configuration\n    image:\n      # Name of the image\n      repository: ghcr.io/k0rdent/kcm/telemetry\n      # Tag of the image\n      tag: latest\n      # Image pull policy, one of [IfNotPresent, Always, Never]\n      pullPolicy: IfNotPresent\n    # Container resources\n    resources:\n      limits:\n        cpu: 500m\n        memory: 256Mi\n      requests:\n        cpu: 100m\n        memory: 64Mi\n    # Node selector to constrain the pod to run on specific nodes\n    nodeSelector: {}\n    # Affinity rules for pod scheduling\n    affinity: {}\n    # Tolerations to allow the pod to schedule on tainted nodes\n    tolerations: []\n    # Global runner's logger settings\n    logger:\n      # Development defaults to (encoder=console,logLevel=debug,stackTraceLevel=warn) Production defaults to (encoder=json,logLevel=info,stackTraceLevel=error)\n      devel: false\n      # Encoder type, one of [json, console]\n      encoder: \"\"\n      # Log level, one of [info, debug, error]\n      log-level: \"\"\n      # Level on which to produce stacktraces, one of [info, error, panic]\n      stacktrace-level: \"\"\n      # Type of time encoding, one of [epoch, millis, nano, iso8601, rfc3339, rfc3339nano]\n      time-encoding: rfc3339\n</code></pre>"},{"location":"appendix/telemetry/configuration/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Settings are not applied:   make sure that the correct <code>regional</code> subpath is either added (if <code>managements</code> object)   or is absent (if <code>regions</code> object).</li> <li>No local files appear:   ensure <code>telemetry.mode: local</code> and that a volume source is configured.   Check the init container <code>telemetry-data-permissions</code> completed successfully.</li> <li>Pod unschedulable with hostPath:   if the pod moves to a different node, the hostPath PV won\u2019t be there;   either pin the PV with <code>nodeAffinity</code> or switch to a PVC-backed option.</li> <li>Conflicting settings:   if you previously set <code>controller.enableTelemetry</code>, migrate to the   <code>telemetry</code> block; the old flag is deprecated and is removed in the 1.5.0 version.</li> </ul>"},{"location":"appendix/telemetry/data_collected/","title":"What\u2019s collected (at a glance)","text":"<p>The KCM controller periodically scrapes each managed cluster and records:</p> <ul> <li>Fleet/size: number of nodes, total CPU (cores) and memory, total GPU units   (AMD + NVIDIA).</li> <li>Per-node profile: OS, architecture, kernel, Kubernetes version, and   flavor (e.g., k0s).</li> <li>GPU details: capacity per vendor and GPU requests aggregated from pods;   whether NVIDIA/AMD GPU operators are installed; count of pods requesting GPUs.</li> <li>Virtualization: number of KubeVirt VMIs.</li> <li>Template and sync info: ClusterTemplate's   name, chart version, sync mode, providers used by the template.</li> <li>User Services: number of services defined in ClusterDeployment   and MultiClusterService objects.</li> <li>Identification labels: cluster namespaced name,   ClusterDeployment UID,   and k0s cluster ID.</li> <li>IPAM allocation: the number of ClusterIPAM bound in a cluster and the overall number of ClusterIPAM objects in a cluster</li> </ul> <p>The exact metrics and their representation vary from the mode.</p>"},{"location":"appendix/telemetry/data_collected/#mode-specific-nuances","title":"Mode-specific nuances","text":"<ul> <li>Online mode: each event carries a Segment AnonymousId equal   to the Management UID.   Segment client context includes: KCM build commit/name/version,   system namespace, runtime OS/arch, and timezone.</li> <li>Local mode: data is written to a daily-rotated JSON file   under a base directory, with a top-level   <code>clusters</code> array containing per-cluster <code>counters</code> and <code>labels</code>.   Metrics are represented as counters; hence, some of the <code>online</code>   mode's metrics are flattened or omitted due to high cardinality   such as per-node profile.</li> </ul>"},{"location":"appendix/telemetry/modes/","title":"Modes","text":"Mode What happens Where it goes <code>disabled</code> Collection is off \u2014 <code>online</code> (default) Events are batched and sent to Segment.io with the default context fields listed below Segment.io CDP <code>local</code> Events are written to files and rotated daily Files on a PersistentVolume mounted at the base directory <p>Local files resemble:</p> <pre><code>{\n  \"clusters\": [\n    {\n      \"counters\": { \"...\": \"...\" },\n      \"labels\": {\n        \"cluster\": \"ns/name\",\n        \"clusterDeploymentID\": \"uid\",\n        \"clusterID\": \"kube-system:uuid\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"appendix/telemetry/modes/#privacy-and-identity-in-online-mode","title":"Privacy and identity in <code>online</code> mode","text":"<p>When operating in <code>online</code> mode, each event includes a Segment context built from the running KCM controller and environment (build commit/name/version, system namespace, runtime OS/arch, timezone), and uses the Management UID as an AnonymousId to bind events to a management cluster \u2014 but not to an individual user.</p>"},{"location":"concepts/","title":"k0rdent Concepts","text":"<p>k0rdent addresses the complexities of modern infrastructure by helping platform  engineers and developers through a declarative, Kubernetes-native approach to  cluster management. k0rdent aims to close the divide between rapidly evolving  application workloads, especially those powered by AI/ML, and the frequently slower  infrastructure that supports them. It achieves this goal by utilizing the  well-established open-source ecosystem, primarily Kubernetes, to offer a modular,  extensible, and repeatable template-based solution. k0rdent simplifies infrastructure,  boosting efficiency, developer speed, and control, so operators can focus on delivering  applications.</p> <p>This section of the documentation includes:</p> <ul> <li>Why k0rdent?</li> <li>k0rdent architecture</li> </ul>"},{"location":"concepts/k0rdent-architecture/","title":"k0rdent architecture","text":"<p>The k0rdent architecture follows a declarative approach to cluster management using Kubernetes principles. The modular extensible architecture provides for a repeatable template-driven solution to interact with sub components such as the Cluster API (CAPI) and other Kubernetes components.</p> <p>The key principles of the architecture include:</p> <ul> <li>The leveraging of Kubernetes core principles</li> <li>A highly aligned but loosely coupled architecture</li> <li>A pluggable and extensible architecture</li> <li>A template-driven approach for repeatability</li> <li>A standards-driven API</li> <li>The leveraging of unmodified upstream components (for example, the Kubernetes Cluster API)</li> <li>Support for integration with custom components downstream</li> </ul> <p>Note</p> <p> This document is a ongoing work in progress, and we would welcome suggestions and questions.</p>"},{"location":"concepts/k0rdent-architecture/#overview","title":"Overview","text":"<p>The k0rdent Management Cluster orchestrates the provisioning and lifecycle of multiple child clusters on multiple clouds and infrastructures, keeping you from having to directly interact with individual infrastructure providers. By abstracting the infrastructure in this way, k0rdent promotes reusability (reducing, for example, the effort required to implement an IDP on a particular cloud), encourages standardization where practical, and lets you use the clouds and technologies you want, while also minimizing the cost of switching components such as open source subsystems, cloud substrates, and so on.</p> <p>The k0rdent architecture comprises the following high level components:</p> <ul> <li>Cluster Management: Tools and controllers for defining, provisioning, and managing clusters.</li> <li>State Management: Controllers and systems for monitoring, updating, and managing the state of child clusters and their workloads.</li> <li>Infrastructure Providers: Services and APIs responsible for the under-the-hood provisioning resources such as virtual machines, networking, and storage for clusters.</li> <li>Templates: Templates define managed child clusters or the workloads that run on them. Instantiating those templates creates the corresponding resources.</li> </ul> <p></p>"},{"location":"concepts/k0rdent-architecture/#management-cluster","title":"Management cluster","text":"<p>The management cluster is the core of the k0rdent architecture. It hosts all of the controllers needed to make k0rdent work. This includes:</p> <ul> <li>k0rdent Cluster Manager (KCM) Controller:  KCM provides a wrapper for k0rdent\u2019s CAPI-related capabilities. It orchestrates:<ul> <li>Cluster API (CAPI) Controllers: CAPI controllers are designed to work with specific infrastructure providers. For example, one CAPI controller manages the creation and lifecycle of Kubernetes clusters running on Amazon Web Services, while another manages those on Azure. It\u2019s also possible to create custom CAPI controllers to integrate with internal systems.</li> <li>k0smotron Controller: k0smotron extends CAPI with additional functionality, including control plane and worker node bootstrap providers for k0s Kubernetes and a control plane provider that supports the creation of a Hosted Control Plane, or a k0s control plane represented by pods on a host Kubernetes cluster, which can be the same cluster that hosts k0rdent. The k0smotron project has also provided a so-called \u2018RemoteMachine\u2019 infrastructure provider for CAPI, enabling deployment and cluster operations via SSH on arbitrary remote Linux servers (including small-scale edge devices).</li> </ul> </li> <li>k0rdent State Manager (KSM) Controller: KSM is responsible for lifecycle managing (deploy, scale, update, upgrade, teardown) services and applications on clusters, and for doing continuous state management of these services and applications. This is currently part of the KCM code base; we may split it out in the future. It orchestrates:<ul> <li>Services Controller: Responsible for coordinating Kubernetes services, such as combinations of services and infrastructure provisioning dependencies that add capabilities to the platform. For example, Nginx, with its dependencies, can be packaged as a service. Artifacts for services are stored locally or in an OCI repository, and are referenced as kubernetes CRD objects.</li> </ul> </li> <li>k0rdent Observability &amp; FinOps (KOF) Controller (not depicted in above diagram): k0rdent Observability and FinOps provides enterprise-grade observability and FinOps capabilities for k0rdent-managed Kubernetes clusters. It enables centralized metrics, logging, and cost management through a unified OpenTelemetry-based architecture.</li> </ul> <p>We\u2019ll take a closer look at these pieces under Roles and Responsibilities.</p>"},{"location":"concepts/k0rdent-architecture/#regional-cluster","title":"Regional cluster","text":"<p>Note</p> <p> Regional clusters are available starting from version 1.4.0.</p> <p>A regional cluster is an optional separate cluster where user workloads and provider-related infrastructure (such as CAPI providers, clusters, and machines) are created.</p> <p>Using a regional cluster is helpful if you don\u2019t want the management cluster to host cluster-related objects. For example, separating hosted control plane pods from the mothership cluster to improve both networking and security.</p> <p>Regional clusters are optional. The management cluster can host infrastructure cluster resources if needed as it used to be until 1.4.0.</p> <p>A regional cluster includes the following components:</p> <ol> <li>Cert Manager</li> <li>Velero</li> <li>CAPI providers and operator</li> <li>ClusterIdentity resources</li> </ol> <p>It does not include: k0rdent API and KCM controller and Flux's Helm and Source controllers.</p> <p>The management cluster remains the single pane of glass, responsible for managing ClusterDeployments, Credentials, Templates, and other k0rdent resources.</p> <p>To set up a regional cluster, you need an existing cluster that is accessible from the management cluster. This can be either:</p> <ul> <li>A cluster created with a k0rdent ClusterDeployment, or</li> <li>An external cluster.</li> </ul> <p>Learn more in Regional Clusters.</p>"},{"location":"concepts/k0rdent-architecture/#cluster-deployments","title":"Cluster Deployments","text":"<p>A cluster deployment is also known as a child cluster, or a workload cluster. It\u2019s a Kubernetes cluster provisioned and managed by the management or regional cluster, and it\u2019s where developers run their applications and workloads. These are \u201cregular\u201d Kubernetes clusters, and don\u2019t host any management or regional components. Clusters are fully isolated from the management cluster via namespaces, and also from each other, making it possible to create multi-tenant environments.</p> <p>You can tailor a child cluster to specific use cases, with customized addons such as ingress controllers, monitoring tools, and logging solutions. You can also define specific Kubernetes configurations (for example, network policies, storage classes, and security policies) so they work for you and your applications or environments.</p> <p>Simply put, child clusters are where applications and workloads run.</p>"},{"location":"concepts/k0rdent-architecture/#templates","title":"Templates","text":"<p>One of the important tenets of the platform engineering philosophy is the use of Infrastructure as Code, but k0rdent takes that one step further through the use of templates. Templates are re-usable text definitions of components that can be used to create and manage clusters. Templates provide a declarative way for users and developers to deploy and manage complex clusters or components while massively reducing the number of parameters they need to configure. Considered generally, k0rdent templates are:</p> <ul> <li>Formatted using YAML: Templates use YAML as an abstraction to represent the target state, so they\u2019re human-readable and editable.</li> <li>Designed to be used in multiple contexts using runtime parameterization: Through the use of placeholders, you can customize templates at runtime without having to directly edit the template.</li> <li>Used for both cluster creation and addon management: Users can define a cluster using YAML, or they can define addons, such as an ingress operator or monitoring tools, to be added to those clusters.</li> <li>Capable of limited scope: k0rdent lets you set restrictions over what templates can be deployed by whom. For example, as the platform manager (see Roles and Responsibilities), you can specify that non-admin users can only execute templates that deploy a particular set of controllers.</li> </ul> <p>Major template types used in k0rdent include:</p> <ul> <li>Cluster Templates: <code>ClusterTemplate</code> objects define clusters in coordination with the clouds and infrastructures on which they run. They're designed to be immutable; they get invoked by k0rdent objects such as <code>ClusterDeployment</code> objects to create and manage individual clusters and groups of clusters.</li> <li>Service Templates: <code>ServiceTemplate</code> objects define services, addons, and workloads that run on clusters. They're also designed to be immutable, and get invoked by <code>ClusterDeployment</code> objects and other k0rdent objects so that IDPs/platforms can be declared and managed as units.</li> </ul>"},{"location":"concepts/k0rdent-architecture/#roles-and-responsibilities","title":"Roles and responsibilities","text":"<p>k0rdent was designed to be used by several groups of people, with hierarchical and complementary roles and responsibilities. You may have your own names for them, but we\u2019ll refer to them as:</p> <ul> <li>Platform Architect: This person or team has global responsibility to the business and technical stakeholders for designing IDPs/platforms for later adaptation to particular clouds and infrastructures, workloads, performance and cost objectives, security and regulatory regimes, and operational requirements. k0rdent enables Platform Architects to create sets of reusable <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, closely defining IDPs/platforms in the abstract.</li> <li>Platform Lead: This person or team (sometimes referred to as 'CloudOps') is primarily responsible for actions corresponding to k0rdent Cluster Manager (KCM). They adapt <code>ClusterTemplate</code> objects to the correct cloud, and they make sure that everything is working properly. They\u2019re also responsible for limiting the Project Team\u2019s access to the <code>Cluster</code> and <code>Service</code> templates necessary to do their jobs. For example, they might limit the templates that can be deployed to an approved set, or provide CAPI operators for only the clouds on which the company wants applications to run, helping to eliminate shadow IT.</li> <li>Platform Engineer: This person or team is responsible for the day-to-day management of the environment. They use <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects provided by the Platform Lead (as authorized to do so) and may create additional <code>ServiceTemplate</code> objects to customize their own Kubernetes cluster so that it's appropriate for their application.</li> </ul>"},{"location":"concepts/k0rdent-architecture/#credentials","title":"Credentials","text":"<p>Creating and managing Kubernetes clusters requires having the proper permissions on the target infrastructure, but you certainly wouldn't want to give out your AWS account information to every single one of your developers.</p> <p>To solve this problem, k0rdent lets you create a <code>Credential</code> object that provides the access your developers need. It works like this:</p> <ol> <li>The platform lead creates a provider-specific <code>ClusterIdentity</code> and <code>Secret</code> that include all of the information necessary to perform various actions.</li> <li>The platform lead then creates a <code>Credential</code> object that references the <code>ClusterIdentity</code>.</li> <li>Developers reference the <code>Credential</code> object, which gives the cluster the ability to access these credentials (little \u201cc\u201d) without having to expose them to developers directly.</li> </ol> <p>Note</p> <p> To be sure credentials are not visible to developers, make sure to limit access to the <code>kcm-system</code> namespace.</p>"},{"location":"concepts/k0rdent-architecture/#tldr-conclusion","title":"TL;DR - Conclusion","text":"<p>k0rdent provides a comprehensive Kubernetes lifecycle management framework through its three core components:</p> <ul> <li>KCM: Cluster provisioning and management</li> <li>KSM: Application and runtime state management</li> <li>KOF: Observability, logging, and cost optimization</li> </ul> <p>With multi-provider support, templated deployments, and strong security controls, k0rdent is being built to enable scalable, efficient, and consistent Kubernetes operations.</p>"},{"location":"concepts/why-k0rdent/","title":"Why k0rdent?","text":"<p>k0rdent was developed to provide for the needs of platform engineers and the developers that they serve, as well as the application workloads that they support. </p>"},{"location":"concepts/why-k0rdent/#applications-and-workloads","title":"Applications and Workloads","text":"<p>Workloads evolve and grow, often faster then the infrastructure needed to support them. Infrastructure is almost always the lagging factor in getting new or existing applications into the hands of users and scaling them to meet user expectations. This challenge has grown exponentially with the rise of AI and ML workloads. Specifically:</p> <ul> <li>Workload complexity is increasing</li> <li>Modern workloads depend on specialized infrastructure</li> <li>Developers have high expectations of time to value</li> </ul>"},{"location":"concepts/why-k0rdent/#platform-engineering","title":"Platform Engineering","text":"<p>Modern infrastructure systems are increasingly complex, and administrators need to manage that complexity while still responding quickly to developers' needs efficiently as possible. This has led to development of internal developer platforms (IDP) and platform engineering. These environments provide the frameworks and tools for increasing developer productivity when developing, deploying, and managing applications and services, enabling developers to focus on their specific tasks or goals and not the underlying complexities. Overall:</p> <ul> <li>Developer platforms increase developer productivity</li> <li>Platform engineers need to implement and grow the platforms</li> <li>Infrastructure needs to support the required complexity</li> </ul>"},{"location":"concepts/why-k0rdent/#modern-infrastructure-systems","title":"Modern Infrastructure Systems","text":"<p>The increasingly distributed nature of modern infrastructure systems and the demands of modern workloads is leading to increasing complexity. Solutions need to solve a diverse set of challenges and provide consistency, repeatability, and prevention of lock-in, all without increasing the burden on operators. Modern platform engineers and operators are increasingly time constrained with the vast number of challenges they need to overcome, including security and compliance, cost management, resilience, and scale to name but a few. Keep in mind that:</p> <ul> <li>Distributed deployments are the new normal</li> <li>Infrastructure management is not just a technical problem</li> <li>Operators need to focus on building value chains</li> </ul>"},{"location":"concepts/why-k0rdent/#open-source","title":"Open Source","text":"<p>The open source ecosystem, and especially Kubernetes, is mature and offers an increasing number of tools that solve real problems. The open source ecosystem, if leveraged correctly, also supports building unique architectures to support a business' needs while helping to avoid lock-in and architectural dead ends. All of these tools need to be selected, deployed, and lifecycle-managed in a way that is repeatable and traceable. Open source:</p> <ul> <li>Prevents lock-in and supports architectural self determination</li> <li>Solutions allow for solving of problems in unique ways</li> <li>Can help solve the problem of managing the complexity of modern infrastructure</li> </ul>"},{"location":"contrib/","title":"k0rdent Documentation Contributor's Guide","text":"<p>The k0rdent community relies on contributions both for code and documentation. As community members you can be particularly helpful by pointing out areas where the documentation can be improved, either in terms of corrections or additions. In addition, we are happy to accept contributions.</p> <ul> <li>Contributor's guide</li> <li>Style guide</li> </ul>"},{"location":"contrib/k0rdent-documentation-contributors-guide/","title":"k0rdent documentation contributors' guide","text":"<p>k0rdent documentation is growing rapidly, and community members are very welcome to contribute by filing pull requests against documents in this repository. Thank you for your interest in k0rdent!</p> <p>This page gives technical guidance for contributing to k0rdent documentation. The k0rdent documentation style guide details our markdown style and provides general recommendations for writing style.</p>"},{"location":"contrib/k0rdent-documentation-contributors-guide/#how-k0rdent-docs-are-built","title":"How k0rdent docs are built","text":"<p>k0rdent documentation is built from Markdown (.md) documents using the MkDocs static site generator with the Material theme. We use several MkDocs extensions and plugins for rendering glossary items, admonitions (e.g., Note, Tip, Warning, etc.) and other special kinds of content.</p> <p>Below you'll find instructions for setting up MkDocs locally to dynamically build and serve the k0rdent documentation from your local clone of the docs repository. This is handy for reviewing the rendered look and feel of changes and additions before filing a pull request.</p>"},{"location":"contrib/k0rdent-documentation-contributors-guide/#how-k0rdent-docs-are-written","title":"How k0rdent docs are written","text":"<p>k0rdent documentation is written mostly in CommonMark as adopted by GitHub, with a few CommonMark-compatible markdown enhancements used by MkDocs, the Material theme, and extensions (see below) to enable more-readable rendering of code, glossary items, images, and admonitions (e.g., NOTE:, TIP:, WARNING:, etc.). The k0rdent documentation style guide details our markdown style, along with those extensions.</p>"},{"location":"contrib/k0rdent-documentation-contributors-guide/#how-k0rdent-docs-is-structured","title":"How k0rdent docs is structured","text":"<p>The structure of k0rdent/docs is shown below:</p> <pre><code>docs                                                # repository containing folder /docs\n\u251c\u2500\u2500 docs                                            # subdirectory for /docs, containing further subdirectories for assets, images, theme, etc.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 assets                                      # illustrations for docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 css                                         # css stylesheets\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 custom_theme                                # Material theme customizations\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 img                                         # site imagery (logos, icons, etc.)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 stylesheets                                 # extra CSS stylesheets\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md                                    # default homepage of the docs site\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE                                     # k0rdent docs license (CC-BY-4.0)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (documentation-files.md)                    # actual docs pages (markdown documents)\n\u251c\u2500\u2500 mkdocs.yml                                      # main configuration file for mkdocs, specifies site navigation and table of contents\n\u251c\u2500\u2500 requirements.txt                                # mkdocs requirements (for installation with pip for building docs locally - see below)\n\u251c\u2500\u2500 Dockerfile                                      # Dockerfile for building a container to serve docs interactively\n\u251c\u2500\u2500 .gitignore                                      # Things to ignore when people push\n\u2514\u2500\u2500 .github                                         # Special GitHub features, including allowed-patterns.yml for placeholders replacing secrets\n</code></pre> <p>Note</p> <p> The k0rdent documentation repository is called <code>docs</code> and cloning the repo will create a containing directory called <code>docs</code>. This directory itself contains a subdirectory <code>/docs</code> which contains Markdown (.md) doc files and related subdirectories like /assets, /img, etc. Do not confuse toplevel docs with subdirectory /docs.</p>"},{"location":"contrib/k0rdent-documentation-contributors-guide/#raising-issues","title":"Raising issues","text":"<p>If you find issues (errors, omissions, etc.) with k0rdent documentation, we would be very grateful if you would notify us by creating an issue.</p>"},{"location":"contrib/k0rdent-documentation-contributors-guide/#contributing-to-k0rdent-docs-simplest-way-use-githubs-webui","title":"Contributing to k0rdent docs - simplest way: use GitHub's webUI","text":"<p>For making small corrections to k0rdent docs, the simplest tool is GitHub's website. You will need a personal (free) or business GitHub account.</p> <ol> <li>Familiarize yourself with the structure of the <code>k0rdent/docs</code> repo.</li> <li>Figure out where the doc you want to update lives, or where a new doc you create might need to live.</li> <li>Visit the k0rdent/docs repo itself. Search for <code>k0rdent/docs</code> or click the following link: (https://github.com/k0rdent/docs)</li> <li>To propose changes to existing docs: On k0rdent/docs, edit the doc you want to change, using the pencil icon (Edit) button in the upper right of GitHub's webUI. This will create a fork of k0rdent/docs in your own account's workspace (if you don't already have your own fork) and create a new feature branch, from which your proposed update will be submitted to k0rdent/docs as a pull request (PR).</li> <li>To create new docs (or propose changes to existing docs): Create a fork of k0rdent/docs in your own account workspace using the Fork button in the upper right of k0rdent/docs. Navigate to the directory in which you want to create your new doc (or go to the directory in which the doc you want to edit lives). Click Add File&gt;Create New File to add a new blank doc, upload an image or other file, or find and edit the doc you want to change.</li> <li>When your edit(s) are complete, click Commit Changes in the upper right.</li> <li>Write a descriptive commit message so project k0rdent docs maintainers understand your proposed addition or change.</li> <li>Click Propose Change.</li> <li>On the next screen, compare your change with content originally there.</li> <li>Click Create Pull Request.</li> </ol> <p>Maintainers will see your pull request and may request discussion or suggest further changes, or they may merge your change. You'll be notified via GitHub either way.</p>"},{"location":"contrib/k0rdent-documentation-contributors-guide/#making-regular-contributions-use-git-on-the-desktop","title":"Making regular contributions - use Git on the desktop","text":"<p>For making regular or at-scale contributions to k0rdent documentation, we recommend using a standard open source workflow as follows:</p> <ol> <li>Create a fork of k0rdent/docs in your own GitHub workspace.</li> <li>Clone your fork to your desktop using git.</li> <li>Set origin remotes (fetch, push) to your fork.</li> <li>Set upstream remotes (fetch, push) to k0rdent/docs (main)</li> </ol> <p>Then, when getting ready to work:</p> <ol> <li>Update your local main from upstream and update main on your fork on GitHub:</li> </ol> <pre><code>git checkout main &amp;&amp; git fetch upstream &amp;&amp; git pull upstream main &amp;&amp; git push origin main\n</code></pre> <ol> <li>Create a new feature branch for each intended commit, to isolate your changes:</li> </ol> <pre><code>git checkout -b my-feature-branch main\n</code></pre> <ol> <li>As needed, particularly if you work on a change for a long time, periodically update your feature branch from upstream main to avoid later conflicts:</li> </ol> <pre><code>git checkout my-feature-branch &amp;&amp; git pull --rebase upstream main\n</code></pre> <ol> <li>When finished editing, add and commit changes:</li> </ol> <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre> <ol> <li>Pre-emptively update once more before pushing:</li> </ol> <pre><code>git fetch upstream &amp;&amp; git rebase upstream/main\n</code></pre> <ol> <li>Push your feature branch to your fork on GitHub:</li> </ol> <pre><code>git push origin my-feature-branch\n</code></pre> <p>This will cause Git to display a confirmation message containing a link you can follow to submit a pull request on GitHub, from your feature branch to k0rdent/docs (main).</p> <ol> <li>Follow the link. Compare changes. Write an informative pull request title and extended description. And submit your Pull Request. Thank you in advance!</li> </ol>"},{"location":"contrib/k0rdent-documentation-contributors-guide/#install-mkdocs-to-view-k0rdent-docs-locally","title":"Install MkDocs to view k0rdent docs locally","text":"<p>When making docs contributions or edits, it can be very helpful to run MkDocs on your desktop, so you can see your additions and changes as they will be rendered online.</p> <p>MkDocs can be run on almost any contemporary Linux environment, desktop or server, inside a container, or on MacOS, Windows, or WSL. It requires recent (in k0rdent's case, 3.9+) versions of Python and pip. So this should work in whatever desktop environment you use: alongside Git and your Markdown editor or IDE.</p> <p>Note</p> <p> k0rdent's MkDocs environment requires Python 3.9+ and a corresponding version of pip. </p> <p>Assuming you have a machine (desktop, laptop, VM) for editing with recent (Python 3.9+) versions of Python and pip:</p> <ol> <li> <p>Begin by creating a fork of k0rdent/docs on GitHub, and clone your fork to your machine. See Making regular contributions - use Git on the desktop</p> </li> <li> <p>Switch into the cloned project directory on your machine:</p> </li> </ol> <pre><code>cd docs\n</code></pre> <ol> <li>Create a venv into which to install MkDocs and dependencies, saved in the /docs repo in a file called <code>requirements.txt</code>.</li> </ol> <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Your terminal should now show <code>(venv)</code> in the prompt. This will create a local /venv folder in your copy of the repo, which the    .gitignore already in your clone will instruct GitHub to ignore when you make commits and file pull requests. </p> <ol> <li> <p>Install MkDocs and dependencies in the venv, using <code>requirements.txt</code>.</p> <p>Note</p> <p> The file <code>requirements.txt</code> is in the toplevel containing folder (called <code>docs</code>) \u2014 not in the <code>/docs</code> subfolder within the toplevel directory.</p> </li> </ol> <pre><code>pip install -r requirements.txt \n</code></pre> <p>Once you've done this, you can execute MkDocs to dynamically build and serve docs locally on localhost (<code>http://127.0.0.1:8000</code>):</p> <pre><code>python3 mkdocs serve\n</code></pre> <p>Then just visit localhost (<code>http://127.0.0.1:8000</code>) in a browser to view and navigate the docs.</p>"},{"location":"contrib/k0rdent-documentation-contributors-guide/#run-mkdocs-in-a-container-to-view-k0rdent-docs-locally","title":"Run MkDocs in a container to view k0rdent docs locally","text":"<p>If you prefer, you can also run MkDocs in a container on your machine to view k0rdent docs locally. This is another way of keeping the MkDocs environment isolated and easily removable.</p> <p>We've put a Dockerfile in the toplevel directory of the repo to help with this. Here's what's in that Dockerfile:</p> <pre><code># Use the official MkDocs Material image\nFROM squidfunk/mkdocs-material:latest\n\n# Set the working directory\nWORKDIR /docs\n\n# Copy and install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy MkDocs files\nCOPY mkdocs.yml .\n\n# Expose the MkDocs server port\nEXPOSE 8000\n</code></pre> <p>As you can see, we're using the official MkDocs Material base image. The Dockerfile creates a working directory called /docs, installs requirements.txt with pip, copies in the mkdocs.yml configuration file, and exposes port 8000 so the container can be accessed as localhost on your machine.</p> <ol> <li>You'll need Docker installed on your machine. Follow instructions here</li> <li>Go to the toplevel directory of the repo, where the Dockerfile and mkdocs.yml reside.</li> <li>Build the container, tagging it with the name <code>mk-local</code>.</li> </ol> <pre><code>docker build -f Dockerfile -t mk-local\n</code></pre> <ol> <li>Then use git to (optionally create and/or) checkout the branch you'll be working in, since this is the branch you want MkDocs to render from.</li> </ol> <pre><code>git checkout -b my-working-branch       # create and checkout new working branch, or ...\ngit checkout my-working-branch          # checkout existing working branch\n</code></pre> <ol> <li>Open a new terminal (so you can stop the container within this terminal window by pressing CTRL-C later), then run the container like this:</li> </ol> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs mk-local\n</code></pre> <p>This ensures that the container is removed when you're finished with it, that it launches as interactive and with a TTY, that the container's port 8000 is mapped to port 8000 on your local machine (enabling you to view the container on localhost - <code>http://127.0.0.1:8000</code>), and it mounts your current directory (the toplevel of the repo) as /docs inside the container. This lets MkDocs see your local copy of the repo to render and serve it to the browser, and refreshes each time you make a change.</p> <p>Just visit the container on <code>http://127.0.0.1:8000</code> to view the docs.</p> <ol> <li>When you want to stop the container, press CTRL+C inside the terminal window used to start it. Alternatively, you can stop the container from any terminal as follows:</li> </ol> <pre><code>docker ps                       # find the container ID, then ...\ndocker stop &lt;container-id&gt;      # stop the container\n</code></pre> <p>Because we started the container with the --rm flag, it will be removed from Docker Engine when it stops. Restart by once again entering:</p> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs mk-local\n</code></pre>"},{"location":"contrib/k0rdent-documentation-style-guide/","title":"k0rdent documentation style guide","text":"<p>This page gives style guidance to our Markdown, and makes some general recommendations regarding writing style for docs. It is not a complete style guide. In most cases, Project k0rdent aspires to follow style recommendations adopted by the Kubernetes Project's SIG Docs group. Please see the Kubernetes Documentation Style Guide for details.</p> <p>If you are looking for technical recommendations about how to contribute to docs, these are found in the k0rdent Documentation Contributors Guide.</p>"},{"location":"contrib/k0rdent-documentation-style-guide/#our-markdown-style-theme-and-extensions","title":"Our Markdown style, theme, and extensions","text":"<p>k0rdent docs uses:</p> <ul> <li>The MkDocs static site generator.</li> <li>The Material theme for MkDocs.</li> </ul> <p>... plus other extensions and plugins that have less of an effect on how you write documentation. Versions of all components are in the file <code>requirements.txt</code> at toplevel in k0rdent/docs.</p> <p>At present, we are mostly utilizing pure CommonMark with certain Material theme extensions (see below), though we are experimenting with other plugins and extensions.</p>"},{"location":"contrib/k0rdent-documentation-style-guide/#commonmark-basics","title":"CommonMark basics","text":"<p>If you don't know Markdown already, CommonMark is a strongly-defined specification of Markdown with unambiguous syntax, plus a suite of tests to validate Markdown implementations against the specification. It's used in MkDocs and (with some iffyness around the edges) GitHub, which means our docs are readable when we use MkDocs to render them into static pages for display on the web, and also readable (in most cases) when you surf GitHub and look at the render there.</p> <p>Comfortingly, the CommonMark spec is very similar to whatever version(s) of Markdown you already know (e.g., GitHub-flavored Markdown) as well as allied structured-text specifications that share DNA with Markdown, like ReStructuredText (.rst), commonly used in open source docs environments.</p> <p>Some resources you may find useful:</p> <ul> <li>CommonMark Markdown Reference</li> <li>CommonMark Markdown Tutorial</li> </ul>"},{"location":"contrib/k0rdent-documentation-style-guide/#admonitions","title":"Admonitions","text":"<p>For the moment, we're coming up to speed with a modern solution for admonitions, but meanwhile are using the OG CommonMark form of 'notes are blockquotes with special headers, rendered prettily by MkDocs. Examples:</p> <pre><code>&gt;NOTE:\n&gt;This is a note.\n&gt;It can go over several lines as needed, and can contain `code`, **boldface**, _italics_, and other body-copy Markdown elements.\n\n&gt;INFO:\n&gt;This is an info box.\n&gt;It can go over several lines as needed, and can contain `code`, **boldface**, _italics_, and other body-copy Markdown elements.\n\n&gt;CAUTION:\n&gt;This is a cautionary admonishment.\n&gt;It can go over several lines as needed, and can contain `code`, **boldface**, _italics_, and other body-copy Markdown elements.\n\n&gt;WARNING:\n&gt;This is a warning.\n&gt;It can go over several lines as needed, and can contain `code`, **boldface**, _italics_, and other body-copy Markdown elements.\n</code></pre> <p>Note</p> <p> This is a note. It can go over several lines as needed, and can contain <code>code</code>, boldface, italics, and other body-copy Markdown elements.</p> <p>Info</p> <p> This is an info box. It can go over several lines as needed, and can contain <code>code</code>, boldface, italics, and other body-copy Markdown elements.</p> <p>Caution</p> <p> This is a cautionary admonishment. It can go over several lines as needed, and can contain <code>code</code>, boldface, italics, and other body-copy Markdown elements.</p> <p>Warning</p> <p> This is a warning. It can go over several lines as needed, and can contain <code>code</code>, boldface, italics, and other body-copy Markdown elements.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Q: What is the best way to upgrade the Kubernetes version of a k0rdent child cluster?</p> <p>A: The simplest way to change the Kubernetes version of a specific child cluster is by updating the <code>ClusterDeployment</code> object. Where to updated it depends on the <code>ClusterTemplate</code> on which the <code>ClusterDeployment</code> is based.  For example, for built-in AWS, Azure, GCP, OpenStack, and Vsphere cluster templates (both standalone and hosted) you set the <code>spec.config.k0s.version</code> value to the desired Kubernetes version. For EKS, and AKS, you set <code>kubernetes.version</code>, and for GKE you set the <code>version</code> value. Note that this applies to built-in templates only; Custom templates may have another parameter exposed to represent the Kubernetes version.</p> <p>You can also create a new <code>ClusterTemplate</code> that will set the version for new clusters created based on that template.</p> <p>Q: Is it possible to control the version of the provider in the KCM Management object? For example, what if k0rdent is using the Kubernetes Cluster API for OpenStack (CAPO) v0.12.4 but I want to use v0.12.2?</p> <p>A: You can change the Kubernetes Cluster API version by creating a new <code>ProviderTemplate</code> that references a Helm chart with the appropriate provider components (with either the desired version hardcoded or configurable via <code>values.yaml</code>), then updating the <code>Management</code> object to reference that template. For example, you could update the <code>Management</code> object to specify a new template for the OpenStack provider:</p> <pre><code>spec:\n  providers:\n  - name: cluster-api-provider-openstack\n    template: new-capo-provider-template-name\n</code></pre>"},{"location":"quickstarts/","title":"Guide to QuickStarts","text":"<p>The following QuickStart chapters provide a recipe for quickly installing and trying k0rdent. Setting up k0rdent for production is detailed in the Administrator Guide.</p>"},{"location":"quickstarts/#what-the-quickstart-covers","title":"What the QuickStart covers","text":"<p>The goal of the QuickStart is:</p> <ul> <li>To get a working environment set up for managing k0rdent.</li> <li>To get a Kubernetes cluster and other tools set up for hosting k0rdent itself.</li> <li>To select a cloud environment (AWS or Azure) and configure k0rdent to lifecycle manage clusters on this substrate.</li> <li>To use k0rdent to deploy a managed cluster.</li> <li>(Optional stretch goal) It's also possible to set up the k0rdent management cluster to simultaneously lifecycle manage clusters on both cloud environments.</li> </ul>"},{"location":"quickstarts/#where-the-quickstart-leads","title":"Where the QuickStart leads","text":"<p>The QuickStart shows and briefly explains the hows, whys, and wherefores of manually setting up k0rdent for use. Once built and validated, the QuickStart setup can be leveraged to begin an expanding sequence of demos that let you explore k0rdent's many features. The demos presently use makefiles to speed and simplify setup and operations. We strongly recommend exploring this fast-evolving resource.</p>"},{"location":"quickstarts/#quickstart-prerequisites","title":"QuickStart Prerequisites","text":"<p>QuickStart prerequisites are simple \u2014 you'll need:</p> <ul> <li>A desktop or virtual machine running a supported version of linux. You'll use this machine to install a basic Kubernetes working environment, and to host a single-node k0s Kubernetes management cluster to host k0rdent components. For simplest setup, configure this machine as follows:</li> <li>A minimum of 8GB RAM, 4 vCPUs, 100GB SSD (for example, AWS <code>t2.xlarge</code> or equivalent)</li> <li>Set up for SSH access using keys (standard for cloud VMs)</li> <li>Set up for passwordless sudo (that is, edit <code>/etc/sudoers</code> to configure your user to issue <code>sudo</code> commands without a password challenge)</li> <li>Inbound traffic: SSH (port 22) and ping from your laptop's IP address</li> <li>Outbound traffic: All to any IP address</li> <li>Apply all recent updates and upgrade local applications (<code>sudo apt update</code>/<code>sudo apt upgrade</code>)</li> <li>(Optional) snapshot the machine in its virgin state</li> <li>Administrative-level access to an AWS or Azure cloud account, depending on which cloud environment you prefer. k0rdent will leverage this cloud to provide infrastructure for hosting child clusters.</li> </ul>"},{"location":"quickstarts/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Any linux based os that supports deploying k0s will work, though you may need to adjust the suggested commands.</p> OS Package Manager Link Ubuntu Server <code>apt</code> 22.04.5 LTS, Jammy Jellyfish <p>Note</p> <p>Other recent versions of 'enterprise' Linux should work with the following instructions as well, though you will need to adapt for different package managers and perhaps use slightly-different provider-recommended methods for installing required dependencies (for example, Helm). Once you've installed k0rdent in the management cluster and have kubectl, Helm, and other resources connected, you'll mostly be dealing with Kubernetes, and everything should work the same way on any host OS.</p>"},{"location":"quickstarts/#limitations","title":"Limitations","text":"<p>This QuickStart guides you in quickly creating a minimal k0rdent working environment. Setting up k0rdent for production is detailed in the Administrator Guide.</p> <p>The current QuickStart focuses on AWS and Azure cloud environments, and guides in creating 'standalone' clusters. In k0rdent parlance, that means 'CNCF-certified Kubernetes clusters with control planes and workers hosted on cloud virtual machines.' The 'CNCF-certified Kubernetes cluster' (in this case) is the k0s Kubernetes distro.</p> <p>k0rdent can do so much more today. Let's take a look at what's possible.</p>"},{"location":"quickstarts/#coming-soon","title":"Coming soon","text":"<p>QuickStarts for other Kubernetes distros, clouds, and environments will appear in the near future (short-term roadmap below):</p> <ul> <li>AWS EKS hosted \u2014 Amazon Elastic Kubernetes Service managed clusters </li> <li>Azure AKS hosted \u2014 Azure Kubernetes Service</li> <li>vSphere standalone \u2014 k0s Kubernetes on vSphere virtual machines</li> <li>OpenStack standalone \u2014 k0s Kubernetes on OpenStack virtual machines</li> </ul> <p>Plus (intermediate-term roadmap) tutorials for using k0rdent to create and manage hybrid, edge, and distributed platforms with Kubernetes-hosted control planes and workers on local or remote substrates will be available soon.</p> <p>Demo/Tutorials: We will also be converting the demos gradually into tutorials that explain how to use k0rdent for:</p> <ul> <li>Adding services to individual managed clusters, enabling management of complete platforms/IDPs</li> <li>Adding services to multiple managed clusters, enabling at-scale implementation and lifecycle management of standardized environments</li> <li>(As a Platform Architect) Authorizing cluster and service templates for use by others, and constraining their use within guardrails (enabling self-service)</li> <li>(As an authorized user) Leveraging shared cluster and service templates to lifecycle manage platforms (performing self-service)</li> <li>... and more</li> </ul> <p>Next you'll want to learn how to:</p> <ul> <li>Set up the Management Node and Cluster</li> <li>Configure and Deploy to AWS</li> <li>Configure and Deploy to Azure</li> <li>Configure and Deploy on any SSH-accessible Linux hosts</li> <li>Configure and Deploy to GCP</li> </ul>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/","title":"QuickStart 1 - Setup Management Cluster","text":"<p>Please review the Guide to QuickStarts for preliminaries. This QuickStart unit details setting up a single-VM environment for managing and interacting with k0rdent, and for hosting its components on a single-node local Kubernetes management cluster. Once k0rdent is installed on the management cluster, you can drive it by SSHing into the management node (<code>kubectl</code> is there and will be provisioned with the appropriate <code>kubeconfig</code>) or remotely by various means. For example, you can install the management cluster <code>kubeconfig</code> in Lens or another Kubernetes dashboard on your laptop, tunnel across from your own local <code>kubectl</code>, and so on.</p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#install-a-kubernetes-cluster-as-the-management-cluster","title":"Install a Kubernetes cluster as the management cluster","text":"<p>The k0rdent management cluster is a plain Kubernetes cluster; you can use any CNCF-certified  distribution to create it. For example, we have documented creating a single-node k0s cluster, a multi-node k0s cluster,  and an AWS EKS multi-node cluster to use as your management cluster. The instructions on this page explain the single-node k0s process, but feel free to deploy one of the other configurations, then move on to configuring and creating a child cluster on your chosen architecture.</p> <p>k0s Kubernetes is a CNCF-certified minimal single-binary Kubernetes that installs with one command and brings along its own CLI. We're using it to quickly set up a single-node management cluster on our manager node. However, k0rdent works on any CNCF-certified Kubernetes. If you choose to use something else, we would love to hear how you set things up to work for you.</p> <p>Note</p> <p> k0s isn't supported on MacOS, but you can install a test cluster using: <pre><code>brew install kind &amp;&amp; kind create cluster -n k0rdent\n</code></pre> Skip ahead to Install kubectl.</p> <p>Download and install k0s:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --enable-worker --no-taints\nsudo k0s start\n</code></pre> <p>You can check to see if the cluster is working by leveraging <code>kubectl</code> (installed and configured automatically by k0s) via the k0s CLI:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#install-kubectl","title":"Install kubectl","text":"<p>k0s installs a compatible <code>kubectl</code> and makes it accessible via its own client. But to make your environment easier to configure, we advise installing <code>kubectl</code> the normal way on the manager node and using it to control the local k0s management cluster:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#get-the-local-k0s-clusters-kubeconfig-for-kubectl","title":"Get the local k0s cluster's kubeconfig for kubectl","text":"<p>On startup, k0s stores the administrator's <code>kubeconfig</code> in a local directory, making it easy to access:</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>At this point, your newly-installed <code>kubectl</code> should be able to interoperate with the k0s management cluster with administrative privileges. Test to see that the cluster is ready (this usually takes about one minute):</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#install-helm","title":"Install Helm","text":"<p>The Helm Kubernetes package manager is used to install k0rdent services. We'll install Helm as follows:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Issuing these commands should produce something very much like the following output:</p> <pre><code>Downloading https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#install-k0rdent","title":"Install k0rdent","text":"<p>Now we'll install k0rdent itself into the k0s management cluster:</p> <p><pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 1.5.0 -n kcm-system --create-namespace\n</code></pre> <pre><code>Pulled: ghcr.io/k0rdent/kcm/charts/kcm:1.5.0\nDigest: sha256:382291ed68a79eb8e03a5b2f2b7d06b56e3fb798158c519433e254b695955391\nNAME: kcm\nLAST DEPLOYED: Thu Oct 30 12:54:32 2025\nNAMESPACE: kcm-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre></p> <p>k0rdent startup takes several minutes.</p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#check-that-k0rdent-cluster-management-pods-are-running","title":"Check that k0rdent cluster management pods are running","text":"<p>One fundamental k0rdent subsystem, k0rdent Cluster Manager (KCM), handles cluster lifecycle management on clouds and infrastructures. For example, it helps you configure and compose clusters and manages infrastructure via Cluster API (CAPI). Before continuing, check that the KCM pods are ready:</p> <pre><code>kubectl get pods -n kcm-system   # check pods in the kcm-system namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre> <p>Pods reported in states other than Running should become ready momentarily.</p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#check-that-the-projectsveltos-pods-are-running","title":"Check that the projectsveltos pods are running","text":"<p>The other fundamental k0rdent subsystem, k0rdent State Manager (KSM), handles services configuration and lifecycle management on clusters. This utilizes the projectsveltos Kubernetes Add-On Controller and other open source projects. Before continuing, check that the KSM pods are ready:</p> <pre><code>kubectl get pods -n projectsveltos   # check pods in the projectsveltos namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre> <p>If you have fewer pods than shown above, just wait 5 minutes or so for all the pods to reconcile and start running.</p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#verify-that-k0rdent-itself-is-ready","title":"Verify that k0rdent itself is ready","text":"<p>The actual measure of whether k0rdent is ready is the state of the <code>Management</code> object. To check, issue this command:</p> <p><pre><code>kubectl get Management -n kcm-system\n</code></pre> <pre><code>NAME   READY   RELEASE     AGE\nkcm    True    kcm-1-5-0   9m\n</code></pre></p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-provider-and-related-templates-are-available","title":"Verify that KCM provider and related templates are available","text":"<p>k0rdent KCM leverages CAPI to manage Kubernetes cluster assembly and host infrastructure. CAPI requires infrastructure providers for different clouds and infrastructure types. These are delivered and referenced within k0rdent using templates, instantiated in the management cluster as objects. Before continuing, verify that the default provider template objects are installed and verified. Other templates are also stored as provider templates in this namespace (for example, the templates that determine setup of KCM itself and other parts of the k0rdent system, such as projectsveltos, which is a component of k0rdent State Manager (KSM, see below)) as well as the k0smotron subsystem, which enables creation and lifecycle management of managed clusters that use Kubernetes-hosted control planes (such as control planes as pods):</p> <pre><code>kubectl get providertemplate -n kcm-system   # list providertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME                                   VALID\ncluster-api-1-0-7                                 true\ncluster-api-provider-aws-1-0-7                    true\ncluster-api-provider-azure-1-0-9                  true\ncluster-api-provider-docker-1-0-5                 true\ncluster-api-provider-gcp-1-0-6                    true\ncluster-api-provider-infoblox-1-0-2               true\ncluster-api-provider-ipam-1-0-3                   true\ncluster-api-provider-k0sproject-k0smotron-1-0-11   true\ncluster-api-provider-openstack-1-0-10              true\ncluster-api-provider-vsphere-1-0-6                true\nk0smotron-1-0-11                                   true\nkcm-1-5-0                                         true\nprojectsveltos-1-1-1                             true\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-clustertemplate-objects-are-available","title":"Verify that KCM ClusterTemplate objects are available","text":"<p>CAPI also requires control plane and bootstrap (worker node) providers to construct and/or manage different Kubernetes cluster distros and variants. Again, these providers are delivered and referenced within k0rdent using templates, instantiated in the management cluster as <code>ClusterTemplate</code> objects. Before continuing, verify that default <code>ClusterTemplate</code> objects are installed and valid:</p> <pre><code>kubectl get clustertemplate -n kcm-system   # list clustertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME                            VALID\nadopted-cluster-1-0-1           true\naws-eks-1-0-3                   true\naws-hosted-cp-1-0-16             true\naws-standalone-cp-1-0-16         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-19           true\nazure-standalone-cp-1-0-17       true\ndocker-hosted-cp-1-0-4          true\ngcp-gke-1-0-6                   true\ngcp-hosted-cp-1-0-16             true\ngcp-standalone-cp-1-0-15         true\nopenstack-standalone-cp-1-0-17   true\nremote-cluster-1-0-15            true\nvsphere-hosted-cp-1-0-15         true\nvsphere-standalone-cp-1-0-15     true\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#next-steps","title":"Next steps","text":"<p>Your QuickStart management node is now complete, and k0rdent is installed and operational. Next, it's time to select AWS, Azure, GCP, or Remote SSH Servers as an environment for hosting managed clusters.</p>"},{"location":"quickstarts/quickstart-2-aws/","title":"QuickStart 2 - AWS target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Amazon Web Services (AWS), and we'll deploy our first child cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an AWS account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done one of the other quickstarts, such as our Azure QuickStart (QuickStart 2 - Azure target environment), you can  use the same management cluster, continuing here with steps to add the ability to manage clusters on AWS. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to AWS (for example, it could be on an Azure virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Note</p> <p> CLOUD SECURITY 101: k0rdent requires some but not all permissions to manage AWS via the CAPA (ClusterAPI for AWS) provider.</p> <p>Because k0rdent doesn't require all permissions, a best practice for using k0rdent with AWS (this pattern is repeated with other clouds and infrastructures) is to create a new user on your account with the particular permissions it and CAPA require. In this section, we'll create and configure IAM for that user, and perform other steps to make that k0rdent user's credentials accessible to it in the management node.</p> <p>Note</p> <p> If you're working on a shared AWS account, please ensure that the k0rdent user is not already set up before creating a new one.</p> <p>Creating a k0rdent user with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with Kubernetes at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstarts/quickstart-2-aws/#install-the-aws-cli","title":"Install the AWS CLI","text":"<p>We'll use the AWS CLI to create and set IAM permissions for the k0rdent user, so we'll install it on our management node:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#install-clusterawsadm","title":"Install clusterawsadm","text":"<p>k0rdent uses Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. This QuickStart leverages <code>clusterawsadm</code>, a CLI tool created by the CAPA project that helps with AWS-specific tasks like IAM role, policy, and credential configuration.</p> <p>To install <code>clusterawsadm</code> on Ubuntu on x86 hardware:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#export-your-administrative-credentials","title":"Export your administrative credentials","text":"<p>You should have these already, preserved somewhere safe. If not, you can visit the AWS webUI (Access Management &gt; Users) and generate new credentials (Access Key ID, Secret Access Key, and Session Token (if using multi-factor authentication)).</p> <p>Export the credentials to the management node environment:</p> <pre><code>export AWS_REGION=EXAMPLE_AWS_REGION\nexport AWS_ACCESS_KEY_ID=EXAMPLE_ACCESS_KEY_ID\nexport AWS_SECRET_ACCESS_KEY=EXAMPLE_SECRET_ACCESS_KEY\nexport AWS_SESSION_TOKEN=EXAMPLE_SESSION_TOKEN # Optional. If you are using Multi-Factor or Single Sign On Auth.\n</code></pre> <p>These credentials will be used both by the AWS CLI (to create your k0rdent user) and by <code>clusterawsadm</code> (to create a CloudFormation template used by CAPA within k0rdent).</p>"},{"location":"quickstarts/quickstart-2-aws/#create-the-k0rdent-aws-user","title":"Create the k0rdent AWS user","text":"<p>Now we can use the AWS CLI to create a new k0rdent user:</p> <p><pre><code> aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <pre><code>{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"k0rdentQuickstart\",\n        \"UserId\": \"EXAMPLE_USER_ID\",\n        \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n        \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n    }\n}\n</code></pre></p> <p>If you don't have permissions to create the user, ask your administrator for assistance. Once the user is created, you won't have to do it again. You can also show them this page to make sure you won't run into any other issues.</p>"},{"location":"quickstarts/quickstart-2-aws/#check-for-available-ips","title":"Check for available IPs","text":"<p>Because k0rdent has 3 availablilty zone NAT gateways, each cluster needs 3 public IPs. Unfortunately, the default <code>EC2-VPC Elastic IPs</code> quota per region is 5, so while you likely won't have issues with a first cluster, if you try to deplay a  second to the same region, you are likely to run into issues.  </p> <p>You can determine how many elastic IPs are available from the command line:</p> <p><pre><code>LIMIT=$(aws ec2 describe-account-attributes --attribute-names vpc-max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)\nUSED=$(aws ec2 describe-addresses --query 'Addresses[*].PublicIp' --output text | wc -w)\nAVAILABLE=$((LIMIT - USED))\necho \"Available Public IPs: $AVAILABLE\"\n</code></pre> <pre><code>Available Public IPs: 5\n</code></pre></p> <p>If you have less than 3 available public IPs, you can request an increase in your quota:</p> <pre><code>aws service-quotas request-service-quota-increase \\\n    --service-code ec2 \\\n    --quota-code L-0263D0A3 \\\n    --desired-value 20\n</code></pre> <p>You can check on the status of your request:</p> <p><pre><code>aws service-quotas list-requested-service-quota-change-history \\\n    --service-code ec2\n</code></pre> <pre><code>{\n    \"RequestedQuotas\": [\n        {\n            \"Id\": \"EXAMPLE_ACCESS_KEY_ID\",\n            \"ServiceCode\": \"ec2\",\n            \"ServiceName\": \"Amazon Elastic Compute Cloud (Amazon EC2)\",\n            \"QuotaCode\": \"L-0263D0A3\",\n            \"QuotaName\": \"EC2-VPC Elastic IPs\",\n            \"DesiredValue\": 20.0,\n            \"Status\": \"PENDING\",\n            \"Created\": \"2025-02-09T02:27:01.573000-05:00\",\n            \"LastUpdated\": \"2025-02-09T02:27:01.956000-05:00\",\n            \"Requester\": \"{\\\"accountId\\\":\\\"EXAMPLE_ACCESS_KEY_ID\\\",\\\"callerArn\\\":\\\"arn:aws:iam::EXAMPLE_ACCESS_KEY_ID:user/nchase\\\"}\",\n            \"QuotaArn\": \"arn:aws:servicequotas:EXAMPLE_AWS_REGION:EXAMPLE_ACCESS_KEY_ID:ec2/L-0263D0A3\",\n            \"GlobalQuota\": false,\n            \"Unit\": \"None\",\n            \"QuotaRequestedAtLevel\": \"ACCOUNT\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-aws/#configure-aws-iam-for-k0rdent","title":"Configure AWS IAM for k0rdent","text":"<p>Before k0rdent CAPI can manage resources on AWS, you need to use <code>clusterawsadm</code> to create a bootstrap CloudFormation stack with additional IAM policies and a service account. You do this under the administrative account credentials you earlier exported to the management node environment:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#attach-iam-policies-to-the-k0rdent-user","title":"Attach IAM policies to the k0rdent user","text":"<p>Next, we'll attach appropriate policies to the k0rdent user. These are:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers-eks.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> <p>We use the AWS CLI to attach them. To do this, you will need to extract the Amazon Resource Name (ARN) for the newly-created user:</p> <pre><code>AWS_ARN_ID=$(aws iam get-user --user-name k0rdentQuickstart --query 'User.Arn' --output text | sed -E 's/.*::([0-9]+):.*/\\1/')\necho $AWS_ARN_ID\n</code></pre> <p>Assemble and execute the following commands to implement the required policies:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> <p>We can check to see that policies were attached to the new user:</p> <p><pre><code>aws iam list-attached-user-policies --user-name k0rdentQuickstart\n</code></pre> And you'll see output that looks like this (this is non-valid example text):</p> <pre><code>{\n    \"AttachedPolicies\": [\n        {\n            \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyArn\": \"arn:aws:iam::AWS_ARN_ID:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\"\n        },\n        {\n            \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyArn\": \"arn:aws:iam::AWS_ARN_ID:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\"\n        },\n        {\n            \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyArn\": \"arn:aws:iam::AWS_ARN_ID:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\"\n        },\n        {\n            \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyArn\": \"arn:aws:iam::AWS_ARN_ID:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\"\n        }\n    ]\n}\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-aws-credentials-for-the-k0rdent-user","title":"Create AWS credentials for the k0rdent user","text":"<p>In the AWS IAM Console, you can now create the Access Key ID and Secret Access Key for the k0rdent user and download them. You can also do this via the AWS CLI:</p> <pre><code>aws iam create-access-key --user-name k0rdentQuickstart\n</code></pre> <p>You should see something like this. It's important to save these credentials securely somewhere other than the management node, since the management node may end up being ephemeral. Again, this is non-valid example text:</p> <pre><code>{\n    \"AccessKey\": {\n        \"UserName\": \"k0rdentQuickstart\",\n        \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n        \"Status\": \"Active\",\n        \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n        \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-iam-credentials-secret-on-the-management-cluster","title":"Create IAM credentials secret on the management cluster","text":"<p>Next, we create a <code>Secret</code> containing credentials for the k0rdent user and apply this to the management cluster running k0rdent, in the <code>kcm-system</code> namespace. Important: if you use another namespace, k0rdent will be unable to read the credentials. To do this, create the following YAML in a file called <code>aws-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: \"EXAMPLE_ACCESS_KEY_ID\"\n  SecretAccessKey: \"EXAMPLE_SECRET_ACCESS_KEY\"\n</code></pre> <p>Remember: the Access Key ID and Secret Access Key are the ones you generated for the k0rdent user, <code>k0rdentQuickStart</code>.</p> <p>Apply this YAML to the management cluster as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-the-awsclusterstaticidentity-object","title":"Create the AWSClusterStaticIdentity object","text":"<p>Next, we need to create an <code>AWSClusterStaticIdentity</code> object that uses the secret.</p> <p>To do this, create a YAML file named <code>aws-cluster-identity.yaml</code> as follows:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Note that the <code>spec.secretRef</code> is the same as the <code>metadata.name</code> of the secret we just created.</p> <p>Create the object as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-the-k0rdent-cluster-manager-credential-object","title":"Create the k0rdent Cluster Manager credential object","text":"<p>Now we create the k0rdent Cluster Manager credential object. As in prior steps, create a YAML file called <code>aws-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>Note that <code>.spec.identityRef.kind</code> must be <code>AWSClusterStaticIdentity</code> and <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>AWSClusterStaticIdentity</code> object.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-the-k0rdent-cluster-identity-resource-template-configmap","title":"Create the k0rdent Cluster Identity resource template ConfigMap","text":"<p>Now we create the k0rdent Cluster Identity resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>aws-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\n</code></pre> <p>Note that <code>ConfigMap</code> is empty. This is expected, we don't need to template any object inside child cluster(s), but we can use that object in the future if need arises.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-resource-template.yaml -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage AWS. To create a cluster, begin by listing the available <code>ClusterTemplate</code> objects provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>aws-standalone-cp-1-0-16</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-3                   true\nkcm-system   aws-hosted-cp-1-0-16             true\nkcm-system   aws-standalone-cp-1-0-16         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-19           true\nkcm-system   azure-standalone-cp-1-0-17       true\nkcm-system   docker-hosted-cp-1-0-4          true\nkcm-system   gcp-gke-1-0-6                   true\nkcm-system   gcp-hosted-cp-1-0-16             true\nkcm-system   gcp-standalone-cp-1-0-15         true\nkcm-system   openstack-standalone-cp-1-0-17   true\nkcm-system   remote-cluster-1-0-17            true\nkcm-system   vsphere-hosted-cp-1-0-15         true\nkcm-system   vsphere-standalone-cp-1-0-15     true\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-aws-clusterdeployment1.yaml</code>. We'll use this to create a <code>ClusterDeployment</code> object in k0rdent, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for k0rdent the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under (that of your k0rdent user), plus the region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-16\n  credential: aws-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n      rootVolumeSize: 32\n    worker:\n      instanceType: t3.small\n      rootVolumeSize: 32\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the <code>ClusterDeployment</code> to deploy the cluster","text":"<p>Finally, we'll apply the <code>ClusterDeployment</code> YAML (<code>my-aws-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can watch the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <p>In a short while, you'll see output such as:</p> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's <code>kubeconfig</code>:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1.kubeconfig\n</code></pre> <p>And you can use the <code>kubeconfig</code> to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-aws-clusterdeployment1.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                        READY   STATUS\nkcm-system   my-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code>:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-aws-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul>"},{"location":"quickstarts/quickstart-2-azure/","title":"QuickStart 2 - Azure target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Azure, and deploying a child cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an Azure account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done one of the other quickstarts, such as our AWS QuickStart (QuickStart 2 - AWS target environment), you can use the same management cluster, continuing here with steps to add the ability to manage clusters on Azure. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to Azure (for example, it could be on an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Note</p> <p> Cloud Security 101: k0rdent requires some but not all permissions to manage Azure resources via the CAPZ (ClusterAPI for Azure) provider. </p> <p>A best practice for using k0rdent with Azure (this pattern is repeated with other clouds and infrastructures) is to create a new Azure Cluster Identity and Service Principal (SP) on your account with the particular permissions k0rdent and CAPZ require. In this section, we'll create and configure those identity abstractions, and perform other steps to make required credentials accessible to k0rdent in the management node.</p> <p>Note</p> <p> If you're working on a shared Azure account, please ensure that the Azure Cluster Identity and Service Principal are not already set up before creating new abstractions.</p> <p>Creating user identity abstractions with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with Kubernetes at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstarts/quickstart-2-azure/#install-the-azure-cli-az","title":"Install the Azure CLI (az)","text":"<p>The Azure CLI (az) is required to interact with Azure resources. Install it according to instructions in How to install the Azure CLI. For Linux/Debian (i.e., Ubuntu Server), it's one command:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#log-in-with-azure-cli","title":"Log in with Azure CLI","text":"<p>Run the az login command to authenticate your session with Azure.</p> <pre><code>az login\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#register-resource-providers","title":"Register resource providers","text":"<p>Azure Resource Manager uses resource providers to manage resources of all different kinds, and required providers must be registered with an Azure account before k0rdent and CAPZ can work with them.</p> <p>You can list resources registered with your account using Azure CLI:</p> <pre><code>az provider list --query \"[?registrationState=='Registered']\" --output table\n</code></pre> <p>And see a listing like this:</p> <pre><code>Namespace                             RegistrationState\n-----------------------------------   -----------------\nMicrosoft.Compute                     Registered\nMicrosoft.Network                     Registered\n</code></pre> <p>You can then select from the commands below (or enter all of them) to register any unregistered resources that k0rdent and CAPZ require:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#get-your-azure-subscription-id","title":"Get your Azure Subscription ID","text":"<p>Use the following command to list Azure subscriptions and their IDs:</p> <pre><code>az account list -o table\n</code></pre> <p>The output will look like this:</p> <pre><code>Name                     SubscriptionId                    TenantId\n-----------------------  -------------------------------   -----------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID   TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre> <p>The Subcription ID is in the second column.</p>"},{"location":"quickstarts/quickstart-2-azure/#create-a-service-principal-for-k0rdent","title":"Create a Service Principal for k0rdent","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. To create it, run the following command with the Azure CLI, replacing <code>&lt;subscription-id&gt;</code> with the ID you copied earlier.</p> <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You'll see output that resembles what's below:</p> <pre><code>{\n \"appId\": \"SP_APP_ID_SP_APP_ID\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"SP_PASSWORD_SP_PASSWORD\",\n \"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> <p>Capture this output and secure the values it contains. We'll need several of these in a moment.</p>"},{"location":"quickstarts/quickstart-2-azure/#create-a-secret-object-with-the-azure-credentials","title":"Create a Secret object with the Azure credentials","text":"<p>In this quickstart we're assuming a self-managed Azure clusters (non-AKS) so create a <code>Secret</code> object that stores the <code>clientSecret</code> (password) from the Service Principal. Create a YAML file called <code>azure-cluster-identity-secret.yaml</code>, as follows, inserting the password for the Service Principal (represented by the placeholder <code>SP_PASSWORD_SP_PASSWORD</code> above):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: SP_PASSWORD_SP_PASSWORD # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to the k0rdent management cluster using the following command:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>secret/azure-cluster-identity-secret created\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#create-the-azureclusteridentity-object","title":"Create the AzureClusterIdentity Object","text":"<p>This object defines the credentials k0rdent and CAPZ will use to manage Azure resources. It references the <code>Secret</code> you just created above.</p> <p>Create a YAML file called <code>azure-cluster-identity.yaml</code>. Make sure that <code>.spec.clientSecret.name</code> matches the <code>metadata.name</code> in the file you created above.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  name: azure-cluster-identity\n  namespace: kcm-system\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  allowedNamespaces: {}\n  clientID: SP_APP_ID_SP_APP_ID # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: SP_TENANT_SP_TENANT # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <p>Note that for non-AKS clusters <code>.spec.kind</code> must be <code>AzureClusterIdentity</code>, and <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>AzureClusterIdentity</code> object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#create-the-configmap-resource-template-object","title":"Create the <code>ConfigMap</code> resource-template Object","text":"<p>Create a YAML with the specification of our resource-template (and the necessary <code>StorageClass</code>) and save it as <code>azure-cluster-identity-resource-template.yaml</code></p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: azure-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    {{- $subnetName := \"\" -}}\n    {{- $securityGroupName := \"\" -}}\n    {{- $routeTableName := \"\" -}}\n    {{- range $cluster.spec.networkSpec.subnets -}}\n      {{- if eq .role \"node\" -}}\n        {{- $subnetName = .name -}}\n        {{- $securityGroupName = .securityGroup.name -}}\n        {{- $routeTableName = .routeTable.name -}}\n        {{- break -}}\n      {{- end -}}\n    {{- end -}}\n    {{- $cloudConfig := dict\n      \"aadClientId\" $identity.spec.clientID\n      \"aadClientSecret\" (index $secret.data \"clientSecret\" | b64dec)\n      \"cloud\" $cluster.spec.azureEnvironment\n      \"loadBalancerName\" \"\"\n      \"loadBalancerSku\" \"Standard\"\n      \"location\" $cluster.spec.location\n      \"maximumLoadBalancerRuleCount\" 250\n      \"resourceGroup\" $cluster.spec.resourceGroup\n      \"routeTableName\" $routeTableName\n      \"securityGroupName\" $securityGroupName\n      \"securityGroupResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n      \"subnetName\" $subnetName\n      \"subscriptionId\" $cluster.spec.subscriptionID\n      \"tenantId\" $identity.spec.tenantID\n      \"useInstanceMetadata\" true\n      \"useManagedIdentityExtension\" false\n      \"vmType\" \"vmss\"\n      \"vnetName\" $cluster.spec.networkSpec.vnet.name\n      \"vnetResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n    -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: azure-cloud-provider\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-config: {{ $cloudConfig | toJson | b64enc }}\n    ---\n    apiVersion: storage.k8s.io/v1\n    kind: StorageClass\n    metadata:\n      name: managed-csi\n      annotations:\n        storageclass.kubernetes.io/is-default-class: \"true\"\n    provisioner: disk.csi.azure.com\n    parameters:\n      skuName: StandardSSD_LRS\n    reclaimPolicy: Delete\n    volumeBindingMode: WaitForFirstConsumer\n    allowVolumeExpansion: true\n</code></pre> <p>Object name needs to be exactly <code>azure-cluster-identity-resource-template</code>, <code>AzureClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-resource-template.yaml\n</code></pre> <pre><code>configmap/azure-cluster-identity-resource-template created\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-azure/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your Azure location/regions:</p> <pre><code>az account list-locations -o table\n</code></pre> <p>You'll see output like this:</p> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n. . .\n</code></pre> <p>What you'll need to insert in your <code>ClusterDeployment</code> is the name (center column) of the region to which you wish to deploy.</p>"},{"location":"quickstarts/quickstart-2-azure/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage Azure. To create a cluster, begin by listing the available <code>ClusterTemplate</code> objects:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the Azure standalone cluster template in its present version (in the example below, that's <code>azure-standalone-cp-1-0-17</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-3                   true\nkcm-system   aws-hosted-cp-1-0-16             true\nkcm-system   aws-standalone-cp-1-0-16         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-19           true\nkcm-system   azure-standalone-cp-1-0-17       true\nkcm-system   docker-hosted-cp-1-0-4          true\nkcm-system   gcp-gke-1-0-6                   true\nkcm-system   gcp-hosted-cp-1-0-16             true\nkcm-system   gcp-standalone-cp-1-0-15         true\nkcm-system   openstack-standalone-cp-1-0-17   true\nkcm-system   remote-cluster-1-0-17            true\nkcm-system   vsphere-hosted-cp-1-0-15         true\nkcm-system   vsphere-standalone-cp-1-0-15     true\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-azure-clusterdeployment1.yaml</code>. We'll use this to create a <code>ClusterDeployment</code> object in k0rdent, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for k0rdent the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under, plus the location/region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-1-0-17 # name of the clustertemplate\n  credential: azure-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    location: \"AZURE_LOCATION\" # Select your desired Azure Location\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the <code>ClusterDeployment</code> YAML (<code>my-azure-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's <code>kubeconfig</code>:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the <code>kubeconfig</code> to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-azure/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-azure/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul>"},{"location":"quickstarts/quickstart-2-gcp/","title":"QuickStart 2 - GCP target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on GCP and deploying a child cluster.</p> <p>Note that if you have already done one of the other quickstarts, such as our AWS or Azure QuickStart (QuickStart 2 - AWS target environment) or Azure QuickStart (QuickStart 2 - Azure target environment),  you can use the same management cluster, continuing here with steps to add the ability to manage clusters on GCP. The  k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to GCP (for example, it could be on  an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should  work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>The GCP provider is available starting with the k0rdent 0.2.0 release, but it's not enabled by default. To enable the GCP provider in k0rdent, edit the <code>Management</code> object and add <code>cluster-api-provider-gcp</code> to the list of <code>spec.providers</code>, and the GCP components will start within a few minutes.</p>"},{"location":"quickstarts/quickstart-2-gcp/#install-gcloud-cli","title":"Install Gcloud CLI","text":"<p>Follow Install the gcloud CLI instruction to install gcloud CLI on your local machine.</p>"},{"location":"quickstarts/quickstart-2-gcp/#authenticate-in-gcp-cloud","title":"Authenticate in GCP cloud","text":"<pre><code>gcloud auth login\n</code></pre> <p>By default, this command will obtain access credentials for your user account via a web-based authorization flow.</p> <p>Set the project in which you want to deploy a cluster. Run:</p> <pre><code>gcloud config set project PROJECT_ID\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#create-a-gcp-service-account","title":"Create a GCP Service Account","text":"<p>Note</p> <p> Skip this step if the Service Account already configured Follow the GCP Service Account creation guide and create a new service account with <code>Editor</code> permissions.</p> <p>If you have plans to deploy <code>GKE</code>, the Service Account will also need the <code>iam.serviceAccountTokenCreator</code> role.</p>"},{"location":"quickstarts/quickstart-2-gcp/#generate-json-key-for-the-gcp-service-account","title":"Generate JSON Key for the GCP Service Account","text":"<p>Note</p> <p> Skip this step if you're going to use an existing key Follow the Create a service account key guide and create a new key with the JSON key type.</p> <p>A JSON file will be automatically downloaded to your computer. You should keep it somewhere safe.</p> <p>The example of the JSON file: <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"project_id\",\n  \"private_key_id\": \"akdof8v8s6n39n29251be52cabbb3984c259f1\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"name@project_id.iam.gserviceaccount.com\",\n  \"client_id\": \"123456778\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/user%40project_id.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-gcp/#create-a-secret-object-with-the-gcp-gke-credentials","title":"Create a Secret object with the GCP (GKE) credentials","text":"<p>Create a <code>Secret</code> object that stores the <code>credentials</code> field under <code>data</code> section. Create a YAML file called <code>gcp-cluster-identity-secret.yaml</code>, as follows, inserting the base64-encoded GCP credentials (represented by the placeholder <code>GCP_B64ENCODED_CREDENTIALS</code> below) that you get on the previous step. To get base64 encoded credentials, run:</p> <pre><code>    GCP_B64ENCODED_CREDENTIALS=$(cat &lt;gcpJSONCredentialsFileName&gt; | base64 -w 0)\n</code></pre> <p><pre><code>cat &gt; gcp-cluster-identity-secret.yaml &lt;&lt; EOF\napiVersion: v1\nkind: Secret\nmetadata:\n\u00a0 name: gcp-cloud-sa\n\u00a0 namespace: kcm-system\n\u00a0 labels:\n\u00a0 \u00a0 k0rdent.mirantis.com/component: \"kcm\"\ndata:\n\u00a0 credentials: ${GCP_B64ENCODED_CREDENTIALS}\ntype: Opaque\nEOF\n</code></pre> Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f gcp-cluster-identity-secret.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>secret/gcp-cloud-sa created\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>gcp-credential.yaml</code>.</p> <p>Note that <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>Secret</code> object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: gcp-credential\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: gcp-cloud-sa\n    namespace: kcm-system\n</code></pre> <pre><code>kubectl apply -f gcp-credential.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/gcp-credential created\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#create-the-configmap-resource-template-object","title":"Create the <code>ConfigMap</code> resource-template Object","text":"<p>Create a YAML with the specification of our resource-template and save it as <code>gcp-cloud-sa-resource-template.yaml</code></p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: gcp-cloud-sa-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $secret := (getResource \"InfrastructureProviderIdentity\") -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: gcp-cloud-sa\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-sa.json: {{ index $secret \"data\" \"credentials\" }}\n</code></pre> <p>Object name needs to be exactly <code>gcp-cloud-sa-resource-template</code> (credentials <code>Secret</code> object name + <code>-resource-template</code> string suffix).</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f gcp-cloud-sa-resource-template.yaml\n</code></pre> <pre><code>configmap/gcp-cloud-sa-resource-template created\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-gcp/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your GCP regions:</p> <pre><code>gcloud compute regions list\n</code></pre> <p>You'll see output like this:</p> <pre><code>NAME                     CPUS    DISKS_GB  ADDRESSES  RESERVED_ADDRESSES  STATUS  TURNDOWN_DATE\nafrica-south1            0/300   0/102400  0/575      0/175               UP\nasia-east1               0/3000  0/102400  0/575      0/175               UP\nasia-east2               0/1500  0/102400  0/575      0/175               UP\nasia-northeast1          0/1500  0/102400  0/575      0/175               UP\nasia-northeast2          0/750   0/102400  0/575      0/175               UP\n. . .\n</code></pre> <p>What you'll need to insert in your ClusterDeployment is the name (first column) of the region you wish to deploy to.</p>"},{"location":"quickstarts/quickstart-2-gcp/#optional-find-or-create-your-network","title":"(optional) Find or create your network","text":"<p>If you want to deploy in existing network, obtain all network names using <code>gcloud</code> CLI and choose one of available: <pre><code>gcloud compute networks list --format=\"value(name)\"\n</code></pre></p> <p>If you prefer to create a new network, follow this instruction.</p>"},{"location":"quickstarts/quickstart-2-gcp/#determine-the-instance-type","title":"Determine the instance type","text":"<p>Find available machine types and its parameters by running:</p> <pre><code>gcloud compute machine-types list | grep REGION\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#find-available-images","title":"Find available images","text":"<p>To list all available images in your GCP project, run:</p> <pre><code>gcloud compute images list --uri\n</code></pre> <p>You'll see output like this:</p> <pre><code>https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-stream-9-arm64-v20250311\nhttps://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-stream-9-v20250311\nhttps://www.googleapis.com/compute/v1/projects/cos-cloud/global/images/cos-105-17412-535-84\nhttps://www.googleapis.com/compute/v1/projects/cos-cloud/global/images/cos-109-17800-436-79\nhttps://www.googleapis.com/compute/v1/projects/cos-cloud/global/images/cos-113-18244-291-82\n. . .\n</code></pre> <p>To build your own image, follow Building images instruction.</p> <p>Replace <code>REGION</code> with the region you'd like your cluster to live.</p>"},{"location":"quickstarts/quickstart-2-gcp/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage GCP. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the GCP standalone cluster template in its present version (in the example below, that's <code>gcp-standalone-cp-1-0-15</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-3                   true\nkcm-system   aws-hosted-cp-1-0-16             true\nkcm-system   aws-standalone-cp-1-0-16         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-19           true\nkcm-system   azure-standalone-cp-1-0-17       true\nkcm-system   docker-hosted-cp-1-0-4          true\nkcm-system   gcp-gke-1-0-6                   true\nkcm-system   gcp-hosted-cp-1-0-16             true\nkcm-system   gcp-standalone-cp-1-0-15         true\nkcm-system   openstack-standalone-cp-1-0-17   true\nkcm-system   remote-cluster-1-0-17            true\nkcm-system   vsphere-hosted-cp-1-0-15         true\nkcm-system   vsphere-standalone-cp-1-0-15     true\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-gcp-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for k0rdent the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under, the GCP project, region, network name, machine types and images you want to use to host control plane and worker nodes.</p> <p>Note</p> <p> If you decide to deploy cluster with <code>publicIP: false</code>, you should make sure your cluster can communicate with the outside world and the load balancer. To achieve this, you may create a Cloud NAT in the region you'd like your cluster to live in by following Set up and manage network address translation with Public NAT instruction.</p> <p>Note</p> <p> Image parameter must be the fully qualified GCP image path. Otherwise, the GCP cloud controller manager will not find it.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-gcp-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: gcp-standalone-cp-1-0-15 # name of the clustertemplate\n  credential: gcp-credential\n  config:\n    project: \"GCP_PROJECT_NAME\"\n    region: \"GCP_REGION\"\n    network:\n      name: \"GCP_NETWORK_NAME\" # The name of an existing network or a new network to be created by Cluster API Provider GCP\n    controlPlane:\n      instanceType: \"CP_MACHINE_TYPE\"\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n    worker:\n      instanceType: \"WORKER_MACHINE_TYPE\"\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-gcp-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-gcp-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-gcp-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-gcp-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-gcp-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-gcp-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-gcp-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-gcp-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-gcp/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl delete ClusterDeployment my-gcp-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-gcp-clusterdeployment1\" deleted\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-gcp/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"quickstarts/quickstart-2-remote/","title":"QuickStart 2 - Existing infrastructure","text":"<p>In many cases, you will want k0rdent (running on your management node) to deploy a k0s child cluster on existing infrastructure. The remote servers will serve as worker nodes in the cluster, while the control plane components will reside within the management cluster and be managed by k0smotron. By doing this, you maximize your existing hardware and simplify administration costs by decreasing the number of servers required to run your infrastructure.</p> <p>The remote machines that will be part of the cluster must meet the following prerequisites:</p> <ol> <li>Linux-based operating system; the remote hosts should meet the k0s system requirements</li> <li>SSH access enabled for the root user</li> <li>Internet access</li> <li>Connectivity between the management cluster and the remote hosts' networks</li> </ol> <p>If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done one of the other quickstarts, such as our AWS QuickStart (QuickStart 2 - AWS target environment) or (QuickStart 2 - Azure target environment), you can use the same management cluster, continuing here with steps to add the ability to manage remote clusters. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Before proceeding, make sure your management cluster meets the following requirements:</p> <ol> <li>A default storage class is configured on the management cluster to support Persistent Volumes.</li> <li>If the API server will be exposed as a <code>LoadBalancer</code>, ensure the appropriate cloud provider is installed on the management cluster.</li> </ol>"},{"location":"quickstarts/quickstart-2-remote/#create-a-secret-object-containing-the-private-ssh-key-to-access-remote-machines","title":"Create a Secret object containing the private SSH key to access remote machines","text":"<p>Create a <code>Secret</code> object to securely store the private SSH key, under the key <code>value</code>, for accessing all remote machines that will be part of the cluster. Start by setting the following environment variables, for example by adding them to an <code>.env</code> file and sourcing it. (Make sure to either update the <code>KEY_PATH</code> to point to the keyfile or enter the directly under <code>PRIVATE_SSH-KEY_B64</code>.)</p> <pre><code># Setup Environment\nKEY_PATH=~/.ssh/id_ed25519\nPRIVATE_SSH_KEY_B64=$(cat $KEY_PATH | base64 -w 0)\nSECRET_NAME=remote-ssh-key\nKCM_SYSTEM_NS=kcm-system\nCREDENTIAL_NAME=remote-cred\nRESOURCE_TEMPLATE_NAME=remote-ssh-key-resource-template\nCLUSTER_DEPLOYMENT_NAME=my-remote-clusterdeployment1\nMACHINE_0_ADDRESS=127.0.0.1\nMACHINE_1_ADDRESS=127.0.0.2\n</code></pre> <p>Now create the <code>remote-ssh-key-secret.yaml</code> file:</p> <pre><code>cat &gt; remote-ssh-key-secret.yaml &lt;&lt; EOF\napiVersion: v1\ndata:\n  value: $PRIVATE_SSH_KEY_B64\nkind: Secret\nmetadata:\n  name: $SECRET_NAME\n  namespace: $KCM_SYSTEM_NS\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nEOF\n</code></pre> <p>Apply the YAML to the k0rdent management cluster: <pre><code>kubectl apply -f remote-ssh-key-secret.yaml\n</code></pre> <pre><code>secret/remote-ssh-key created\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-remote/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML file with the specification of our credential and save it as <code>remote-cred.yaml</code>.</p> <p>Note that <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>Secret</code> object created in the previous step.</p> <pre><code>cat &gt; remote-cred.yaml &lt;&lt; EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: $CREDENTIAL_NAME\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: $SECRET_NAME\n    namespace: $KCM_SYSTEM_NS\nEOF\n</code></pre> <p>Apply the YAML to your cluster: <pre><code>kubectl apply -f remote-cred.yaml\n</code></pre> <pre><code>credential.k0rdent.mirantis.com/remote-cred created\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-remote/#create-the-cluster-identity-resource-template-configmap","title":"Create the Cluster Identity resource template ConfigMap","text":"<p>Now we create the k0rdent <code>ClusterIdentity</code> resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>remote-ssh-key-resource-template.yaml</code>:</p> <p><pre><code>cat &gt; remote-ssh-key-resource-template.yaml &lt;&lt; EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: $RESOURCE_TEMPLATE_NAME\n  namespace: $KCM_SYSTEM_NS\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\nEOF\n</code></pre> Note that the <code>ConfigMap</code> doesn't have any values, only metadata. This is expected, as we don't need to template any object inside child clusters, but we can use that object in the future if the need arises.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f remote-ssh-key-resource-template.yaml -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>To create a remote cluster, begin by listing the available <code>ClusterTemplate</code> objects provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling the following. Make note of the name of the Remote Cluster template in its present version (in the example below, that's <code>remote-cluster-1-0-15</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-3                   true\nkcm-system   aws-hosted-cp-1-0-16             true\nkcm-system   aws-standalone-cp-1-0-16         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-19           true\nkcm-system   azure-standalone-cp-1-0-17       true\nkcm-system   docker-hosted-cp-1-0-4          true\nkcm-system   gcp-gke-1-0-6                   true\nkcm-system   gcp-hosted-cp-1-0-16             true\nkcm-system   gcp-standalone-cp-1-0-15         true\nkcm-system   openstack-hosted-cp-1-0-17       true\nkcm-system   openstack-standalone-cp-1-0-17   true\nkcm-system   remote-cluster-1-0-15            true\nkcm-system   vsphere-hosted-cp-1-0-15         true\nkcm-system   vsphere-standalone-cp-1-0-15     true\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>To deploy a cluster, create a YAML file called <code>my-remote-clusterdeployment1.yaml</code>. We'll use this to create a <code>ClusterDeployment</code> object representing the deployed cluster. The <code>ClusterDeployment</code> identifies for k0rdent the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under, plus the machines' IP addresses (represented by the placeholder <code>MACHINE_0_ADDRESS</code> and <code>MACHINE_1_ADDRESS</code> below), the SSH port of the remote machines and the user to use when connecting to remote machines (<code>root</code>):</p> <p>Note</p> <p> The user must have root permissions.  Also, the service type should be correctly configured. If using the <code>LoadBalancer</code> service type, ensure the appropriate cloud provider is installed on the management cluster. For other service types (such as <code>ClusterIP</code> or <code>NodePort</code>), verify that the management cluster network is accessible from the host machines to allow virtual machines to connect to the API server.</p> <pre><code>cat &gt; my-remote-clusterdeployment1.yaml &lt;&lt; EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $CLUSTER_DEPLOYMENT_NAME\n  namespace: $KCM_SYSTEM_NS\nspec:\n  template: remote-cluster-1-0-15 # name of the clustertemplate\n  credential: remote-cred\n  propagateCredentials: false\n  config:\n    k0smotron:\n      service:\n        type: LoadBalancer\n    machines:\n    - address: $MACHINE_0_ADDRESS\n      user: root # The user must have root permissions \n      port: 22\n    - address: $MACHINE_1_ADDRESS\n      user: root # The user must have root permissions \n      port: 22\nEOF\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#apply-the-clusterdeployment-to-deploy-the-management-cluster","title":"Apply the ClusterDeployment to deploy the management cluster","text":"<p>Finally, we'll apply the <code>ClusterDeployment</code> YAML (<code>my-remote-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-remote-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm the creation:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-remote-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusters.cluster.x-k8s.io my-remote-clusterdeployment1 --watch\n</code></pre> <p>To verify that the remote machines were successfuly provisioned, run:</p> <pre><code>kubectl -n kcm-system get remotemachines.infrastructure.cluster.x-k8s.io -l helm.toolkit.fluxcd.io/name=my-remote-clusterdeployment1 -o=jsonpath={.items[*].status}\n</code></pre> <p>If the machines were provisioned, the output of this command will be similar to:</p> <pre><code>{\"ready\":true}{\"ready\":true}\n</code></pre> <p>If there is any error, the output will contain an error message.</p>"},{"location":"quickstarts/quickstart-2-remote/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-remote-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-remote-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-remote-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects on the management cluster:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-remote-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-remote/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code> from the management cluster:</p> <p><pre><code>kubectl delete ClusterDeployment my-remote-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-remote-clusterdeployment1\" deleted\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-remote/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros such as AWS EKS and Azure AKS</li> </ul>"},{"location":"reference/","title":"Reference Guides","text":"<p>The k0rdent documentation includes references to:</p> <ul> <li>k0rdent CRDs</li> <li>k0rdent Templates</li> </ul>"},{"location":"reference/crds/","title":"API Reference","text":"<p>Packages:</p> <ul> <li>k0rdent.mirantis.com/v1beta1</li> </ul>"},{"location":"reference/crds/#k0rdentmirantiscomv1beta1","title":"k0rdent.mirantis.com/v1beta1","text":"<p>Resource Types:</p> <ul> <li> <p>AccessManagement</p> </li> <li> <p>ClusterDeployment</p> </li> <li> <p>ClusterIPAMClaim</p> </li> <li> <p>ClusterIPAM</p> </li> <li> <p>ClusterTemplateChain</p> </li> <li> <p>ClusterTemplate</p> </li> <li> <p>Credential</p> </li> <li> <p>ManagementBackup</p> </li> <li> <p>Management</p> </li> <li> <p>MultiClusterService</p> </li> <li> <p>ProviderInterface</p> </li> <li> <p>ProviderTemplate</p> </li> <li> <p>Release</p> </li> <li> <p>ServiceTemplateChain</p> </li> <li> <p>ServiceTemplate</p> </li> </ul>"},{"location":"reference/crds/#accessmanagement","title":"AccessManagement","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagement is the Schema for the AccessManagements API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string AccessManagement true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            AccessManagementSpec defines the desired state of AccessManagement false status object            AccessManagementStatus defines the observed state of AccessManagement false"},{"location":"reference/crds/#accessmanagementspec","title":"AccessManagement.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagementSpec defines the desired state of AccessManagement</p> Name Type Description Required accessRules []object            AccessRules is the list of access rules. Each AccessRule enforces objects distribution to the TargetNamespaces. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindex","title":"AccessManagement.spec.accessRules[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessRule is the definition of the AccessManagement access rule. Each AccessRule enforces Templates and Credentials distribution to the TargetNamespaces</p> Name Type Description Required clusterTemplateChains []string            ClusterTemplateChains lists the names of ClusterTemplateChains whose ClusterTemplates will be distributed to all namespaces specified in TargetNamespaces. false credentials []string            Credentials is the list of Credential names that will be distributed to all the namespaces specified in TargetNamespaces. false serviceTemplateChains []string            ServiceTemplateChains lists the names of ServiceTemplateChains whose ServiceTemplates will be distributed to all namespaces specified in TargetNamespaces. false targetNamespaces object            TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset. Validations:<li>((has(self.stringSelector) ? 1 : 0) + (has(self.selector) ? 1 : 0) + (has(self.list) ? 1 : 0)) &lt;= 1: only one of spec.targetNamespaces.selector or spec.targetNamespaces.stringSelector or spec.targetNamespaces.list can be specified</li> false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespaces","title":"AccessManagement.spec.accessRules[index].targetNamespaces","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset.</p> Name Type Description Required list []string            List is the list of namespaces to select. Mutually exclusive with StringSelector and Selector. false selector object            Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List. false stringSelector string            StringSelector is a label query to select namespaces. Mutually exclusive with Selector and List. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespacesselector","title":"AccessManagement.spec.accessRules[index].targetNamespaces.selector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespacesselectormatchexpressionsindex","title":"AccessManagement.spec.accessRules[index].targetNamespaces.selector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#accessmanagementstatus","title":"AccessManagement.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagementStatus defines the observed state of AccessManagement</p> Name Type Description Required current []object            Current reflects the applied access rules configuration. false error string            Error is the error message occurred during the reconciliation (if any) false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false"},{"location":"reference/crds/#accessmanagementstatuscurrentindex","title":"AccessManagement.status.current[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessRule is the definition of the AccessManagement access rule. Each AccessRule enforces Templates and Credentials distribution to the TargetNamespaces</p> Name Type Description Required clusterTemplateChains []string            ClusterTemplateChains lists the names of ClusterTemplateChains whose ClusterTemplates will be distributed to all namespaces specified in TargetNamespaces. false credentials []string            Credentials is the list of Credential names that will be distributed to all the namespaces specified in TargetNamespaces. false serviceTemplateChains []string            ServiceTemplateChains lists the names of ServiceTemplateChains whose ServiceTemplates will be distributed to all namespaces specified in TargetNamespaces. false targetNamespaces object            TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset. Validations:<li>((has(self.stringSelector) ? 1 : 0) + (has(self.selector) ? 1 : 0) + (has(self.list) ? 1 : 0)) &lt;= 1: only one of spec.targetNamespaces.selector or spec.targetNamespaces.stringSelector or spec.targetNamespaces.list can be specified</li> false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespaces","title":"AccessManagement.status.current[index].targetNamespaces","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset.</p> Name Type Description Required list []string            List is the list of namespaces to select. Mutually exclusive with StringSelector and Selector. false selector object            Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List. false stringSelector string            StringSelector is a label query to select namespaces. Mutually exclusive with Selector and List. false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespacesselector","title":"AccessManagement.status.current[index].targetNamespaces.selector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespacesselectormatchexpressionsindex","title":"AccessManagement.status.current[index].targetNamespaces.selector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#clusterdeployment","title":"ClusterDeployment","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeployment is the Schema for the ClusterDeployments API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterDeployment true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterDeploymentSpec defines the desired state of ClusterDeployment false status object            ClusterDeploymentStatus defines the observed state of ClusterDeployment false"},{"location":"reference/crds/#clusterdeploymentspec","title":"ClusterDeployment.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeploymentSpec defines the desired state of ClusterDeployment</p> Name Type Description Required template string            Template is a reference to a Template object located in the same namespace. true config JSON            Config allows to provide parameters for template customization. If no Config provided, the field will be populated with the default values for the template and DryRun will be enabled. false credential string            Name reference to the related Credentials object. false dryRun boolean            DryRun specifies whether the template should be applied after validation or only validated. false ipamClaim object            IPAMClaim defines IP Address Management (IPAM) requirements for the cluster. It can either reference an existing IPAM claim or specify an inline claim. false propagateCredentials boolean            PropagateCredentials indicates whether credentials should be propagated for use by CCM (Cloud Controller Manager). Default: true false serviceSpec object            ServiceSpec is spec related to deployment of services. false"},{"location":"reference/crds/#clusterdeploymentspecipamclaim","title":"ClusterDeployment.spec.ipamClaim","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>IPAMClaim defines IP Address Management (IPAM) requirements for the cluster. It can either reference an existing IPAM claim or specify an inline claim.</p> Name Type Description Required ref string            ClusterIPAMClaimRef is the name of an existing ClusterIPAMClaim resource to use. false spec object            ClusterIPAMClaimSpec defines the inline IPAM claim specification if no reference is provided. This allows for dynamic IP address allocation during cluster provisioning. false"},{"location":"reference/crds/#clusterdeploymentspecipamclaimspec","title":"ClusterDeployment.spec.ipamClaim.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMClaimSpec defines the inline IPAM claim specification if no reference is provided. This allows for dynamic IP address allocation during cluster provisioning.</p> Name Type Description Required provider enum            Provider is the name of the provider that this claim will be consumed by Enum: in-cluster, ipam-infoblox true cluster string            Cluster is the reference to the [ClusterDeployment] that this claim is for Validations:<li>oldSelf == '' || self == oldSelf: Cluster reference is immutable once set</li> false clusterIPAMRef string            ClusterIPAMRef is the reference to the [ClusterIPAM] resource that this claim is for Validations:<li>oldSelf == '' || self == oldSelf: ClusterIPAM reference is immutable once set</li> false clusterNetwork object            ClusterNetwork defines the allocation for requisitioning ip addresses for use by the k8s cluster itself false externalNetwork object            ExternalNetwork defines the allocation for requisitioning ip addresses for use by services such as load balancers false nodeNetwork object            NodeNetwork defines the allocation requisitioning ip addresses for cluster nodes false"},{"location":"reference/crds/#clusterdeploymentspecipamclaimspecclusternetwork","title":"ClusterDeployment.spec.ipamClaim.spec.clusterNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterNetwork defines the allocation for requisitioning ip addresses for use by the k8s cluster itself</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusterdeploymentspecipamclaimspecexternalnetwork","title":"ClusterDeployment.spec.ipamClaim.spec.externalNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ExternalNetwork defines the allocation for requisitioning ip addresses for use by services such as load balancers</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusterdeploymentspecipamclaimspecnodenetwork","title":"ClusterDeployment.spec.ipamClaim.spec.nodeNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>NodeNetwork defines the allocation requisitioning ip addresses for cluster nodes</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusterdeploymentspecservicespec","title":"ClusterDeployment.spec.serviceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceSpec is spec related to deployment of services.</p> Name Type Description Required continueOnError boolean            ContinueOnError specifies if the services deployment should continue if an error occurs. Default: false false driftExclusions []object            DriftExclusions specifies specific configurations of resources to ignore for drift detection. false driftIgnore []object            DriftIgnore specifies resources to ignore for drift detection. false priority integer            Priority sets the priority for the services defined in this spec. Higher value means higher priority and lower means lower. In case of conflict with another object managing the service, the one with higher priority will get to deploy its services. Format: int32 Default: 100 Minimum: 1 Maximum: 2.147483646e+09 false reload boolean            Reload instances via rolling upgrade when a ConfigMap/Secret mounted as volume is modified. false services []object            Services is a list of services created via ServiceTemplates that could be installed on the target cluster. false stopOnConflict boolean            StopOnConflict specifies what to do in case of a conflict. E.g. If another object is already managing a service. By default the remaining services will be deployed even if conflict is detected. If set to true, the deployment will stop after encountering the first conflict. Default: false false syncMode enum            SyncMode specifies how services are synced in the target cluster. Enum: OneTime, Continuous, ContinuousWithDriftDetection, DryRun Default: Continuous false templateResourceRefs []object            TemplateResourceRefs is a list of resources to collect from the management cluster, the values from which can be used in templates. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftexclusionsindex","title":"ClusterDeployment.spec.serviceSpec.driftExclusions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required paths []string            Paths is a slice of JSON6902 paths to exclude from configuration drift evaluation. true target object            Target points to the resources that the paths refers to. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftexclusionsindextarget","title":"ClusterDeployment.spec.serviceSpec.driftExclusions[index].target","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Target points to the resources that the paths refers to.</p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftignoreindex","title":"ClusterDeployment.spec.serviceSpec.driftIgnore[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#clusterdeploymentspecservicespecservicesindex","title":"ClusterDeployment.spec.serviceSpec.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Service represents a Service to be deployed.</p> Name Type Description Required name string            Name is the chart release. true template string            Template is a reference to a Template object located in the same namespace. true disable boolean            Disable can be set to disable handling of this service. false namespace string            Namespace is the namespace the release will be installed in. It will default to Name if not provided. false templateChain string            TemplateChain defines the ServiceTemplateChain object that will be used to deploy the service along with desired ServiceTemplate version. false values string            Values is the helm values to be passed to the chart used by the template. The string type is used in order to allow for templating. false valuesFrom []object            ValuesFrom can reference a ConfigMap or Secret containing helm values. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecservicesindexvaluesfromindex","title":"ClusterDeployment.spec.serviceSpec.services[index].valuesFrom[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required kind enum            Kind of the resource. Supported kinds are: - ConfigMap/Secret Enum: ConfigMap, Secret true name string            Name of the referenced resource. Name can be expressed as a template and instantiate using any cluster field. true namespace string            Namespace of the referenced resource. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. For Profile namespace must be left empty. The Profile namespace will be used. Namespace can be expressed as a template and instantiate using any cluster field. false optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other ValueFroms. Default: false false"},{"location":"reference/crds/#clusterdeploymentspecservicespectemplateresourcerefsindex","title":"ClusterDeployment.spec.serviceSpec.templateResourceRefs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required identifier string            Identifier is how the resource will be referred to in the template true resource object            Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field. true optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other TemplateResourceRefs. Default: false false"},{"location":"reference/crds/#clusterdeploymentspecservicespectemplateresourcerefsindexresource","title":"ClusterDeployment.spec.serviceSpec.templateResourceRefs[index].resource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field.</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#clusterdeploymentstatus","title":"ClusterDeployment.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeploymentStatus defines the observed state of ClusterDeployment</p> Name Type Description Required availableUpgrades []string            AvailableUpgrades is the list of ClusterTemplate names to which this cluster can be upgraded. It can be an empty array, which means no upgrades are available. false conditions []object            Conditions contains details for the current state of the ClusterDeployment. false k8sVersion string            Currently compatible exact Kubernetes version of the cluster. Being set only if provided by the corresponding ClusterTemplate. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false services []object            Services contains details for the state of services. false servicesUpgradePaths []object            ServicesUpgradePaths contains details for the state of services upgrade paths. false"},{"location":"reference/crds/#clusterdeploymentstatusconditionsindex","title":"ClusterDeployment.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#clusterdeploymentstatusservicesindex","title":"ClusterDeployment.status.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceStatus contains details for the state of services.</p> Name Type Description Required clusterName string            ClusterName is the name of the associated cluster. true clusterNamespace string            ClusterNamespace is the namespace of the associated cluster. false conditions []object            Conditions contains details for the current state of managed services. false"},{"location":"reference/crds/#clusterdeploymentstatusservicesindexconditionsindex","title":"ClusterDeployment.status.services[index].conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#clusterdeploymentstatusservicesupgradepathsindex","title":"ClusterDeployment.status.servicesUpgradePaths[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceUpgradePaths contains details for the state of service upgrade paths.</p> Name Type Description Required name string            Name is the name of the service. true namespace string            Namespace is the namespace of the service. true template string            Template is the name of the current service template. true availableUpgrades []object            AvailableUpgrades contains details for the state of available upgrades. false"},{"location":"reference/crds/#clusterdeploymentstatusservicesupgradepathsindexavailableupgradesindex","title":"ClusterDeployment.status.servicesUpgradePaths[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>UpgradePath contains details for the state of service upgrade paths.</p> Name Type Description Required upgradePaths []string            Versions contains the list of versions that service can be upgraded to. false"},{"location":"reference/crds/#clusteripamclaim","title":"ClusterIPAMClaim","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMClaim is the Schema for the clusteripamclaims API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterIPAMClaim true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterIPAMClaimSpec defines the desired state of ClusterIPAMClaim false status object            ClusterIPAMClaimStatus defines the observed state of ClusterIPAMClaim false"},{"location":"reference/crds/#clusteripamclaimspec","title":"ClusterIPAMClaim.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMClaimSpec defines the desired state of ClusterIPAMClaim</p> Name Type Description Required provider enum            Provider is the name of the provider that this claim will be consumed by Enum: in-cluster, ipam-infoblox true cluster string            Cluster is the reference to the [ClusterDeployment] that this claim is for Validations:<li>oldSelf == '' || self == oldSelf: Cluster reference is immutable once set</li> false clusterIPAMRef string            ClusterIPAMRef is the reference to the [ClusterIPAM] resource that this claim is for Validations:<li>oldSelf == '' || self == oldSelf: ClusterIPAM reference is immutable once set</li> false clusterNetwork object            ClusterNetwork defines the allocation for requisitioning ip addresses for use by the k8s cluster itself false externalNetwork object            ExternalNetwork defines the allocation for requisitioning ip addresses for use by services such as load balancers false nodeNetwork object            NodeNetwork defines the allocation requisitioning ip addresses for cluster nodes false"},{"location":"reference/crds/#clusteripamclaimspecclusternetwork","title":"ClusterIPAMClaim.spec.clusterNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterNetwork defines the allocation for requisitioning ip addresses for use by the k8s cluster itself</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusteripamclaimspecexternalnetwork","title":"ClusterIPAMClaim.spec.externalNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ExternalNetwork defines the allocation for requisitioning ip addresses for use by services such as load balancers</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusteripamclaimspecnodenetwork","title":"ClusterIPAMClaim.spec.nodeNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>NodeNetwork defines the allocation requisitioning ip addresses for cluster nodes</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusteripamclaimstatus","title":"ClusterIPAMClaim.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMClaimStatus defines the observed state of ClusterIPAMClaim</p> Name Type Description Required bound boolean            Bound is a flag to indicate that the claim is bound because all ip addresses are allocated Default: false true conditions []object            Conditions contains details for the current state of the [ClusterIPAMClaim] false"},{"location":"reference/crds/#clusteripamclaimstatusconditionsindex","title":"ClusterIPAMClaim.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#clusteripam","title":"ClusterIPAM","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAM is the Schema for the clusteripams API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterIPAM true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterIPAMSpec defines the desired state of ClusterIPAM false status object            ClusterIPAMStatus defines the observed state of ClusterIPAM false"},{"location":"reference/crds/#clusteripamspec","title":"ClusterIPAM.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMSpec defines the desired state of ClusterIPAM</p> Name Type Description Required clusterIPAMClaimRef string            ClusterIPAMClaimRef is a reference to the [ClusterIPAMClaim] that this [ClusterIPAM] is bound to. Validations:<li>oldSelf == '' || self == oldSelf: Claim reference is immutable once set</li> false provider enum            The provider that this claim will be consumed by Enum: in-cluster, ipam-infoblox false"},{"location":"reference/crds/#clusteripamstatus","title":"ClusterIPAM.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMStatus defines the observed state of ClusterIPAM</p> Name Type Description Required phase enum            Phase is the current phase of the ClusterIPAM. Enum: Pending, Bound false providerData []object            ProviderData is the provider specific data produced for the ClusterIPAM. This field is represented as a list, because it will store multiple entries for different networks - nodes, cluster (pods, services), external - for the same provider. false"},{"location":"reference/crds/#clusteripamstatusproviderdataindex","title":"ClusterIPAM.status.providerData[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required config JSON            Data is the IPAM provider specific data false name string            Name of the IPAM provider data false ready boolean            Ready indicates that the IPAM provider data is ready false"},{"location":"reference/crds/#clustertemplatechain","title":"ClusterTemplateChain","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateChain is the Schema for the clustertemplatechains API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterTemplateChain true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            TemplateChainSpec defines the desired state of *TemplateChain Validations:<li>self == oldSelf: Spec is immutable</li> false status object            TemplateChainStatus defines the observed state of *TemplateChain false"},{"location":"reference/crds/#clustertemplatechainspec","title":"ClusterTemplateChain.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainSpec defines the desired state of *TemplateChain</p> Name Type Description Required supportedTemplates []object            SupportedTemplates is the list of supported Templates definitions and all available upgrade sequences for it. false"},{"location":"reference/crds/#clustertemplatechainspecsupportedtemplatesindex","title":"ClusterTemplateChain.spec.supportedTemplates[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SupportedTemplate is the supported Template definition and all available upgrade sequences for it</p> Name Type Description Required name string            Name is the name of the Template. true availableUpgrades []object            AvailableUpgrades is the list of available upgrades for the specified Template. false"},{"location":"reference/crds/#clustertemplatechainspecsupportedtemplatesindexavailableupgradesindex","title":"ClusterTemplateChain.spec.supportedTemplates[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AvailableUpgrade is the definition of the available upgrade for the Template</p> Name Type Description Required name string            Name is the name of the Template to which the upgrade is available. true"},{"location":"reference/crds/#clustertemplatechainstatus","title":"ClusterTemplateChain.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainStatus defines the observed state of *TemplateChain</p> Name Type Description Required valid boolean            Valid indicates whether the chain is valid and can be considered when calculating available upgrade paths. false validationError string            ValidationError provides information regarding issues encountered during templatechain validation. false"},{"location":"reference/crds/#clustertemplate","title":"ClusterTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplate is the Schema for the clustertemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterTemplateSpec defines the desired state of ClusterTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>!has(self.helm.chartSource): .spec.helm.chartSource is not supported for ClusterTemplates</li> false status object            ClusterTemplateStatus defines the observed state of ClusterTemplate false"},{"location":"reference/crds/#clustertemplatespec","title":"ClusterTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateSpec defines the desired state of ClusterTemplate</p> Name Type Description Required helm object            HelmSpec references a Helm chart representing the KCM template Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> true k8sVersion string            Kubernetes exact version in the SemVer format provided by this ClusterTemplate. false providerContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false providers []string            Providers represent required CAPI providers. Should be set if not present in the Helm chart metadata. false"},{"location":"reference/crds/#clustertemplatespechelm","title":"ClusterTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HelmSpec references a Helm chart representing the KCM template</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#clustertemplatespechelmchartref","title":"ClusterTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#clustertemplatespechelmchartsource","title":"ClusterTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#clustertemplatespechelmchartsourcelocalsourceref","title":"ClusterTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespec","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucket","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketcertsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketproxysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketsts","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketstscertsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketstssecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgit","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitincludeindex","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitincludeindexrepository","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitproxysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitverify","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitverifysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecoci","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocicertsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocilayerselector","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociproxysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocisecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverify","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverifysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartspec","title":"ClusterTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#clustertemplatespechelmchartspecsourceref","title":"ClusterTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#clustertemplatespechelmchartspecverify","title":"ClusterTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#clustertemplatespechelmchartspecverifymatchoidcidentityindex","title":"ClusterTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#clustertemplatespechelmchartspecverifysecretref","title":"ClusterTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatestatus","title":"ClusterTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateStatus defines the observed state of ClusterTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false k8sVersion string            Kubernetes exact version in the SemVer format provided by this ClusterTemplate. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false providerContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false providers []string            Providers represent required CAPI providers. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#clustertemplatestatuschartref","title":"ClusterTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#credential","title":"Credential","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Credential is the Schema for the credentials API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string Credential true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            CredentialSpec defines the desired state of Credential false status object            CredentialStatus defines the observed state of Credential false"},{"location":"reference/crds/#credentialspec","title":"Credential.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CredentialSpec defines the desired state of Credential</p> Name Type Description Required identityRef object            Reference to the Credential Identity true description string            Description of the Credential object false"},{"location":"reference/crds/#credentialspecidentityref","title":"Credential.spec.identityRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference to the Credential Identity</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#credentialstatus","title":"Credential.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CredentialStatus defines the observed state of Credential</p> Name Type Description Required ready boolean            Ready holds the readiness of Credentials. Default: false true conditions []object            Conditions contains details for the current state of the Credential. false"},{"location":"reference/crds/#credentialstatusconditionsindex","title":"Credential.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#managementbackup","title":"ManagementBackup","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackup is the Schema for the managementbackups API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ManagementBackup true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ManagementBackupSpec defines the desired state of ManagementBackup false status object            ManagementBackupStatus defines the observed state of ManagementBackup false"},{"location":"reference/crds/#managementbackupspec","title":"ManagementBackup.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackupSpec defines the desired state of ManagementBackup</p> Name Type Description Required performOnManagementUpgrade boolean            PerformOnManagementUpgrade indicates that a single [ManagementBackup] should be created and stored in the [ManagementBackup] storage location if not default before the [Management] release upgrade. false schedule string            Schedule is a Cron expression defining when to run the scheduled [ManagementBackup]. If not set, the object is considered to be run only once. false storageLocation string            StorageLocation is the name of a [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.StorageLocation] where the backup should be stored. false"},{"location":"reference/crds/#managementbackupstatus","title":"ManagementBackup.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackupStatus defines the observed state of ManagementBackup</p> Name Type Description Required error string            Error stores messages in case of failed backup creation. false lastBackup object            Most recently [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup] that has been created. false lastBackupName string            Name of most recently created [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup]. false lastBackupTime string            Time of the most recently created [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup]. Format: date-time false nextAttempt string            NextAttempt indicates the time when the next backup will be created. Always absent for a single [ManagementBackup]. Format: date-time false"},{"location":"reference/crds/#managementbackupstatuslastbackup","title":"ManagementBackup.status.lastBackup","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Most recently [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup] that has been created.</p> Name Type Description Required backupItemOperationsAttempted integer            BackupItemOperationsAttempted is the total number of attempted async BackupItemAction operations for this backup. false backupItemOperationsCompleted integer            BackupItemOperationsCompleted is the total number of successfully completed async BackupItemAction operations for this backup. false backupItemOperationsFailed integer            BackupItemOperationsFailed is the total number of async BackupItemAction operations for this backup which ended with an error. false completionTimestamp string            CompletionTimestamp records the time a backup was completed. Completion time is recorded even on failed backups. Completion time is recorded before uploading the backup object. The server's time is used for CompletionTimestamps Format: date-time false csiVolumeSnapshotsAttempted integer            CSIVolumeSnapshotsAttempted is the total number of attempted CSI VolumeSnapshots for this backup. false csiVolumeSnapshotsCompleted integer            CSIVolumeSnapshotsCompleted is the total number of successfully completed CSI VolumeSnapshots for this backup. false errors integer            Errors is a count of all error messages that were generated during execution of the backup.  The actual errors are in the backup's log file in object storage. false expiration string            Expiration is when this Backup is eligible for garbage-collection. Format: date-time false failureReason string            FailureReason is an error that caused the entire backup to fail. false formatVersion string            FormatVersion is the backup format version, including major, minor, and patch version. false hookStatus object            HookStatus contains information about the status of the hooks. false phase enum            Phase is the current state of the Backup. Enum: New, FailedValidation, InProgress, WaitingForPluginOperations, WaitingForPluginOperationsPartiallyFailed, Finalizing, FinalizingPartiallyFailed, Completed, PartiallyFailed, Failed, Deleting false progress object            Progress contains information about the backup's execution progress. Note that this information is best-effort only -- if Velero fails to update it during a backup for any reason, it may be inaccurate/stale. false startTimestamp string            StartTimestamp records the time a backup was started. Separate from CreationTimestamp, since that value changes on restores. The server's time is used for StartTimestamps Format: date-time false validationErrors []string            ValidationErrors is a slice of all validation errors (if applicable). false version integer            Version is the backup format major version. Deprecated: Please see FormatVersion false volumeSnapshotsAttempted integer            VolumeSnapshotsAttempted is the total number of attempted volume snapshots for this backup. false volumeSnapshotsCompleted integer            VolumeSnapshotsCompleted is the total number of successfully completed volume snapshots for this backup. false warnings integer            Warnings is a count of all warning messages that were generated during execution of the backup. The actual warnings are in the backup's log file in object storage. false"},{"location":"reference/crds/#managementbackupstatuslastbackuphookstatus","title":"ManagementBackup.status.lastBackup.hookStatus","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HookStatus contains information about the status of the hooks.</p> Name Type Description Required hooksAttempted integer            HooksAttempted is the total number of attempted hooks Specifically, HooksAttempted represents the number of hooks that failed to execute and the number of hooks that executed successfully. false hooksFailed integer            HooksFailed is the total number of hooks which ended with an error false"},{"location":"reference/crds/#managementbackupstatuslastbackupprogress","title":"ManagementBackup.status.lastBackup.progress","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Progress contains information about the backup's execution progress. Note that this information is best-effort only -- if Velero fails to update it during a backup for any reason, it may be inaccurate/stale.</p> Name Type Description Required itemsBackedUp integer            ItemsBackedUp is the number of items that have actually been written to the backup tarball so far. false totalItems integer            TotalItems is the total number of items to be backed up. This number may change throughout the execution of the backup due to plugins that return additional related items to back up, the velero.io/exclude-from-backup label, and various other filters that happen as items are processed. false"},{"location":"reference/crds/#management","title":"Management","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Management is the Schema for the managements API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string Management true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ManagementSpec defines the desired state of Management false status object            ManagementStatus defines the observed state of Management false"},{"location":"reference/crds/#managementspec","title":"Management.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementSpec defines the desired state of Management</p> Name Type Description Required release string            Release references the Release object. true core object            Core holds the core Management components that are mandatory. If not specified, will be populated with the default values. false providers []object            Providers is the list of supported CAPI providers. false"},{"location":"reference/crds/#managementspeccore","title":"Management.spec.core","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Core holds the core Management components that are mandatory. If not specified, will be populated with the default values.</p> Name Type Description Required capi object            CAPI represents the core Cluster API component and references the Cluster API template. false kcm object            KCM represents the core KCM component and references the KCM template. false"},{"location":"reference/crds/#managementspeccorecapi","title":"Management.spec.core.capi","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CAPI represents the core Cluster API component and references the Cluster API template.</p> Name Type Description Required config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementspeccorekcm","title":"Management.spec.core.kcm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>KCM represents the core KCM component and references the KCM template.</p> Name Type Description Required config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementspecprovidersindex","title":"Management.spec.providers[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required name string            Name of the provider. true config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementstatus","title":"Management.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementStatus defines the observed state of Management</p> Name Type Description Required availableProviders []string            AvailableProviders holds all available CAPI providers. false backupName string            BackupName is a name of the management cluster scheduled backup. false capiContracts map[string]map[string]string            For each CAPI provider name holds its compatibility [contract versions] in a key-value pairs, where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false components map[string]object            Components indicates the status of installed KCM components and CAPI providers. false conditions []object            Conditions represents the observations of a Management's current state. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false release string            Release indicates the current Release object. false"},{"location":"reference/crds/#managementstatuscomponentskey","title":"Management.status.components[key]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ComponentStatus is the status of Management component installation</p> Name Type Description Required error string            Error stores as error message in case of failed installation false exposedProviders []string            ExposedProviders is a list of CAPI providers this component exposes false success boolean            Success represents if a component installation was successful false template string            Template is the name of the Template associated with this component. false"},{"location":"reference/crds/#managementstatusconditionsindex","title":"Management.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservice","title":"MultiClusterService","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterService is the Schema for the multiclusterservices API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string MultiClusterService true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            MultiClusterServiceSpec defines the desired state of MultiClusterService false status object            MultiClusterServiceStatus defines the observed state of MultiClusterService. false"},{"location":"reference/crds/#multiclusterservicespec","title":"MultiClusterService.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterServiceSpec defines the desired state of MultiClusterService</p> Name Type Description Required clusterSelector object            ClusterSelector identifies target clusters to manage services on. false serviceSpec object            ServiceSpec is spec related to deployment of services. false"},{"location":"reference/crds/#multiclusterservicespecclusterselector","title":"MultiClusterService.spec.clusterSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterSelector identifies target clusters to manage services on.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#multiclusterservicespecclusterselectormatchexpressionsindex","title":"MultiClusterService.spec.clusterSelector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#multiclusterservicespecservicespec","title":"MultiClusterService.spec.serviceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceSpec is spec related to deployment of services.</p> Name Type Description Required continueOnError boolean            ContinueOnError specifies if the services deployment should continue if an error occurs. Default: false false driftExclusions []object            DriftExclusions specifies specific configurations of resources to ignore for drift detection. false driftIgnore []object            DriftIgnore specifies resources to ignore for drift detection. false priority integer            Priority sets the priority for the services defined in this spec. Higher value means higher priority and lower means lower. In case of conflict with another object managing the service, the one with higher priority will get to deploy its services. Format: int32 Default: 100 Minimum: 1 Maximum: 2.147483646e+09 false reload boolean            Reload instances via rolling upgrade when a ConfigMap/Secret mounted as volume is modified. false services []object            Services is a list of services created via ServiceTemplates that could be installed on the target cluster. false stopOnConflict boolean            StopOnConflict specifies what to do in case of a conflict. E.g. If another object is already managing a service. By default the remaining services will be deployed even if conflict is detected. If set to true, the deployment will stop after encountering the first conflict. Default: false false syncMode enum            SyncMode specifies how services are synced in the target cluster. Enum: OneTime, Continuous, ContinuousWithDriftDetection, DryRun Default: Continuous false templateResourceRefs []object            TemplateResourceRefs is a list of resources to collect from the management cluster, the values from which can be used in templates. false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftexclusionsindex","title":"MultiClusterService.spec.serviceSpec.driftExclusions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required paths []string            Paths is a slice of JSON6902 paths to exclude from configuration drift evaluation. true target object            Target points to the resources that the paths refers to. false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftexclusionsindextarget","title":"MultiClusterService.spec.serviceSpec.driftExclusions[index].target","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Target points to the resources that the paths refers to.</p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftignoreindex","title":"MultiClusterService.spec.serviceSpec.driftIgnore[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#multiclusterservicespecservicespecservicesindex","title":"MultiClusterService.spec.serviceSpec.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Service represents a Service to be deployed.</p> Name Type Description Required name string            Name is the chart release. true template string            Template is a reference to a Template object located in the same namespace. true disable boolean            Disable can be set to disable handling of this service. false namespace string            Namespace is the namespace the release will be installed in. It will default to Name if not provided. false templateChain string            TemplateChain defines the ServiceTemplateChain object that will be used to deploy the service along with desired ServiceTemplate version. false values string            Values is the helm values to be passed to the chart used by the template. The string type is used in order to allow for templating. false helmOptions object            HelmOptions are the options used during the helm install of the service.          false valuesFrom []object            ValuesFrom can reference a ConfigMap or Secret containing helm values. false"},{"location":"reference/crds/#multiclusterservicespecservicespecservicesindexvaluesfromindex","title":"MultiClusterService.spec.serviceSpec.services[index].valuesFrom[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required kind enum            Kind of the resource. Supported kinds are: - ConfigMap/Secret Enum: ConfigMap, Secret true name string            Name of the referenced resource. Name can be expressed as a template and instantiate using any cluster field. true namespace string            Namespace of the referenced resource. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. For Profile namespace must be left empty. The Profile namespace will be used. Namespace can be expressed as a template and instantiate using any cluster field. false optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other ValueFroms. Default: false false"},{"location":"reference/crds/#multiclusterservicespecservicespecservicesindexhelmoptions","title":"MultiClusterService.spec.serviceSpec.services[index].helmOptions","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required enableClientCache boolean            enableClientCache is a flag to enable Helm client cache. If it is not specified, it will be set to false.          false dependencyUpdate  boolean            dependencyUpdate is a flag to update dependencies if they are missing before installing the chart false wait boolean            Wait if set, will wait until all Pods, PVCs, Services, and minimum number of Pods of a Deployment, StatefulSet, or ReplicaSet are in a ready state before marking the release as successful. It will wait for as long as --timeout false waitForJobs boolean            waitForJobs if set and --wait enabled, will wait until all Jobs have been completed before marking the release as successful. It will wait for as long as --timeout          false createNamespace boolean            createNamespace if set will create a namespace for the helm release when installed          false skipCRDs boolean            skipCRDs controls whether CRDs should be installed during install/upgrade operation. By default, CRDs are installed if not already present.          false atomic boolean            atomic if set, the installation process deletes the installation/upgrades on failure. The --wait flag will be set automatically if --atomic is used          false disableHooks boolean            disableHooks prevent hooks from running during install/upgrade/uninstall          false disableOpenAPIValidation boolean            disableOpenAPIValidation if set, the installation process will not validate rendered templates against the Kubernetes OpenAPI Schema          false timeout string            time to wait for any individual Kubernetes operation (like Jobs for hooks) (default 5m0s)          false skipSchemaValidation boolean            skipSchemaValidation determines if JSON schema validation is disabled.          false replace boolean            replace if set indicates to replace an older release with this one          false labels map[string]string            labels that would be added to release metadata.          false description string            description is the description of an helm operation          false"},{"location":"reference/crds/#multiclusterservicespecservicespectemplateresourcerefsindex","title":"MultiClusterService.spec.serviceSpec.templateResourceRefs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required identifier string            Identifier is how the resource will be referred to in the template true resource object            Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field. true optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other TemplateResourceRefs. Default: false false"},{"location":"reference/crds/#multiclusterservicespecservicespectemplateresourcerefsindexresource","title":"MultiClusterService.spec.serviceSpec.templateResourceRefs[index].resource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field.</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#multiclusterservicestatus","title":"MultiClusterService.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterServiceStatus defines the observed state of MultiClusterService.</p> Name Type Description Required conditions []object            Conditions contains details for the current state of the MultiClusterService. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false services []object            Services contains details for the state of services. false servicesUpgradePaths []object            ServicesUpgradePaths contains details for the state of services upgrade paths. false"},{"location":"reference/crds/#multiclusterservicestatusconditionsindex","title":"MultiClusterService.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservicestatusservicesindex","title":"MultiClusterService.status.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceStatus contains details for the state of services.</p> Name Type Description Required clusterName string            ClusterName is the name of the associated cluster. true clusterNamespace string            ClusterNamespace is the namespace of the associated cluster. false conditions []object            Conditions contains details for the current state of managed services. false"},{"location":"reference/crds/#multiclusterservicestatusservicesindexconditionsindex","title":"MultiClusterService.status.services[index].conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservicestatusservicesupgradepathsindex","title":"MultiClusterService.status.servicesUpgradePaths[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceUpgradePaths contains details for the state of service upgrade paths.</p> Name Type Description Required name string            Name is the name of the service. true namespace string            Namespace is the namespace of the service. true template string            Template is the name of the current service template. true availableUpgrades []object            AvailableUpgrades contains details for the state of available upgrades. false"},{"location":"reference/crds/#multiclusterservicestatusservicesupgradepathsindexavailableupgradesindex","title":"MultiClusterService.status.servicesUpgradePaths[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>UpgradePath contains details for the state of service upgrade paths.</p> Name Type Description Required upgradePaths []string            Versions contains the list of versions that service can be upgraded to. false"},{"location":"reference/crds/#providerinterface","title":"ProviderInterface","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterface is the Schema for the ProviderInterface API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ProviderInterface true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ProviderInterfaceSpec defines the desired state of ProviderInterface false status object            ProviderInterfaceStatus defines the observed state of ProviderInterface false"},{"location":"reference/crds/#providerinterfacespec","title":"ProviderInterface.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterfaceSpec defines the desired state of ProviderInterface</p> Name Type Description Required clusterGVKs []object            ClusterGVKs defines the Group-Version-Kind resources this provider can manage false clusterIdentityKinds []string            ClusterIdentityKinds defines the Kind of identity objects supported by this provider false description string            Description provides a human-readable explanation of what this provider does false"},{"location":"reference/crds/#providerinterfacespecclustergvksindex","title":"ProviderInterface.spec.clusterGVKs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GroupVersionKind unambiguously identifies a kind. It doesn't anonymously include GroupVersion to avoid automatic coercion. It doesn't use a GroupVersion to avoid custom marshalling Note: mirror of https://github.com/kubernetes/apimachinery/blob/v0.32.3/pkg/runtime/schema/group_version.go#L140-L146</p> Name Type Description Required group string true kind string true version string true"},{"location":"reference/crds/#providerinterfacestatus","title":"ProviderInterface.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterfaceStatus defines the observed state of ProviderInterface</p> Name Type Description Required exposedProviders string            ExposedProviders contains the list of exposed provider false"},{"location":"reference/crds/#providertemplate","title":"ProviderTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplate is the Schema for the providertemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ProviderTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ProviderTemplateSpec defines the desired state of ProviderTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>!has(self.helm.chartSource): .spec.helm.chartSource is not supported for ProviderTemplates</li> false status object            ProviderTemplateStatus defines the observed state of ProviderTemplate false"},{"location":"reference/crds/#providertemplatespec","title":"ProviderTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplateSpec defines the desired state of ProviderTemplate</p> Name Type Description Required capiContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false helm object            HelmSpec references a Helm chart representing the KCM template Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> false providers []string            Providers represent exposed CAPI providers. Should be set if not present in the Helm chart metadata. false"},{"location":"reference/crds/#providertemplatespechelm","title":"ProviderTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HelmSpec references a Helm chart representing the KCM template</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#providertemplatespechelmchartref","title":"ProviderTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#providertemplatespechelmchartsource","title":"ProviderTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#providertemplatespechelmchartsourcelocalsourceref","title":"ProviderTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespec","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucket","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketcertsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketproxysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketsts","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketstscertsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketstssecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgit","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitincludeindex","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitincludeindexrepository","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitproxysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitverify","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitverifysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecoci","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocicertsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocilayerselector","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociproxysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocisecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverify","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverifysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartspec","title":"ProviderTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#providertemplatespechelmchartspecsourceref","title":"ProviderTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#providertemplatespechelmchartspecverify","title":"ProviderTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#providertemplatespechelmchartspecverifymatchoidcidentityindex","title":"ProviderTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#providertemplatespechelmchartspecverifysecretref","title":"ProviderTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatestatus","title":"ProviderTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplateStatus defines the observed state of ProviderTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true capiContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false providers []string            Providers represent exposed CAPI providers. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#providertemplatestatuschartref","title":"ProviderTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#release","title":"Release","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Release is the Schema for the releases API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string Release true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ReleaseSpec defines the desired state of Release false status object            ReleaseStatus defines the observed state of Release false"},{"location":"reference/crds/#releasespec","title":"Release.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ReleaseSpec defines the desired state of Release</p> Name Type Description Required capi object            CAPI references the Cluster API template. true kcm object            KCM references the KCM template. true version string            Version of the KCM Release in the semver format. true providers []object            Providers contains a list of Providers associated with the Release. false"},{"location":"reference/crds/#releasespeccapi","title":"Release.spec.capi","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CAPI references the Cluster API template.</p> Name Type Description Required template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasespeckcm","title":"Release.spec.kcm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>KCM references the KCM template.</p> Name Type Description Required template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasespecprovidersindex","title":"Release.spec.providers[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required name string            Name of the provider. true template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasestatus","title":"Release.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ReleaseStatus defines the observed state of Release</p> Name Type Description Required conditions []object            Conditions contains details for the current state of the Release false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false ready boolean            Ready indicates whether KCM is ready to be upgraded to this Release. false"},{"location":"reference/crds/#releasestatusconditionsindex","title":"Release.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#servicetemplatechain","title":"ServiceTemplateChain","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateChain is the Schema for the servicetemplatechains API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ServiceTemplateChain true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            TemplateChainSpec defines the desired state of *TemplateChain Validations:<li>self == oldSelf: Spec is immutable</li> false status object            TemplateChainStatus defines the observed state of *TemplateChain false"},{"location":"reference/crds/#servicetemplatechainspec","title":"ServiceTemplateChain.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainSpec defines the desired state of *TemplateChain</p> Name Type Description Required supportedTemplates []object            SupportedTemplates is the list of supported Templates definitions and all available upgrade sequences for it. false"},{"location":"reference/crds/#servicetemplatechainspecsupportedtemplatesindex","title":"ServiceTemplateChain.spec.supportedTemplates[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SupportedTemplate is the supported Template definition and all available upgrade sequences for it</p> Name Type Description Required name string            Name is the name of the Template. true availableUpgrades []object            AvailableUpgrades is the list of available upgrades for the specified Template. false"},{"location":"reference/crds/#servicetemplatechainspecsupportedtemplatesindexavailableupgradesindex","title":"ServiceTemplateChain.spec.supportedTemplates[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AvailableUpgrade is the definition of the available upgrade for the Template</p> Name Type Description Required name string            Name is the name of the Template to which the upgrade is available. true"},{"location":"reference/crds/#servicetemplatechainstatus","title":"ServiceTemplateChain.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainStatus defines the observed state of *TemplateChain</p> Name Type Description Required valid boolean            Valid indicates whether the chain is valid and can be considered when calculating available upgrade paths. false validationError string            ValidationError provides information regarding issues encountered during templatechain validation. false"},{"location":"reference/crds/#servicetemplate","title":"ServiceTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplate is the Schema for the servicetemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ServiceTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ServiceTemplateSpec defines the desired state of ServiceTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>has(self.helm) ? (!has(self.kustomize) &amp;&amp; !has(self.resources)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.kustomize) ? (!has(self.helm) &amp;&amp; !has(self.resources)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.resources) ? (!has(self.kustomize) &amp;&amp; !has(self.helm)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.helm) || has(self.kustomize) || has(self.resources): One of Helm, Kustomize, or Resources must be specified.</li> false status object            ServiceTemplateStatus defines the observed state of ServiceTemplate false"},{"location":"reference/crds/#servicetemplatespec","title":"ServiceTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateSpec defines the desired state of ServiceTemplate</p> Name Type Description Required helm object            Helm contains the Helm chart information for the template. Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> false k8sConstraint string            Constraint describing compatible K8S versions of the cluster set in the SemVer format. false kustomize object            Kustomize contains the Kustomize configuration for the template. Validations:<li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false resources object            Resources contains the resource configuration for the template. Validations:<li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false version string            Version is the semantic version of the application backed by template. false helmOptions object            HelmOptions are the options used during the helm install of the service.          false"},{"location":"reference/crds/#servicetemplatespechelm","title":"ServiceTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Helm contains the Helm chart information for the template.</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#servicetemplatespechelmchartref","title":"ServiceTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#servicetemplatespechelmchartsource","title":"ServiceTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespechelmchartsourcelocalsourceref","title":"ServiceTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespec","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucket","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketcertsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketproxysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketsts","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketstscertsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketstssecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgit","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitincludeindex","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitincludeindexrepository","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitproxysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitverify","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitverifysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecoci","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocicertsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocilayerselector","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociproxysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocisecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverify","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverifysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartspec","title":"ServiceTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#servicetemplatespechelmchartspecsourceref","title":"ServiceTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#servicetemplatespechelmchartspecverify","title":"ServiceTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespechelmchartspecverifymatchoidcidentityindex","title":"ServiceTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespechelmchartspecverifysecretref","title":"ServiceTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomize","title":"ServiceTemplate.spec.kustomize","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Kustomize contains the Kustomize configuration for the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespeckustomizelocalsourceref","title":"ServiceTemplate.spec.kustomize.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespec","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucket","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketcertsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketproxysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketsts","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketstscertsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketstssecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgit","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitincludeindex","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitincludeindexrepository","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitproxysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitverify","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitverifysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecoci","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocicertsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocilayerselector","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociproxysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocisecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverify","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverifymatchoidcidentityindex","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverifysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresources","title":"ServiceTemplate.spec.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resources contains the resource configuration for the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespecresourceslocalsourceref","title":"ServiceTemplate.spec.resources.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespec","title":"ServiceTemplate.spec.resources.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucket","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketcertsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketproxysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketsts","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketstscertsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketstssecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgit","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitincludeindex","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitincludeindexrepository","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitproxysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitverify","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitverifysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecoci","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocicertsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocilayerselector","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociproxysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocisecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverify","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverifymatchoidcidentityindex","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverifysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatestatus","title":"ServiceTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateStatus defines the observed state of ServiceTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false k8sConstraint string            Constraint describing compatible K8S versions of the cluster set in the SemVer format. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false sourceStatus object            SourceStatus reflects the status of the source. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#servicetemplatestatuschartref","title":"ServiceTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#servicetemplatestatussourcestatus","title":"ServiceTemplate.status.sourceStatus","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceStatus reflects the status of the source.</p> Name Type Description Required kind string            Kind is the kind of the remote source. true name string            Name is the name of the remote source. true namespace string            Namespace is the namespace of the remote source. true artifact object            Artifact is the artifact that was generated from the template source. false conditions []object            Conditions reflects the conditions of the remote source object. false observedGeneration integer            ObservedGeneration is the latest source generation observed by the controller. Format: int64 false"},{"location":"reference/crds/#servicetemplatestatussourcestatusartifact","title":"ServiceTemplate.status.sourceStatus.artifact","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Artifact is the artifact that was generated from the template source.</p> Name Type Description Required lastUpdateTime string            LastUpdateTime is the timestamp corresponding to the last update of the Artifact. Format: date-time true path string            Path is the relative file path of the Artifact. It can be used to locate the file in the root of the Artifact storage on the local file system of the controller managing the Source. true revision string            Revision is a human-readable identifier traceable in the origin source system. It can be a Git commit SHA, Git tag, a Helm chart version, etc. true url string            URL is the HTTP address of the Artifact as exposed by the controller managing the Source. It can be used to retrieve the Artifact for consumption, e.g. by another controller applying the Artifact contents. true digest string            Digest is the digest of the file in the form of ':'. false metadata map[string]string            Metadata holds upstream information such as OCI annotations. false size integer            Size is the number of bytes in the file. Format: int64 false"},{"location":"reference/crds/#servicetemplatestatussourcestatusconditionsindex","title":"ServiceTemplate.status.sourceStatus.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/template/","title":"The Templates system","text":"<p>By default, k0rdent delivers a set of default <code>ProviderTemplate</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects:</p> <ul> <li><code>ProviderTemplate</code>    The template containing the configuration of the provider (for example, k0smotron or AWS). These are cluster-scoped.</li> <li><code>ClusterTemplate</code>    The template containing the configuration of the cluster objects. These are namespace-scoped.</li> <li><code>ServiceTemplate</code>    The template containing the configuration of the service to be installed on the cluster deployment. These are namespace-scoped.</li> </ul> <p>All Templates are immutable, so if you want to change something about a cluster that has been deployed, you have to apply a whole new template. You can also build your own templates and use them for deployment along with the templates shipped with k0rdent.</p>"},{"location":"reference/template/#template-naming-convention","title":"Template Naming Convention","text":"<p>The templates can have any name. However, since they are immutable, we have adopted a naming convention that includes semantic versioning in the name, as in <code>template-&lt;major&gt;-&lt;minor&gt;-&lt;patch&gt;</code>. Below are some examples for each of the templates.</p> <p>Example</p> <p>An example of a <code>ProviderTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ProviderTemplate\nmetadata:\n  name: cluster-api-1-5-0\nspec:\n  helm:\n    chartSpec:\n      chart: cluster-api\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 1.5.0\nstatus:\n  capiContracts:\n    v1alpha3: \"\"\n    v1alpha4: \"\"\n    v1beta1: \"\"\n  chartRef:\n    kind: HelmChart\n    name: cluster-api-0-0-4\n    namespace: kcm-system\n  config:\n    config: {}\n    configSecret:\n      create: false\n      name: \"\"\n      namespace: \"\"\n  description: A Helm chart for Cluster API core components\n  observedGeneration: 1\n  valid: true\n</code></pre></p> <p>Example</p> <p>An example of a <code>ClusterTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nmetadata:\n  name: aws-standalone-cp-1-0-16\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: aws-standalone-cp\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 1.5.0\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: aws-standalone-cp-1-0-16\n    namespace: kcm-system\n  config:\n    bastion:\n      allowedCIDRBlocks: []\n      ami: \"\"\n      disableIngressRules: false\n      enabled: false\n      instanceType: t2.micro\n    clusterIdentity:\n      kind: AWSClusterStaticIdentity\n      name: \"\"\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    controlPlaneNumber: 3\n    extensions:\n      chartRepository: \"\"\n      imageRepository: \"\"\n    k0s:\n      version: v1.31.1+k0s.1\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    workersNumber: 2\n  description: 'An kcm template to deploy a k0s cluster on AWS with bootstrapped control\n    plane nodes. '\n  observedGeneration: 1\n  providerContracts:\n    bootstrap-k0smotron: v1beta1\n    control-plane-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n  providers:\n  - bootstrap-k0smotron\n  - control-plane-k0smotron\n  - infrastructure-aws\n  valid: true\n</code></pre></p> <p>Example</p> <p>An example of a <code>ServiceTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 3.2.6\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: kyverno-3-2-6\n    namespace: kcm-system\n  description: A Helm chart to refer the official kyverno helm chart\n  observedGeneration: 1\n  valid: true\n</code></pre></p>"},{"location":"reference/template/#template-life-cycle-management","title":"Template Life Cycle Management","text":"<p>Cluster and Service Templates can be delivered to target namespaces using the <code>AccessManagement</code>, <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects. The <code>AccessManagement</code> object contains the list of access rules to apply. Each access rule contains the namespaces' definition for delivering templates into and the template chains to deliver. Each <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> contains the supported templates and the upgrade sequences for them.</p> <p>The example of <code>ClusterTemplate</code> Management:</p> <ol> <li> <p>Create a <code>ClusterTemplateChain</code> object in the system namespace (defaults to <code>kcm-system</code>). Properly configure     the list of <code>.spec.supportedTemplates[].availableUpgrades</code> for the specified <code>ClusterTemplate</code> if you want to     allow upgrading. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-2\n      availableUpgrades:\n        - name: aws-standalone-cp-1-0-16\n    - name: aws-standalone-cp-1-0-16\n</code></pre> </li> <li> <p>Edit the <code>AccessManagement</code> object and configure the <code>.spec.accessRules</code>.     For example, to apply all templates and upgrade sequences defined in the <code>aws</code> <code>ClusterTemplateChain</code> to the     <code>default</code> namespace, add the following <code>accessRule</code>:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - default\n    clusterTemplateChains:\n      - aws\n</code></pre> </li> </ol> <p>The kcm controllers will deliver all the <code>ClusterTemplate</code> objects across the target namespaces. As a result, the following new objects should be created:</p> <ul> <li><code>ClusterTemplateChain</code> <code>default/aws</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-2</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-1-0-16</code> (available for the upgrade from <code>aws-standalone-cp-0-0-2</code>)</li> </ul> <p>Note</p> <ol> <li>The target <code>ClusterTemplate</code> defined as being available for the upgrade should reference the same helm chart name as the source <code>ClusterTemplate</code>. Otherwise, after the upgrade is triggered, the cluster will be removed and then recreated from scratch, even if the objects in the helm chart are the same.</li> <li>The target template should not affect immutable fields or any other incompatible internal objects upgrades, otherwise the upgrade will fail.</li> </ol> <ul> <li>Removing predefined templates</li> <li>Bring-your-own (BYO) templates</li> <li>Templates for Amazon Web Services</li> <li>Templates for Azure</li> <li>Templates for Google Cloud Platform</li> <li>Templates for OpenStack</li> <li>Templates for vSphere</li> <li>Templates for Remote deployments using SSH</li> </ul>"},{"location":"reference/template/template-aws/","title":"AWS template parameters","text":""},{"location":"reference/template/template-aws/#aws-ami","title":"AWS AMI","text":"<p>For AWS, by default k0rdent looks up the AMI ID automatically, using the latest Amazon Linux 2 image.</p> <p>You can override lookup parameters to search your desired image automatically or you can use a specific AMI ID directly. If both the AMI ID and lookup parameters are defined, the AMI ID will have higher precedence.</p>"},{"location":"reference/template/template-aws/#image-lookup","title":"Image lookup","text":"<p>To configure automatic AMI lookup, k0rdent uses three parameters:</p> <ul> <li> <p><code>.imageLookup.format</code> - Used directly as a value for the <code>name</code> filter (see the describe-images filters). This field supports substitutions for <code>{{.BaseOS}}</code> and <code>{{.K8sVersion}}</code> with the base OS and kubernetes version, respectively.</p> </li> <li> <p><code>.imageLookup.org</code> - The AWS org ID that will be used as value for the <code>owner-id</code> filter.</p> </li> <li> <p><code>.imageLookup.baseOS</code> - The string to be used as a value for the <code>{{.BaseOS}}</code> substitution in the <code>.imageLookup.format</code> string.</p> </li> </ul>"},{"location":"reference/template/template-aws/#ami-id","title":"AMI ID","text":"<p>The AMI ID can be directly used in the <code>.amiID</code> parameter.</p>"},{"location":"reference/template/template-aws/#capa-prebuilt-amis","title":"CAPA prebuilt AMIs","text":"<p>Use <code>clusterawsadm</code> to get available AMIs to create a <code>ClusterDeployment</code>:</p> <pre><code>clusterawsadm ami list\n</code></pre> <p>For details, see Pre-built Kubernetes AMIs.</p>"},{"location":"reference/template/template-aws/#ssh-access-to-cluster-nodes","title":"SSH access to cluster nodes","text":"<p>To access nodes using SSH you'll need to do two things:</p> <ul> <li>Add an SSH key added in the region where you want to deploy the cluster</li> <li>Enable Bastion host is enabled</li> </ul>"},{"location":"reference/template/template-aws/#ssh-keys","title":"SSH keys","text":"<p>Only one SSH key is supported and it should be added in AWS prior to creating the <code>ClusterDeployment</code> object. The name of the key should then be placed under the <code>.spec.config.sshKeyName</code>.</p> <p>The same SSH key will be used for all machines and a bastion host, or jump box. The bastion host is used as a single entry point that provides access to the rest of the cluster, enabling you to more tightly control access.</p> <p>To enable the bastion, set the <code>.spec.config.bastion.enabled</code> option in the <code>ClusterDeployment</code> object to <code>true</code>.</p> <p>You can get a full list of the bastion configuration options in the CAPA docs.</p> <p>The resulting <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-standalone-cp-1-0-16\n  credential: aws-cred\n  config:\n    clusterLabels: {}\n    sshKeyName: foobar\n    bastion:\n      enabled: true\n...\n</code></pre>"},{"location":"reference/template/template-aws/#configuring-some-of-k0s-k0smotron-parameters","title":"Configuring some of k0s, k0smotron parameters","text":"<ul> <li><code>k0s.arch</code> (string): Defines K0s Arch in its download URL. Available if global.k0sURL    is set. Possible values: <code>\"amd64\"</code> (default), <code>\"arm64\"</code>, <code>\"arm\"</code>.</li> <li><code>k0s.cpArgs</code> <sup>only standalone</sup> (array of strings): A list of extra arguments to be passed to k0s controller.    See: https://docs.k0sproject.io/stable/cli/k0s_controller.</li> <li><code>k0s.workerArgs</code> (array of strings): A list of extra arguments for configuring the k0s worker node. See: https://docs.k0sproject.io/stable/cli/k0s_worker.</li> <li><code>k0smotron.controllerPlaneFlags</code> <sup>only hosted</sup> (array of strings): The <code>controllerPlaneFlags</code> parameter enables you to configure additional flags for the k0s control plane    and to override existing flags. The default flags are kept unless they are explicitly overriden. Flags with arguments must be specified as a single    string, such as <code>--some-flag=argument</code>.</li> </ul>"},{"location":"reference/template/template-aws/#eks-templates","title":"EKS templates","text":"<p>Warning</p> <p> When deploying an EKS cluster please note that additional steps may be needed for proper VPC removal.</p> <p>Warning</p> <p> You may encounter an issue where EKS machines are not created due to the <code>ControlPlaneIsStable</code> preflight check failure during EKS cluster deployment. Please follow this instruction to apply the workaround.</p> <p>EKS templates use parameters similar to AWS and the resulting EKS <code>ClusterDeployment</code> looks something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-eks-1-0-3\n  credential: aws-cred\n  config:\n    clusterLabels: {}\n    sshKeyName: foobar\n    region: ${AWS_REGION}\n    workersNumber: 1\n...\n</code></pre>"},{"location":"reference/template/template-aws/#non-root-volumes","title":"Non-root volumes","text":"<p>To configure options for non-root storage volumes, set the corresponding fields in the <code>ClusterDeployment</code> responsible for these settings, which depend on the template in use:</p> Template Section(s) <code>aws-standalone-cp-1-0-16</code> <code>.spec.controlPlane.nonRootVolumes</code>,<code>.spec.worker.nonRootVolumes</code> <code>aws-hosted-cp-1-0-16</code> <code>.spec.nonRootVolumes</code> <code>aws-eks-1-0-3</code> <code>.spec.worker.nonRootVolumes</code> <p>The <code>nonRootVolumes</code> field is a list of Volumes.</p>"},{"location":"reference/template/template-aws/#setting-the-managementclustername-parameter-for-hosted-control-plane-clusters","title":"Setting the <code>managementClusterName</code> parameter for Hosted Control Plane clusters","text":"<p>The <code>managementClusterName</code> parameter is required for hosted control plane clusters. It should match the <code>&lt;cluster-id&gt;</code> from the <code>kubernetes.io/cluster/&lt;cluster-id&gt; = owned</code> tag on existing AWS network resources:</p>"},{"location":"reference/template/template-aws/#management-cluster-deployed-on-eks","title":"Management Cluster Deployed on EKS","text":"<ul> <li> <p>If deployed using k0rdent (<code>aws-eks</code> template), the EKS cluster name defaults to:   <code>&lt;namespace&gt;_&lt;clusterdeploymentname&gt;-cp</code>, or to the value of <code>eksClusterName</code> if explicitly set.</p> </li> <li> <p>If deployed separately (using <code>eksctl</code> or manually via the AWS console), the <code>&lt;cluster-id&gt;</code> is typically the EKS cluster name.</p> </li> </ul>"},{"location":"reference/template/template-aws/#management-cluster-is-ec2-based-self-managed","title":"Management Cluster is EC2-Based (Self-Managed)","text":"<ul> <li> <p>If deployed using k0rdent (<code>aws-standalone-cp</code> template), use the name of the ClusterDeployment object.</p> </li> <li> <p>If created with Cluster API Provider AWS, use the value of <code>spec.clusterName</code> from the Cluster object.</p> </li> </ul>"},{"location":"reference/template/template-azure/","title":"Azure machine parameters","text":""},{"location":"reference/template/template-azure/#ssh","title":"SSH","text":"<p>The SSH public key can be passed to <code>.spec.config.sshPublicKey</code> parameter (in the case of a hosted control plane) or <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (in the case of a standalone control plane) of the <code>ClusterDeployment</code> object.</p> <p>It should be encoded in base64 format.</p>"},{"location":"reference/template/template-azure/#vm-size","title":"VM size","text":"<p>Azure supports various VM sizes which can be retrieved with the following command:</p> <pre><code>az vm list-sizes --location \"&lt;location&gt;\" -o table\n</code></pre> <p>Then desired VM size could be passed to the:</p> <ul> <li><code>.spec.config.vmSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.vmSize</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.vmSize</code> - for worker nodes in the standalone deployment.</li> </ul> <p>Example: Standard_A4_v2</p>"},{"location":"reference/template/template-azure/#root-volume-size","title":"Root Volume size","text":"<p>Root volume size of the VM (in GB) can be changed through the following parameters:</p> <ul> <li><code>.spec.config.rootVolumeSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.rootVolumeSize</code> - for control plane nodes in the   standalone deployment.</li> <li><code>.spec.config.worker.rootVolumeSize</code> - for worker nodes in the standalone   deployment.</li> </ul> <p>Default value: 30</p> <p>Please note that this value can't be less than size of the root volume defined in your image.</p>"},{"location":"reference/template/template-azure/#vm-image","title":"VM Image","text":"<p>You can define the image which will be used for your machine using the following parameters:</p> <p>*<code>.spec.config.image</code> - for hosted CP deployment.</p> <ul> <li><code>.spec.config.controlPlane.image</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.image</code> - for worker nodes in the standalone deployment.</li> </ul> <p>There are multiple self-excluding ways to define the image source (for example Azure Compute Gallery, Azure Marketplace, and so on).</p> <p>If <code>image</code> is not specified, the default will be the <code>ubuntu-2204</code> image from the Azure Marketplace.</p> <p>Only one of the following image sources can be configured at a time:</p> <ul> <li>image.id</li> <li>image.computeGallery</li> <li>image.marketplace</li> </ul> <p>If multiple sources are specified, the precedence is as follows:</p> <ol> <li><code>id</code></li> <li><code>computeGallery</code></li> <li><code>marketplace</code></li> </ol> <p>Example:</p> <pre><code>spec:\n  config:\n    image:\n      computeGallery:\n        gallery: aksazurelinux-f7c7cda5-1c9a-4bdc-a222-9614c968580b\n        name: V2gen2arm64\n        version: 202501.05.0\n</code></pre> <p>Detailed information regarding image can be found in CAPZ documentation</p> <p>By default, the latest official CAPZ Ubuntu based image is used.</p>"},{"location":"reference/template/template-azure/#configuring-k0s-k0smotron-parameters","title":"Configuring k0s, k0smotron parameters","text":"<ul> <li><code>k0s.arch</code> (string): Defines the K0s Arch in its download URL. Available if global.k0sURL    is set. Possible values: <code>\"amd64\"</code> (default), <code>\"arm64\"</code>, <code>\"arm\"</code>.</li> <li><code>k0s.cpArgs</code> <sup>only standalone</sup> (array of strings): A list of extra arguments to be passed to k0s controller.    See: https://docs.k0sproject.io/stable/cli/k0s_controller.</li> <li><code>k0s.workerArgs</code> (array of strings): A list of extra arguments for configuring the k0s worker node. See: https://docs.k0sproject.io/stable/cli/k0s_worker.</li> <li><code>k0smotron.controllerPlaneFlags</code> <sup>only hosted</sup> (array of strings): The <code>controllerPlaneFlags</code> parameter enables you to configure additional flags for the k0s control plane    and to override existing flags. The default flags are kept unless they are explicitly overriden. Flags with arguments must be specified as a single    string, such as <code>--some-flag=argument</code>.</li> </ul>"},{"location":"reference/template/template-byo/","title":"Bring Your Own Templates","text":"<p>In addition to the templates that ship with k0rdent, it's possible to make your own. These might represent different types of clusters, or they may represent additional services to add to a cluster. Follow these steps:</p>"},{"location":"reference/template/template-byo/#create-a-source-object","title":"Create a Source Object","text":"<p>Info</p> <p> Skip this step if you're using an existing source.</p> <p><code>ClusterTemplate</code> and <code>ProviderTemplate</code> are based on a Helm chart. <code>ServiceTemplate</code>, apart from Helm chart, can be also based on kustomization or raw resources. Regardless of the type of the resources to be deployed using template, the corresponding source object should be created.</p> <p>The source can be one of the following types:</p> Template <code>HelmRepository</code> <code>Bucket</code> <code>OCIRepository</code> <code>ConfigMap</code> <code>Secret</code> <code>ClusterTemplate</code> V X X X X <code>ProviderTemplate</code> V X X X X <code>ServiceTemplate</code> V V V V V <ul> <li>HelmRepository</li> <li>GitRepository</li> <li>Bucket</li> </ul> <p>Info</p> <p> <code>ConfigMap</code> and <code>Secret</code> can only be used as a source of kustomization or raw resources for <code>ServiceTemplate</code>. To deploy kustomization using <code>ConfigMap</code> or <code>Secret</code> the kustomization folder must be archived in *.tar.gz and then <code>ConfigMap</code> or <code>Secret</code> must be created from resulting archive:</p> <pre><code>kubectl create configmap foo --from-file=kustomization.tar.gz\n</code></pre> <p>To deploy raw resources using <code>ConfigMap</code> or <code>Secret</code> source object must be created from raw resource files:</p> <pre><code>kubectl create configmap bar --from-file=namespace.yaml --from-file=deployment.yaml\n</code></pre> <p>Note that it's important to pay attention to where the source resides. Cluster-scoped <code>ProviderTemplate</code> objects must reside in the system namespace (<code>kcm-system</code> by default) but other template sources must reside in the same namespace as the templates that will come from them.</p> <p>For example, this YAML describes a custom <code>Source</code> object of <code>kind</code> <code>HelmRepository</code>:</p> <p>Note</p> <p> The custom <code>HelmRepository</code> must have the label <code>k0rdent.mirantis.com/managed: \"true\"</code>.</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: k0rdent-catalog\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/k0rdent/kcm/charts\n</code></pre>"},{"location":"reference/template/template-byo/#create-the-template","title":"Create the Template","text":"<p>Once you have the source, you can create the actual template. This template can be one of three types:</p> <ul> <li><code>ClusterTemplate</code></li> <li><code>ServiceTemplate</code></li> <li><code>ProviderTemplate</code></li> </ul> <p>For <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, configure the namespace where this template should reside (<code>metadata.namespace</code>).</p> <p>For defining the Helm chart, you have three choices. You can either:</p> <ol> <li>Specify the actual Helm chart definition in the <code>.spec.helm.chartSpec</code> field of the    HelmChartSpec kind,</li> <li>Reference an existing <code>HelmChart</code> object in <code>.spec.helm.chartRef</code></li> </ol> <p>Note</p> <p> Next option is supported for <code>ServiceTemplate</code> objects only</p> <ol> <li>Reference FluxCD source of the Helm chart in <code>.spec.helm.chartSource</code></li> </ol> <p>Note</p> <p> <code>spec.helm.chartSpec</code>, <code>spec.helm.chartRef</code>, <code>spec.helm.chartSource</code> are mutually exclusive.</p> <p>To automatically create the <code>HelmChart</code> for the <code>Template</code>, configure the following custom helm chart parameters under <code>spec.helm.chartSpec</code>:</p> Field Description <code>sourceRef</code>LocalHelmChartSourceReference Reference to the source object (for example, <code>HelmRepository</code>, <code>GitRepository</code>, or <code>Bucket</code>) in the same namespace as the Template. <code>chart</code>string The name of the Helm chart available in the source. <code>version</code>string Version is the chart version semver expression. Defaults to latest when omitted. <code>interval</code>Kubernetes meta/v1.Duration The frequency at which the <code>sourceRef</code> is checked for updates. Defaults to 10 minutes. <p>For the complete list of the <code>HelmChart</code> parameters, see: HelmChartSpec.</p> <p>The controller automatically creates the <code>HelmChart</code> object based on the chartSpec defined in <code>.spec.helm.chartSpec</code>.</p> <p>Note</p> <p> <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects should reside in the same namespace as the <code>ClusterDeployment</code> referencing them. The <code>ClusterDeployment</code> can't reference the Template from another namespace (the creation request will be declined by the admission webhook). All <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects shipped with KCM reside in the system namespace (defaults to <code>kcm-system</code>). To get the instructions on how to distribute Templates along multiple namespaces, read Template Life Cycle Management.</p>"},{"location":"reference/template/template-byo/#alternative-template-sources","title":"Alternative Template Sources","text":"<p>Aside from Helm charts, <code>ServiceTemplate</code> also supports alternative resource definitions using either Kustomize or raw resources.</p> <p>You can use one of the following fields in <code>.spec</code> (they are mutually exclusive):</p> <ul> <li><code>.spec.kustomize</code></li> <li><code>.spec.resources</code></li> <li><code>.spec.helm.chartSource</code></li> </ul> <p>Each of these fields accepts a <code>SourceSpec</code>, which defines the origin of the template content. Only one can be used at a time.</p> <p>A <code>SourceSpec</code> includes:</p> Field Description <code>deploymentType</code> Must be either <code>Local</code> or <code>Remote</code>. Defines whether resources will be deployed to management (local) or to managed (remote) cluster. Ignored if defined in <code>.spec.helm.chartSource</code>. <code>localSourceRef</code> Reference to a local source (e.g., <code>ConfigMap</code>, <code>Secret</code>, <code>GitRepository</code>, <code>Bucket</code>, <code>OCIRepository</code>). <code>ConfigMap</code> and <code>Secret</code> are not supported in <code>.spec.helm.chartSource</code>. <code>remoteSourceSpec</code> Configuration for a remote source. Includes support for <code>Git</code>, <code>Bucket</code>, or <code>OCI</code> repositories. <code>path</code> Path within the source object pointing to the manifests or kustomize config. Ignored when deploying raw resources using <code>ConfigMap</code> or <code>Secret</code> <p>Note</p> <p> Fields <code>.spec.*.remoteSourceSpec.git</code>, <code>.spec.*.remoteSource.Spec.bucket</code> and <code>.spec.*.remoteSourceSpec.oci</code> are mutually exclusive.</p> <p>Example using <code>.spec.kustomize</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n   name: example-kustomization\n   namespace: kcm-system\nspec:\n  kustomize:\n    deploymentType: Remote\n    remoteSourceSpec:\n      git:\n        url: https://github.com/example/repo\n        ref:\n          branch: main\n    path: ./overlays/dev\n</code></pre> <p>Example using <code>.spec.resources</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n   name: example-resources\n   namespace: kcm-system\nspec:\n  resources:\n    deploymentType: Local\n    localSourceRef:\n      kind: ConfigMap\n      name: my-configmap\n    path: ./manifests\n</code></pre> <p>All of the above follow the same mutual exclusivity and version constraint rules as Helm.</p>"},{"location":"reference/template/template-byo/#required-and-exposed-providers-definition","title":"Required and exposed providers definition","text":"<p>The <code>*Template</code> object must specify the list of Cluster API providers that are either required (for <code>ClusterTemplates</code> and <code>ServiceTemplates</code>) or exposed (for <code>ProviderTemplates</code>). These providers include <code>infrastructure</code>, <code>bootstrap</code>, and <code>control-plane</code>. This can be achieved in two ways:</p> <ol> <li>By listing the providers explicitly in the <code>spec.providers</code> field.</li> <li>Alternatively, by including specific annotations in the <code>Chart.yaml</code> of the referenced Helm chart. The annotations should list the providers as a <code>comma-separated</code> value.</li> </ol> <p>For example:</p> <p>In a <code>Template</code> spec:</p> <pre><code>spec:\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n</code></pre> <p>In a <code>Chart.yaml</code>:</p> <pre><code>annotations:\n    cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n</code></pre>"},{"location":"reference/template/template-byo/#compatibility-attributes","title":"Compatibility attributes","text":"<p>Each of the <code>*Template</code> resources has compatibility versions attributes to constraint the core <code>CAPI</code>, <code>CAPI</code> provider or Kubernetes versions. CAPI-related version constraints must be set in the <code>CAPI</code> contract format. Kubernetes version constraints must be set in the Semantic Version format. Each attribute can be set either via the corresponding <code>.spec</code> fields or via the annotations. Values set via the <code>.spec</code> have precedence over the values set via the annotations.</p> <p>Note</p> <p> All of the compatibility attributes are optional, and validation checks only take place if both of the corresponding type attributes (e.g. provider contract versions in both <code>ProviderTemplate</code> and <code>ClusterTemplate</code>) are set.</p> <ol> <li> <p>The <code>ProviderTemplate</code> resource has dedicated fields to set compatible <code>CAPI</code> contract versions along with CRDs contract versions supported by the provider. Given contract versions will then be set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the core <code>CAPI</code> contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core <code>CAPI</code>. For the core <code>CAPI</code> Template values should be empty.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ProviderTemplate\n# ...\nspec:\n  providers:\n  - infrastructure-aws\n  capiContracts:\n    # commented is the example exclusively for the core CAPI Template\n    # v1alpha3: \"\"\n    # v1alpha4: \"\"\n    # v1beta1: \"\"\n    v1alpha3: v1alpha3\n    v1alpha4: v1alpha4\n    v1beta1: v1beta1_v1beta2\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code> with the same logic as in the <code>.spec</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws\n  cluster.x-k8s.io/v1alpha3: v1alpha3\n  cluster.x-k8s.io/v1alpha4: v1alpha4\n  cluster.x-k8s.io/v1beta1: v1beta1_v1beta2\n</code></pre> </li> <li> <p>The <code>ClusterTemplate</code> resource has dedicated fields to set an exact compatible Kubernetes version in the Semantic Version format and required contract versions per each provider to match against the related <code>ProviderTemplate</code> objects. Given compatibility attributes will be then set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\n# ...\nspec:\n  k8sVersion: 1.30.0 # only exact semantic version is applicable\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n  providerContracts:\n    bootstrap-k0sproject-k0smotron: v1beta1 # only a single contract version is applicable\n    control-plane-k0sproject-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n</code></pre> <p>Example with the <code>.annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n  cluster.x-k8s.io/bootstrap-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/control-plane-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/infrastructure-aws: v1beta2\n  k0rdent.mirantis.com/k8s-version: 1.30.0\n</code></pre> </li> <li> <p>The <code>ServiceTemplate</code> resource has dedicated fields to set an compatibility constrained Kubernetes version to match against the related <code>ClusterTemplate</code> objects. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\n# ...\nspec:\n  k8sConstraint: \"^1.30.0\" # only semantic version constraints are applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>k0rdent.mirantis.com/k8s-version-constraint: ^1.30.0\n</code></pre> </li> </ol>"},{"location":"reference/template/template-byo/#compatibility-attributes-enforcement","title":"Compatibility attributes enforcement","text":"<p>The aforedescribed attributes are checked for compliance with the following rules:</p> <ul> <li>Both the exact and constraint version of the same type (for example <code>k8sVersion</code> and <code>k8sConstraint</code>) must be set, otherwise no check is performed;</li> <li>If a <code>ClusterTemplate</code> object's provider's contract version does not satisfy contract versions from the related <code>ProviderTemplate</code> object, updates to the <code>ClusterDeployment</code> object will be blocked;</li> <li>If a <code>ProviderTemplate</code> object's <code>CAPI</code> contract version (for example, in a <code>v1beta1: v1beta1_v1beta2</code> key-value pair, the key <code>v1beta1</code> is the core <code>CAPI</code> contract version) is not listed in the core <code>CAPI</code> <code>ProviderTemplate</code> object, the updates to the <code>Management</code> object will be blocked;</li> <li>If a <code>ClusterTemplate</code> object's exact kubernetes version does not satisfy the kubernetes version constraint from the related <code>ServiceTemplate</code> object, the updates to the <code>ClusterDeployment</code> object will be blocked.</li> </ul>"},{"location":"reference/template/template-gcp/","title":"GCP template parameters","text":""},{"location":"reference/template/template-gcp/#common-clusterdeployment-parameters","title":"Common ClusterDeployment parameters","text":"<ul> <li><code>controlPlaneNumber</code> (number): The number of the control plane nodes (or pods for the hosted cluster). Not available for the GKE cluster template.</li> <li><code>workersNumber</code> (number): The number of the worker nodes. For GKE clusters this parameter should be divisible by the number of zones in <code>machines.nodeLocations</code>. If <code>nodeLocations</code> is not specified, must be divisible by the number of zones in this region (default: <code>3</code>).</li> <li><code>clusterNetwork.apiServerPort</code> (number): The port the API Server should bind to. Not available for the GKE cluster template.</li> <li><code>clusterNetwork.pods.cidrBlocks</code> (array): A list of CIDR blocks from which Pod networks are allocated.</li> <li><code>clusterNetwork.services.cidrBlocks</code> (array): A list of CIDR blocks from which service VIPs are allocated.</li> <li><code>clusterLabels</code> (object): Labels to apply to the <code>Cluster</code> object.</li> <li><code>clusterAnnotations</code> (object): Annotations to apply to the <code>Cluster</code> object.</li> </ul>"},{"location":"reference/template/template-gcp/#gcp-cluster-parameters","title":"GCP Cluster parameters","text":"<p>The following parameters are available for standalone and hosted cluster templates:</p> <ul> <li><code>project</code> (string): The name of the project in which to deploy the cluster.</li> <li><code>region</code> (string): The GCP Region in which the cluster lives.</li> <li><code>network.name</code> (string): The name of an existing GCP network or a new network to be created by Cluster API Provider GCP.</li> <li><code>network.mtu</code> (number): Maximum Transmission Unit in bytes.</li> <li><code>additionalLabels</code> (object): Additional set of labels to add to all the GCP resources.</li> </ul>"},{"location":"reference/template/template-gcp/#gcp-machines-parameters","title":"GCP Machines parameters","text":"<p>The following parameters are available for <code>controlPlane</code> (for standalone cluster template) and <code>worker</code> (for standalone and hosted cluster templates) machines (for example, <code>controlPlane.instanceType</code>):</p> <ul> <li><code>instanceType</code> (string): The type of instance to create.</li> <li><code>subnet</code> (string): A reference to the subnetwork to use for this instance.</li> <li><code>providerID</code> (string): The unique identifier as specified by the cloud provider.</li> <li><code>imageFamily</code> (string): The full reference to a valid image family to be used for this machine.</li> <li><code>image</code> (string): The full reference to a valid image to be used for this machine. Takes precedence over imageFamily.</li> <li><code>additionalLabels</code> (object): Additional set of labels to add to an instance.</li> <li><code>publicIP</code> (boolean): Specifies whether the instance should get a public IP.</li> <li><code>additionalNetworkTags</code> (array): A list of network tags that should be applied to the instance.</li> <li><code>rootDeviceSize</code> (number): The size of the root volume in GB.</li> <li><code>rootDeviceType</code> (string): The type of the root volume. One of: <code>pd-standard</code>,<code>pd-ssd</code>,<code>pd-balanced</code>,<code>hyperdisk-balanced</code>.</li> <li><code>serviceAccount.email</code> (string): Email address of the service account.</li> <li><code>serviceAccount.scopes</code> (array): The list of scopes to be made available for this service account.</li> <li><code>ipForwarding</code> (string): Allows this instance to send and receive packets with non-matching destination or source IPs. One of: <code>Enabled</code>,<code>Disabled</code>.</li> </ul>"},{"location":"reference/template/template-gcp/#k0s-parameters","title":"K0s Parameters","text":"<ul> <li><code>k0s.version</code> (string): K0s version.</li> <li><code>k0s.api.extraArgs</code> (object): Map of key-values (strings) for any extra arguments to pass down to the Kubernetes API server process.</li> <li><code>k0s.arch</code> (string): Defines the K0s Arch in its download URL. Available if global.k0sURL    is set. Possible values: <code>\"amd64\"</code> (default), <code>\"arm64\"</code>, <code>\"arm\"</code>.</li> <li><code>k0s.cpArgs</code> <sup>only standalone</sup> (array of strings): A list of extra arguments to be passed to k0s controller.    See: https://docs.k0sproject.io/stable/cli/k0s_controller.</li> <li><code>k0s.workerArgs</code> (array of strings): A list of extra arguments for configuring the k0s worker node. See: https://docs.k0sproject.io/stable/cli/k0s_worker.</li> </ul>"},{"location":"reference/template/template-gcp/#k0smotron-parameters","title":"K0smotron Parameters","text":"<p>Available for the hosted cluster template only.</p> <ul> <li><code>k0smotron.service.type</code> (string): An ingress method for a service. One of: <code>ClusterIP</code>, <code>NodePort</code>, <code>LoadBalancer</code>. Defaults to: <code>LoadBalancer</code>.</li> <li><code>k0smotron.service.apiPort</code> (number): The Kubernetes API port. If empty, K0smotron will pick it automatically.</li> <li><code>k0smotron.service.konnectivityPort</code> (number): The Konnectivity port. If empty, K0smotron will pick it automatically.</li> <li><code>k0smotron.controllerPlaneFlags</code> (array of strings): The <code>controllerPlaneFlags</code> parameter enables you to configure additional flags for the k0s control plane    and to override existing flags. The default flags are kept unless they are explicitly overriden. Flags with arguments must be specified as a single    string, such as <code>--some-flag=argument</code>.</li> </ul>"},{"location":"reference/template/template-gcp/#extensions-parameters","title":"Extensions parameters","text":"<ul> <li><code>extensions.chartRepository</code> (string): Custom Helm repository.</li> <li><code>extensions.imageRepository</code> (string): Custom images\u2019 repository.</li> </ul>"},{"location":"reference/template/template-gcp/#gke-template-parameters","title":"GKE template parameters","text":""},{"location":"reference/template/template-gcp/#gke-cluster-parameters","title":"GKE Cluster Parameters","text":"<ul> <li><code>gkeClusterName</code> (string): The name of the GKE cluster. If unspecified, a default name is created based on the namespace and managed control plane name.</li> <li><code>project</code> (string): The name of the GCP project where the cluster is deployed.</li> <li><code>enableAutopilot</code> (boolean): Indicates whether to enable Autopilot for this GKE cluster.</li> <li><code>releaseChannel</code> (string): The release channel of the GKE cluster.</li> <li><code>controlPlaneVersion</code> (string): Deprecated. Use the <code>version</code> field instead. The control plane version of the GKE cluster. If unspecified, the default version supported by GKE is used.</li> <li><code>masterAuthorizedNetworksConfig</code> (object): Configuration options for the master authorized networks feature. If unspecified, the feature is disabled.</li> <li><code>region</code> (string): The GCP region where the cluster is deployed.</li> <li><code>location</code> (string): The location where the GKE cluster is created. If unspecified, the cluster is regional; otherwise, specifying a location creates a zonal cluster.</li> <li><code>network.name</code> (string): The GCP network name.</li> <li><code>network.mtu</code> (number): Maximum Transmission Unit (MTU) in bytes.</li> <li><code>version</code> (string): Represents the control plane version of the GKE cluster.</li> </ul>"},{"location":"reference/template/template-gcp/#gke-managed-machines-parameters","title":"GKE Managed Machines Parameters","text":"<ul> <li><code>machines.nodePoolName</code> (string): The name of the GKE node pool. If unspecified, a default name is created based on the namespace and managed machine pool name.</li> <li><code>machines.machineType</code> (string): The name of a Google Compute Engine machine type. Defaults to <code>e2-medium</code>.</li> <li><code>machines.diskSizeGB</code> (number): The size of the disk attached to each node (in GB). The smallest allowed disk size is 10GB. If unspecified, the default disk size is 100GB.</li> <li><code>machines.localSsdCount</code> (number,null): The number of local SSD disks attached to the node.</li> <li><code>machines.scaling.enableAutoscaling</code> (boolean): Indicates if autoscaling is enabled for this node pool. Defaults to true.</li> <li><code>machines.scaling.minCount</code> (number,null): The minimum number of nodes in the node pool.</li> <li><code>machines.scaling.maxCount</code> (number,null): The maximum number of nodes in the node pool.</li> <li><code>machines.scaling.locationPolicy</code> (string): Location policy used when scaling up a node pool.</li> <li><code>machines.nodeLocations</code> (array): The list of zones where the node pool\u2019s nodes are located.</li> <li><code>machines.imageType</code> (string): The image type used for this node pool.</li> <li><code>machines.instanceType</code> (string): The Compute Engine machine type.</li> <li><code>machines.diskType</code> (string): The type of disk attached to each node.</li> <li><code>machines.maxPodsPerNode</code> (number,null): The maximum number of pods allowed per node.</li> <li><code>machines.kubernetesLabels</code> (object): Labels applied to the nodes of the node pool.</li> <li><code>machines.kubernetesTaints</code> (array): Taints applied to the nodes of the node pool.</li> <li><code>machines.additionalLabels</code> (object): Additional labels added to GCP resources managed by the provider.</li> <li><code>machines.management.autoUpgrade</code> (boolean): Specifies if node auto-upgrade is enabled. Defaul</li> <li><code>machines.management.autoRepair</code> (boolean): Specifies if node auto-repair is enabled.</li> </ul> <p>The resulting GCP standalone <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: gcp-standalone-cp-1-0-15\n  credential: gcp-cred\n  config:\n    clusterLabels:\n      foo: bar\n    project: dev\n    region: us-east4\n    network:\n      name: default\n    controlPlane:\n      instanceType: n1-standard-2\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n      rootDeviceSize: 100\n      ipForwarding: Enabled\n    controlPlaneNumber: 1\n    worker:\n      instanceType: n1-standard-2\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n      rootDeviceSize: 50\n      ipForwarding: Enabled\n    workersNumber: 1\n...\n</code></pre> <p>The resulting GCP hosted <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: gcp-hosted-cp-1-0-16\n  credential: gcp-cred\n  config:\n    clusterAnnotations:\n      foo: bar\n    project: dev\n    region: us-east4\n    network:\n      name: default\n    controlPlaneNumber: 3\n    worker:\n      instanceType: n1-standard-2\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n      rootDeviceSize: 50\n      ipForwarding: Enabled\n    workersNumber: 2\n...\n</code></pre> <p>The resulting GCP GKE <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: gcp-gke-1-0-6\n  credential: gcp-cred\n  propagateCredentials: false\n  config:\n    clusterLabels:\n      env: dev\n    gkeClusterName: \"dev-cluster-1\"\n    workersNumber: 3 # Should be divisible by the number of zones in `machines.nodeLocations`. If `machines.nodeLocations` is not specified, must be divisible by the number of zones in this region (default: 3)\n    project: dev\n    region: us-east4\n    network:\n      name: default\n    machines:\n      diskSizeGB: 120\n      scaling:\n        enableAutoscaling: true\n        minCount: 5\n        maxCount: 25\n      maxPodsPerNode: 250\n...\n</code></pre>"},{"location":"reference/template/template-openstack/","title":"OpenStack Machine parameters","text":""},{"location":"reference/template/template-openstack/#clusterdeployment-parameters","title":"ClusterDeployment Parameters","text":"<p>To deploy an OpenStack cluster, the following are the primary parameters in the <code>ClusterDeployment</code> resource:</p> Parameter Example Description <code>.spec.credential</code> <code>openstack-cluster-identity-cred</code> Reference to the <code>Credential</code> object. <code>.spec.template</code> <code>openstack-standalone-cp-1-0-17</code> Reference to the <code>ClusterTemplate</code>. <code>.spec.config.authURL</code> <code>https://keystone.yourorg.net/</code> Keystone authentication endpoint for OpenStack. <code>.spec.config.controlPlaneNumber</code> <code>3</code> Number of control plane nodes. <code>.spec.config.workersNumber</code> <code>2</code> Number of worker nodes. <code>.spec.config.clusterLabels</code> <code>k0rdent: demo</code> Labels to apply to the cluster. Used by MultiClusterService. <code>.spec.config.ccmRegional</code> <code>true</code> Enables the OpenStack CCM OS_CCM_REGIONAL envvar feature and allows OpenStack CCM to define the region in nodes <p>Note</p> <p> When deploying OpenStack clusters with <code>openstack-standalone-cp</code> template version 1-0-12 or newer or <code>openstack-hosted-cp</code> template version 1-0-2 or newer, the <code>identityRef.name</code> parameter is ignored and can be omitted. For older template versions, this parameter is required and must match the name of the <code>Secret</code> containing the <code>clouds.yaml</code> configuration.</p>"},{"location":"reference/template/template-openstack/#ssh-configuration","title":"SSH Configuration","text":"<p>To access deployed machines over ssh requires two things:</p> <ul> <li><code>sshKeyName</code> - the reference name for an existing SSH key configured in OpenStack.</li> <li><code>bastion</code> - bastion host being enabled and its flavor and image specified.</li> </ul>"},{"location":"reference/template/template-openstack/#ssh-keys","title":"SSH keys","text":"<p>Specify the SSH public key using the <code>.spec.config.controlPlane.sshKeyName</code> and <code>.spec.config.worker.sshKeyName</code> parameters (for the standalone control plane) or <code>spec.config.sshKeyName</code> parameter (for the hosted control plane).</p>"},{"location":"reference/template/template-openstack/#bastion","title":"Bastion","text":"<p>Specify <code>.spec.config.bastion.enabled</code> to enable it as well as provide <code>sshKeyName</code>, <code>flavor</code> and <code>image</code> in <code>.spec.config.bastion.spec</code>, similarly to workers and control plane.</p> <p>Example `ClusterDeployment with enabled bastion can be found below.</p>"},{"location":"reference/template/template-openstack/#machine-configuration","title":"Machine Configuration","text":"<p>Configurations for control plane and worker nodes are specified separately under <code>.spec.config.controlPlane</code> and <code>.spec.config.worker</code> for standalone control plane or under <code>spec.config</code> for hosted control plane.</p> Parameter Example Description <code>flavor</code> <code>m1.medium</code> OpenStack flavor for the instance. <code>image.filter.name</code> <code>ubuntu-22.04-x86_64</code> Name of the image. <code>sshKeyName</code> <code>ramesses-pk</code> Reference name for an existing SSH key. <code>securityGroups[].filter.name</code> <code>default</code> Security group for the instance. <p>Note</p> <p> Make sure <code>.spec.credential</code> references the <code>Credential</code> object. The recommended minimum vCPU value for the control plane flavor is 2, while for the worker node flavor, it is 1. For detailed information, refer to the machine-flavor CAPI docs.</p>"},{"location":"reference/template/template-openstack/#external-network-configuration","title":"External Network Configuration","text":"<p>If your OpenStack cloud contains more than one network marked as external it is necessary to provide which one clusterapi should use when creating a cluster. You do this by providing <code>.spec.config.externlNetwork.filter.name</code> value with the name of your external network.</p>"},{"location":"reference/template/template-openstack/#load-balancer-configuration","title":"Load Balancer Configuration","text":"<p>If your user doesn't have access to or your cloud doesn't utilize octavia load balancer it is possible to disable usage of it by specifying <code>.spec.config.apiServerLoadBalancer.enabled</code> as <code>false</code> (for standalone control plane only).</p> <p>Warning</p> <p>Disabling loadbalancer blocks usage of <code>LoadBalancer</code> type services in cluster until one is manually installed.</p>"},{"location":"reference/template/template-openstack/#configuring-some-of-k0s-parameters","title":"Configuring some of k0s parameters","text":"<ul> <li><code>k0s.arch</code> (string): Defines K0s Arch in its download URL. Available if global.k0sURL    is set. Possible values: <code>\"amd64\"</code> (default), <code>\"arm64\"</code>, <code>\"arm\"</code>.</li> <li><code>k0s.cpArgs</code> (array of strings): A list of extra arguments to be passed to k0s controller. For standalone control plane only.    See: https://docs.k0sproject.io/stable/cli/k0s_controller.</li> <li><code>k0s.workerArgs</code> (array of strings): A list of extra arguments for configuring the k0s worker node. See: https://docs.k0sproject.io/stable/cli/k0s_worker.</li> </ul>"},{"location":"reference/template/template-openstack/#hosted-clusterdeployment-parameters","title":"Hosted ClusterDeployment parameters","text":""},{"location":"reference/template/template-openstack/#network-configuration","title":"Network configuration","text":"<p>The following parameters under <code>spec.config</code> are specific to the hosted cluster deployment:</p> <ul> <li> <p><code>network.filter</code> (object): Specifies a query to select an OpenStack network. The value of NetworkFilter type. Required.</p> <p>Example:</p> <pre><code>  network:\n    filter:\n      name: my-network-name\n</code></pre> </li> <li> <p><code>subnets[].filter</code> (array): Specifies a query to select an OpenStack subnet. The value of SubnetFilter type. Required.</p> <p>Example:</p> <pre><code>  subnets:\n  - filter:\n      name: my-subnet-name\n</code></pre> </li> <li> <p><code>router.filter</code> (object): Specifies a query to select an OpenStack router. The value of RouterFilter type. Required.</p> <p>Example:</p> <pre><code>  router:\n    filter:\n      name: my-router-name\n</code></pre> </li> <li> <p><code>ports[].network.filter</code> (object): Specifies a query to select an OpenStack network. The value of NetworkFilter type. Required.</p> <p>Example:</p> <pre><code>  ports:\n  - network:\n      filter:\n        name: my-network-name\n</code></pre> </li> <li> <p><code>managedSecurityGroups</code> (object): Defines the desired state of security groups and rules for the cluster. When not   defined, security groups will not be created. The value of ManagedSecurityGroups type.</p> <p>Example: <pre><code>  managedSecurityGroups:\n    allowAllInClusterTraffic: false\n</code></pre></p> </li> </ul>"},{"location":"reference/template/template-openstack/#k0smotron-parameters","title":"K0smotron Parameters","text":"<p>Available for the hosted cluster template only.</p> <ul> <li><code>k0smotron.service.type</code> (string): An ingress method for a service. One of: <code>ClusterIP</code>, <code>NodePort</code>, <code>LoadBalancer</code>. Defaults to: <code>LoadBalancer</code>.</li> <li><code>k0smotron.service.apiPort</code> (number): The Kubernetes API port. If empty, K0smotron will pick it automatically.</li> <li><code>k0smotron.service.konnectivityPort</code> (number): The Konnectivity port. If empty, K0smotron will pick it automatically.</li> <li><code>k0smotron.controllerPlaneFlags</code> (array of strings): The <code>controllerPlaneFlags</code> parameter enables you to configure additional flags for the k0s control plane   and to override existing flags. The default flags are kept unless they are explicitly overriden. Flags with arguments must be specified as a single   string, such as <code>--some-flag=argument</code>.</li> </ul>"},{"location":"reference/template/template-openstack/#example-clusterdeployment","title":"Example ClusterDeployment","text":"<p>The standalone <code>ClusterDeployment</code> may look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-1-0-17\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels:\n      k0rdent: demo\n    controlPlaneNumber: 1\n    workersNumber: 1\n    bastion:\n      enabled: true\n      spec:\n        sshKeyName: my-public-key\n        flavor: m1.small\n        image:\n          filter:\n            name: ubuntu-22.04-x86_64\n    controlPlane:\n      sshKeyName: bastion-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      sshKeyName: bastion-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    externalNetwork:\n      filter:\n        name: \"public\"\n    authURL: https://my-keystone-openstack-url.com\n    identityRef:\n      name: openstack-cloud-config\n      cloudName: openstack\n      region: RegionOne\n</code></pre> <p>The hosted <code>ClusterDeployment</code> may look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-hosted-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-hosted-cp-1-0-7\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels:\n      k0rdent: demo\n    workersNumber: 3\n    bastion:\n      enabled: true\n      spec:\n        sshKeyName: my-ssh-key\n        flavor: m1.medium\n        image:\n          filter:\n            name: ubuntu-22.04-x86_64\n    sshKeyName: my-ssh-key\n    flavor: m1.medium\n    image:\n      filter:\n        name: ubuntu-22.04-x86_64\n    externalNetwork:\n      filter:\n        name: \"public\"\n    identityRef:\n      name: \"openstack-cloud-config\"\n      cloudName: \"openstack\"\n      region: RegionOne\n\n    managedSecurityGroups:\n      allowAllInClusterTraffic: false\n    network:\n      filter:\n        name: my-network-name\n    router:\n      filter:\n        name: my-router-name\n    subnets:\n    - filter:\n        name: my-subnet-name\n    ports:\n    - network:\n        filter:\n          name: my-network-name\n    securityGroups:\n    - filter:\n        name: my-security-group-name\n</code></pre>"},{"location":"reference/template/template-predefined/","title":"Removing predefined templates","text":""},{"location":"reference/template/template-predefined/#remove-templates-shipped-with-k0rdent","title":"Remove Templates shipped with k0rdent","text":"<p>If you need to limit the templates that exist in your k0rdent installation, follow the instructions below:</p> <ol> <li> <p>Get the list of <code>ProviderTemplate</code>, <code>ClusterTemplate</code> or <code>ServiceTemplate</code> objects shipped with k0rdent. For example, for <code>ClusterTemplate</code> objects, run:</p> <pre><code>kubectl get clustertemplates -n kcm-system -l helm.toolkit.fluxcd.io/name=kcm-templates\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-5-0           true\nkcm-system   aws-eks-1-0-3                   true\nkcm-system   aws-hosted-cp-1-0-16             true\nkcm-system   aws-standalone-cp-1-0-16         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-19           true\nkcm-system   azure-standalone-cp-1-0-17       true\nkcm-system   openstack-standalone-cp-1-0-17   true\nkcm-system   vsphere-hosted-cp-1-0-15         true\nkcm-system   vsphere-standalone-cp-1-0-15     true\n</code></pre> </li> <li> <p>Remove the template from the list using <code>kubectl delete</code>, as in:</p> <pre><code>kubectl delete clustertemplate -n kcm-system &lt;template-name&gt;\n</code></pre> </li> </ol>"},{"location":"reference/template/template-remote/","title":"Remote machine parameters","text":""},{"location":"reference/template/template-remote/#clusterdeployment-parameters","title":"ClusterDeployment Parameters","text":"<p>To deploy a cluster using k0rdent on any SSH accessible Linux host, configure the following parameters:</p>"},{"location":"reference/template/template-remote/#cluster-parameters","title":"Cluster parameters","text":"<ul> <li><code>controlPlaneNumber</code> (number): Specifies how many control plane pods you want to deploy. For example, 3 means three control plane pods will be set up.</li> <li><code>clusterLabels</code> (map[string]string): A dictionary of labels that can be applied to the cluster for better management and filtering. Example: <code>{\"environment\": \"production\"}</code>.</li> <li><code>clusterAnnotations</code> (map[string]string): A dictionary for annotations, which are often used for non-identifying information. Example: <code>{\"team\": \"devops\"}</code>.</li> <li><code>clusterNetwork.pods.cidrBlocks</code> (array of strings): The IP address range allocated for Pods in the cluster. Example: <code>[\"10.244.0.0/16\"]</code>.</li> <li><code>clusterNetwork.services.cidrBlocks</code> (array of strings): The IP address range allocated for Services. Example: <code>[\"10.96.0.0/12\"]</code>.</li> </ul>"},{"location":"reference/template/template-remote/#machines-parameters","title":"Machines Parameters","text":"<ul> <li><code>machines[].address</code> (string): Address is the IP address or DNS name of the remote machine. Examples: <code>\"10.130.0.237\"</code>, <code>\"2001:0db8:85a3:0000:0000:8a2e:0370:7334\"</code>, <code>\"some-hostname\"</code>.</li> <li><code>machines[].port</code> (number): The SSH port of the remote machine. Default is 22. Example: <code>22</code>.</li> <li><code>machines[].user</code> (string): The user name for SSH login. Default is root. Example: <code>\"root\"</code>.</li> <li><code>machines[].useSudo</code> (boolean): Whether or not to use sudo for running commands on the remote machine. Example: <code>false</code>.</li> <li><code>machines[].provisionJob.scpCommand</code> (string): The command to use for copying files to remote machines. Example: <code>\"scp\"</code>.</li> <li><code>machines[].provisionJob.sshCommand</code> (string): The command to use for connecting to remote machines. Example: <code>\"ssh\"</code>.</li> <li><code>machines[].provisionJob.jobSpecTemplate.metadata</code>: Kubernetes metadata for the provisioning job, such as labels or annotations. See: https://docs.k0smotron.io/stable/resource-reference/#remotemachinespecprovisionjobjobspectemplatemetadata for more information.</li> <li><code>machines[].provisionJob.jobSpecTemplate.spec</code>: Specification for the provisioning job, detailing the job\u2019s behavior and configuration. See: https://docs.k0smotron.io/stable/resource-reference/#remotemachinespecprovisionjobjobspectemplatespec for more information.</li> <li><code>machines[].k0s.args</code> (array of strings): A list of extra arguments for configuring the k0s worker node. See: https://docs.k0sproject.io/stable/cli/k0s_worker.</li> </ul>"},{"location":"reference/template/template-remote/#k0smotron-parameters","title":"K0smotron Parameters","text":"<ul> <li><code>k0smotron.controllerPlaneFlags</code> (array of strings): The <code>controllerPlaneFlags</code> parameter enables you to configure additional flags for the k0s control plane and to override existing flags. The default flags are kept unless they are explicitly overriden. Flags with arguments must be specified as a single string, such as <code>--some-flag=argument</code>.</li> <li><code>k0smotron.persistence.type</code> (string): This parameter defines the persistence type for the control plane\u2019s state. Example: <code>\"EmptyDir\"</code>. See https://docs.k0smotron.io/stable/configuration/#persistence for more information.</li> <li><code>k0smotron.service.type</code> (string): This parameter defines the type of service for the Kubernetes API server: <code>\"ClusterIP\"</code>, <code>\"NodePort\"</code>, or <code>\"LoadBalancer\"</code>.</li> <li><code>k0smotron.service.apiPort</code> (number): This parameter defines the port for accessing the Kubernetes API server. Example: <code>30443</code>.</li> <li><code>k0smotron.service.konnectivityPort</code> (number): This parameter indicates the port for the Konnectivity service. Example: <code>30132</code>.</li> </ul>"},{"location":"reference/template/template-remote/#k0s-parameters","title":"K0s Parameters","text":"<ul> <li><code>k0s.version</code> (string): Specifies the version of the k0s Kubernetes distribution. Example: <code>\"v1.32.2+k0s.0\"</code>.</li> <li><code>k0s.arch</code> (string): Defines K0s Arch in its download URL. Available if global.k0sURL    is set. Possible values: <code>\"amd64\"</code> (default), <code>\"arm64\"</code>, <code>\"arm\"</code>.</li> <li><code>k0s.api.extraArgs</code>: Additional arguments to pass to the Kubernetes API server. Example: <code>{\"--some-arg\": \"value\"}</code>.</li> <li><code>k0s.network</code>: Network settings for the k0s cluster. Example: <code>{\"dns\": \"8.8.8.8\"}</code>.</li> <li><code>k0s.extensions.helm.repositories</code> (array of objects): Helm repositories to add during the cluster setup. Example: <code>[{ \"name\": \"repo1\", \"url\": \"https://charts.repo1.com\" }]</code>.</li> <li><code>k0s.extensions.helm.charts</code> (array of objects): Helm charts to be installed during bootstrap. Example: <code>[{ \"name\": \"chart1\", \"namespace\": \"kube-system\", \"chartname\": \"repo1/chart1\" }]</code>.</li> </ul>"},{"location":"reference/template/template-remote/#example-clusterdeployment","title":"Example ClusterDeployment","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-remote-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: remote-cluster-1-0-15\n  credential: remote-cred\n  propagateCredentials: false\n  config:\n    controlPlaneNumber: 1\n    clusterLabels:\n      k0rdent: demo\n    clusterAnnotations:\n      k0rdent: demo\n    k0smotron:\n      service:\n        type: LoadBalancer\n    machines:\n      - address: 10.130.0.237\n        user: root\n        port: 22\n        k0s:\n          args:\n            - --debug\n      - address: 10.130.0.174\n        user: root\n        port: 22\n    k0s:\n      version: v1.31.5+k0s.0\n      extension:\n        helm:\n          repositories:\n            - name: custom-repo\n              url: https://custom-repo-url\n          charts:\n            - name: custom-chart\n              namespace: kube-system\n              chartname: custom-repo/custom-chart\n              version: \"0.0.8\"\n              values: |\n                customKey: customValue\n</code></pre>"},{"location":"reference/template/template-vsphere/","title":"vSphere cluster template parameters","text":""},{"location":"reference/template/template-vsphere/#clusterdeployment-parameters","title":"ClusterDeployment parameters","text":"<p>To create a cluster deployment a number of parameters should be passed to the <code>ClusterDeployment</code> object.</p>"},{"location":"reference/template/template-vsphere/#parameter-list","title":"Parameter list","text":"<p>The following is the list of vSphere specific parameters that are required for successful cluster creation.</p> Parameter Example Description <code>.spec.config.vsphere.server</code> <code>vcenter.example.com</code> Address of the vSphere instance <code>.spec.config.vsphere.thumbprint</code> <code>\"00:00:00:...\"</code> Certificate thumbprint <code>.spec.config.vsphere.datacenter</code> <code>DC</code> Datacenter name <code>.spec.config.vsphere.datastore</code> <code>/DC/datastore/DS</code> Datastore path <code>.spec.config.vsphere.resourcePool</code> <code>/DC/host/vCluster/Resources/ResPool</code> Resource pool path <code>.spec.config.vsphere.folder</code> <code>/DC/vm/example</code> Folder path <code>.spec.config.controlPlane.network</code> <code>/DC/network/vm_net</code> Network path for <code>controlPlane</code> <code>.spec.config.worker.network</code> <code>/DC/network/vm_net</code> Network path for <code>worker</code> <code>.spec.config.*.ssh.publicKey</code> <code>\"ssh-ed25519 AAAA...\"</code> SSH public key in <code>authorized_keys</code> format <code>.spec.config.*.vmTemplate</code> <code>/DC/vm/templates/ubuntu</code> VM template image path <code>.spec.config.controlPlaneEndpointIP</code> <code>172.16.0.10</code> <code>kube-vip</code> vIP which will be created for control plane endpoint <p>To get the vSphere certificate thumbprint you can use the following command:</p> <pre><code>curl -sw %{certs} https://vcenter.example.com | openssl x509 -sha256 -fingerprint -noout | awk -F '=' '{print $2}'\n</code></pre> <p><code>govc</code>, a vSphere CLI, can also help to discover proper values for some of the parameters:</p> <pre><code># vsphere.datacenter\ngovc ls\n\n# vsphere.datastore\ngovc ls /*/datastore/*\n\n# vsphere.resourcePool\ngovc ls /*/host/*/Resources/*\n\n# vsphere.folder\ngovc ls -l /*/vm/**\n\n# controlPlane.network, worker.network\ngovc ls /*/network/*\n\n# *.vmTemplate\ngovc vm.info -t '*'\n</code></pre> <p>Note</p> <p> Follow official <code>govc</code> installation instructions from here. The <code>govc</code> usage guide is here.</p> <p>Minimal <code>govc</code> configuration requires setting: <code>GOVC_URL</code>, <code>GOVC_USERNAME</code>, <code>GOVC_PASSWORD</code> environment variables.</p>"},{"location":"reference/template/template-vsphere/#configuring-some-of-k0s-k0smotron-parameters","title":"Configuring some of k0s, k0smotron parameters","text":"<ul> <li><code>k0s.arch</code> (string): Defines K0s Arch in its download URL. Available if global.k0sURL    is set. Possible values: <code>\"amd64\"</code> (default), <code>\"arm64\"</code>, <code>\"arm\"</code>.</li> <li><code>k0s.cpArgs</code> <sup>only standalone</sup> (array of strings): A list of extra arguments to be passed to k0s controller.    See: https://docs.k0sproject.io/stable/cli/k0s_controller.</li> <li><code>k0s.workerArgs</code> (array of strings): A list of extra arguments for configuring the k0s worker node. See: https://docs.k0sproject.io/stable/cli/k0s_worker.</li> <li><code>k0smotron.controllerPlaneFlags</code> <sup>only hosted</sup> (array of strings): The <code>controllerPlaneFlags</code> parameter enables you to configure additional flags for the k0s control plane    and to override existing flags. The default flags are kept unless they are explicitly overriden. Flags with arguments must be specified as a single    string, such as <code>--some-flag=argument</code>.</li> </ul>"},{"location":"reference/template/template-vsphere/#example-of-a-clusterdeployment-cr","title":"Example of a ClusterDeployment CR","text":"<p>With all above parameters provided your <code>ClusterDeployment</code> can look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-standalone-cp-1-0-15\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"&lt;VSPHERE_SERVER&gt;\"\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n</code></pre> <p>Don't forget to replace placeholder values such as <code>VSPHERE_SERVER</code> with actual values for your environment.</p>"},{"location":"reference/template/template-vsphere/#ssh","title":"SSH","text":"<p>Currently SSH configuration on vSphere expects that the user is already created before template creation. Because of that you must pass the username along with the SSH public key to configure SSH access.</p> <p>The SSH public key can be passed to <code>.spec.config.ssh.publicKey</code> (in the case of a hosted control plane) or <code>.spec.config.controlPlane.ssh.publicKey</code> and <code>.spec.config.worker.ssh.publicKey</code> (in the case of a standalone control) of the <code>ClusterDeployment</code> object.</p> <p>The SSH public key must be passed literally as a string.</p> <p>You can pass the username to <code>.spec.config.controlPlane.ssh.user</code>, <code>.spec.config.worker.ssh.user</code> or <code>.spec.config.ssh.user</code>, depending on you deployment model.</p>"},{"location":"reference/template/template-vsphere/#vm-resources","title":"VM resources","text":"<p>The following parameters are used to define VM resources:</p> Parameter Example Description <code>.rootVolumeSize</code> <code>50</code> Root volume size in GB (can't be less than the one defined in the image) <code>.cpus</code> <code>2</code> Number of CPUs <code>.memory</code> <code>4096</code> Memory size in MB <p>The resource parameters are the same for hosted and standalone CP deployments, but they are positioned differently in the spec, which means that they're going to:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"reference/template/template-vsphere/#vm-image-and-network","title":"VM Image and network","text":"<p>To provide image template path and network path the following parameters must be used:</p> Parameter Example Description <code>.vmTemplate</code> <code>/DC/vm/template</code> Image template path <code>.network</code> <code>/DC/network/Net</code> Network path <p>As with resource parameters the position of these parameters in the <code>ClusterDeployment</code> depends on deployment type and these parameters are used in:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"release-notes/","title":"k0rdent Release Notes","text":"<p>k0rdent provides release notes for each official release.</p> <ul> <li>k0rdent v1.0.0</li> <li>k0rdent v1.1.0</li> <li>k0rdent v1.3.0</li> <li>k0rdent v1.3.1</li> <li>k0rdent v1.4.0</li> <li>k0rdent v1.5.0</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/","title":"k0rdent OSS v1.0.0 Release Notes","text":"<p>Released: May 20, 2025</p> <p>k0rdent OSS is the upstream, community-driven version of the k0rdent platform, released under the Apache 2.0 license. It delivers the core functionality of k0rdent as an open, composable control plane for managing Kubernetes clusters, services, and observability across public clouds, private data centers, and edge environments.</p>"},{"location":"release-notes/release-notes-v1.0.0/#oss-component-breakdown","title":"OSS Component Breakdown","text":"<ul> <li> <p>KCM (k0rdent Cluster Management)     Manages provisioning, upgrade, scaling, and lifecycle of Kubernetes clusters via Cluster API.</p> </li> <li> <p>KSM (k0rdent State Management)     Uses declarative, templated ServiceTemplates to manage consistent deployment of services like Istio, Flux, and cert-manager across clusters.</p> </li> <li> <p>KOF (k0rdent Observability &amp; FinOps)     Provides metrics, logs, dashboards, and cost visibility using VictoriaMetrics and OpenCost integration.</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#major-highlights","title":"Major Highlights","text":"<p>This GA release of the full k0rdent OSS platform includes:</p> <ul> <li>Fully stabilized v1beta1 APIs across all components</li> <li>Production-grade multi-cluster support</li> <li>Enterprise-ready service templating and orchestration</li> <li>Integrated observability stack with AI/ML workload support</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#new-features","title":"New Features","text":""},{"location":"release-notes/release-notes-v1.0.0/#multi-cluster-management","title":"Multi-Cluster Management","text":"<ul> <li> <p>ServiceTemplateChains</p> <p>Enables conditional, chained service deployments with upgrade paths [#1433 \u00b7 simplifies complex service topologies]</p> </li> <li> <p>Global Values Support</p> <p>Reuse consistent variables across ClusterTemplates and ProviderTemplates [#1535 \u00b7 ensures DRY, repeatable infrastructure definitions]</p> </li> <li> <p>IPAM Controller Integration</p> <p>Automates IP address management across environments [#1260 \u00b7 removes manual allocation and avoids IP conflicts]</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#observability-enhancements-kof","title":"Observability Enhancements (KOF)","text":"<ul> <li> <p>NVIDIA GPU Monitoring</p> <p>Native Grafana dashboards for AI/ML workloads [#257 \u00b7 visibility into GPU utilization without extra config]</p> </li> <li> <p>Kube API Server Metrics</p> <p>Adopts OpenTelemetry-based monitoring of API server health [#259 \u00b7 immediate insights into control plane performance]</p> </li> <li> <p>VictoriaMetrics Log Cluster Migration</p> <p>Scalable logging stack with high-retention support [#274 \u00b7 improved storage efficiency and performance]</p> </li> <li> <p>Cluster Annotation Support for Promxy/Datasource Config</p> <p>Allows per-cluster HTTP config customization [#276]</p> </li> <li> <p>Custom Resource Limits for Observability Stack</p> <p>Fine-tune Grafana and VM components [#263 \u00b7 optimize memory/CPU usage across environments]</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#notable-fixes","title":"Notable Fixes","text":"<ul> <li>Fixed invalid CR references in OpenStack/IPAM providers      [#1522, #1496]</li> <li>Improved e2e test reliability and configuration fetching      [#1517, #1463]</li> <li>Fixed resource tuning for VM services      [#279]</li> <li>Corrected Istio remote secret creation      [#270]</li> <li>Addressed Helm/YQ compatibility issue (2-arg enforcement)      [#282]</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#component-provider-versions","title":"Component &amp; Provider Versions","text":"Component / Provider Version Cluster API v1.9.7 CAPI Provider AWS v2.8.2 CAPI Provider Azure v1.19.4 CAPI Provider Docker v1.9.6 CAPI Provider GCP v1.8.1 CAPI Provider Infoblox v0.1.0-alpha.8 CAPI Provider IPAM v0.18.0 CAPI Provider k0smotron v1.5.2 CAPI Provider OpenStack (ORC) v0.12.3 / v2.1.0 CAPI Provider vSphere v1.13.0 Project Sveltos v0.54.0"},{"location":"release-notes/release-notes-v1.0.0/#platform-benefits-in-v100","title":"Platform Benefits in v1.0.0","text":"<ul> <li>Declarative, template-driven provisioning of clusters and services  </li> <li>Observability built-in, including GPU and API metrics  </li> <li>GitOps-ready: compatible with ArgoCD, FluxCD, Velero, etc.  </li> <li>Unified Kubernetes-native APIs with long-term schema stability</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#upgrade-notes","title":"Upgrade Notes","text":"<ul> <li>Ensure all <code>ClusterTemplate</code>, <code>ServiceTemplate</code>, and <code>ProviderTemplate</code> definitions are updated to use <code>apiVersion: v1beta1</code>.</li> <li>Custom tooling or integrations built on v1alpha1 resources must be updated.</li> <li>We recommend creating a Velero backup of your management cluster before upgrade.</li> <li>Use helm upgrade <code>--reuse-values</code> to preserve your existing configuration.</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#deprecations","title":"Deprecations","text":"<ul> <li>All <code>v1alpha1</code> APIs are now deprecated and will be removed in a future release.</li> <li>Support for the legacy <code>loki-stack</code> logging backend has been removed. Migrate to the <code>victoria-log-cluster</code> stack.</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#known-issues","title":"Known Issues","text":"<ul> <li>Grafana dashboards may take up to 60 seconds to initialise after cluster deployment.</li> <li>MultiClusterService priority conflicts may require manual resolution if priorities are equal.</li> <li>In high-latency networks, IPAM reconciliation can be delayed.</li> <li>Velero restore across cloud providers may require exclusion of specific resources (see docs).</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#release-metadata","title":"Release Metadata","text":"Key Value Helm Charts kcm: 1.0.0, kof: 1.0.0, ksm: 1.0.0 OCI Registry ghcr.io/k0rdent SBOM Not included in OSS OCI Signature Support Enterprise only Release Tags v1.0.0 across all components"},{"location":"release-notes/release-notes-v1.0.0/#contributors","title":"Contributors","text":"<p>Huge thanks to the following contributors for making this release possible: @gmlexx, @denis-ryzhkov, @ramessesii2, @aglarendil, @kylewuolle, @a13x5, @eromanova, @zerospiel, @BROngineer, @cdunkelb</p>"},{"location":"release-notes/release-notes-v1.0.0/#resources","title":"Resources","text":"<ul> <li>Documentation</li> <li>GitHub Repositories</li> <li>CNCF Slack Channels: #k0rdent</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#try-it-out","title":"Try It Out","text":"<p>QuickStart guide: https://docs.k0rdent.io/1.0.0/quickstarts/</p>"},{"location":"release-notes/release-notes-v1.1.0/","title":"k0rdent OSS v1.1.0 Release Notes","text":"<p>Released: June 30, 2025</p> <p>k0rdent OSS is the upstream, community-driven version of the k0rdent platform, released under the Apache 2.0 license. It delivers the core functionality of k0rdent as an open, composable control plane for managing Kubernetes clusters, services, and observability across public clouds, private data centers, and edge environments.</p>"},{"location":"release-notes/release-notes-v1.1.0/#component-provider-versions","title":"Component &amp; Provider Versions","text":"Component / Provider Version Cluster API v1.10.3 CAPI Provider AWS v2.8.2 CAPI Provider Azure v1.19.4 CAPI Provider Docker v1.9.6 CAPI Provider GCP v1.9.0 CAPI Provider Infoblox v0.1.0-alpha.8 CAPI Provider IPAM v1.0.2 CAPI Provider k0smotron v1.5.4 CAPI Provider OpenStack (ORC) v0.12.3 / v2.1.0 CAPI Provider vSphere v1.13.0 Project Sveltos v0.54.0"},{"location":"release-notes/release-notes-v1.1.0/#new-features","title":"\ud83d\ude80 New Features \ud83d\ude80","text":"<ul> <li>Pass certificate secret to the default helm repository (#1558)</li> <li>Ability to set k0s dl url arch (#1682)</li> <li>K0s cp/worker flags (#1681)</li> <li>Adapt openstack template to mount CA cert (#1624)</li> <li>Add support for GitRepository via fluxcd source. (#1631)</li> <li>Add support for mounting registry and k0s URL certificates (#1595)</li> <li>Mount registry certificate secret in CAPI operator (#1580)</li> <li>Pass registry CA cert to hosted CP components (#1659)</li> <li>StateManagementProvider API and controller (#1489)</li> <li>Add non-root volumes to AWS Templates (#1567)</li> <li>Automatically update stuck sveltos tokens (#1588)</li> <li>Disable v1alpha1 api (#1678)</li> <li>Propagate registry-credentials across cld (#1598)</li> <li>Switching to upstream PrometheusRules at promxy and regional with patches for all/specific clusters (#248)</li> <li>Add server to kof-operator for prometheus observability (#275)</li> <li>Add configurable UI port setting (#314)</li> <li>ContainerHighMemoryUsage alert for CAPI Operator and others (#317)</li> <li>Configure Grafana SSO using Dex (#319)</li> <li>Add autoinstrumentation to kof operator to collect metrics and traces (#344)</li> <li>Custom image registries PRs and resolved conflicts (#348)</li> <li>Sync kof operator resources when cluster annotation changes (#340)</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#improvements","title":"\u2728 Improvements \u2728","text":"<ul> <li>Add tests for kof operator UI (#299)</li> <li>Bump <code>vite</code> from 6.2.0 to 6.2.7 (#306)</li> <li>Use latest KCM release for CI (#308)</li> <li>Add adopted regional cluster deployment (#309)</li> <li>Add kof-ui docs (#313)</li> <li>Align chart versions; sveltos dashboard bump to 0.54.0 (#316, #318)</li> <li>Add adopted child cluster deployment (#321)</li> <li>Replace collector event receiver (#327)</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#upgrade-notes","title":"\u2757 Upgrade Notes \u2757","text":"<ul> <li>After upgrading KOF, please run:</li> </ul> <pre><code>kubectl apply --server-side --force-conflicts \\\n  -f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre> <ul> <li>And run the same for each regional cluster:</li> </ul> <pre><code>kubectl get secret -n kcm-system $REGIONAL_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; regional-kubeconfig\n\nKUBECONFIG=regional-kubeconfig kubectl apply --server-side --force-conflicts \\\n  -f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre> <p>This is noted as required in the grafana-operator release notes.</p>"},{"location":"release-notes/release-notes-v1.1.0/#notable-changes","title":"\u2728 Notable Changes \u2728","text":"<ul> <li>Remove namespace from AWSClusterStaticIdentity reference (#1585)</li> <li>Align vsphere-standalone tpl, fn names, logs, packages (#1683)</li> <li>Auto-update all configs (#1650)</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#notable-fixes","title":"\ud83d\udc1b Notable Fixes \ud83d\udc1b","text":"<ul> <li>Flapping delete conditions (#1653)</li> <li>CAPI operator registry cert mount volume mismatch (#1621)</li> <li>Correct the k0s version of aws-standalone-cp (#1623)</li> <li>Ensure services are removed before ClusterDeployment removal (#1584)</li> <li>IPAM claim inline processing (#1652)</li> <li>Rework the helm-push Makefile target (#1520, #1524)</li> <li>Set configure-cloud-routes=false in gcp ccm parameters (#1540)</li> <li>Add CertSecretRef to registry configuration (#1630)</li> <li>Check if services from self are running before deleting ClusterDeployment (#1648)</li> <li>Don't expect operator provider in ProviderTemplates (#1638)</li> <li>Global values for azure, capi, k0smotron providers (#1593)</li> <li>Helm values for servicetemplates not showing in status.config (#1544)</li> <li>Image paths inaccuracies (#1615)</li> <li>Properly preserve meta during copying (#1604)</li> <li>Use global.registry in orc templates (#1566)</li> <li>Addon-controller ServiceMonitor watches wrong namespace (#290)</li> <li>Filter out projectsveltos_* metrics from kcm cm (#291)</li> <li>Errors on upgrade of kof ServiceTemplates (#295)</li> <li>Logs sorting order (#301)</li> <li>Remove shadcn add command causing unwanted file updates (#307)</li> <li>Temporary adaptation of new alerts to current metrics (#310)</li> <li>Workaround for <code>generatorURL</code> in alerts and \"See source\" in Grafana (#312)</li> <li>Too many open files in <code>sveltos-dashboard</code> (#320)</li> <li>Use correct namespace name for MCS (#332)</li> <li>Update grafana to fix CVE-2025-4123 (#339)</li> <li>Allow to parametrise operators and ingress-nginx values (#337)</li> <li>Make Sveltos follow cluster updates (#333)</li> <li>Pinned versions of Grafana plugins and Promxy as part of aig-gap solution (#349)</li> <li>Duplicate <code>version</code> field in Grafana (#350)</li> <li>Pattern dist not found on <code>kof-operator-release</code> (#353, #354)</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#deprecations-and-removals","title":"Deprecations and Removals","text":"<ul> <li>All <code>v1alpha1</code> APIs are have previously been deprecated v1.0.0 and have now been removed.</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#known-issues","title":"Known Issues","text":"<ul> <li>None</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#release-metadata","title":"Release Metadata","text":"Key Value Helm Charts kcm: 1.1.0, kof: 1.1.0, ksm: 1.1.0 OCI Registry ghcr.io/k0rdent SBOM Not included in OSS OCI Signature Support Enterprise only Release Tags v1.1.0 across all components"},{"location":"release-notes/release-notes-v1.1.0/#contributors","title":"Contributors","text":"<p>Huge thanks to the following contributors for making this release possible: @gmlexx, @denis-ryzhkov, @aglarendil, @kylewuolle, @a13x5, @eromanova, @zerospiel, @BROngineer, @Kshatrix, @dis-xcom, @wahabmk, @AndrejsPon00</p>"},{"location":"release-notes/release-notes-v1.1.0/#resources","title":"Resources","text":"<ul> <li>Documentation</li> <li>GitHub Repositories</li> <li>CNCF Slack Channels: #k0rdent</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#try-it-out","title":"Try It Out","text":"<p>QuickStart guide: https://docs.k0rdent.io/1.0.0/quickstarts/</p>"},{"location":"release-notes/release-notes-v1.3.0/","title":"k0rdent 1.3 Release Notes","text":"<p>Release date: 2025-08-28</p>"},{"location":"release-notes/release-notes-v1.3.0/#components-versions","title":"Components Versions","text":"Provider Name Version Cluster API v1.10.4 Cluster API Provider AWS v2.8.4 Cluster API Provider Azure v1.20.2 Cluster API Provider Docker v1.9.6 Cluster API Provider GCP v1.10.0 Cluster API Provider Infoblox v0.1.0-alpha.8 Cluster API Provider IPAM v1.0.2 Cluster API Provider k0smotron v1.6.2 Cluster API Provider OpenStack (ORC) v0.12.4 (v2.1.0) Cluster API Provider vSphere v1.13.1 Projectsveltos v0.57.2"},{"location":"release-notes/release-notes-v1.3.0/#highlights","title":"Highlights","text":"<ul> <li> <p>k0rdent Cluster Manager (KCM):</p> <ul> <li>Telemetry Collection</li> <li>Support to customize the default Helm install or upgrade timeout</li> </ul> </li> <li> <p>ServiceSet API &amp; Sveltos provider (KSM): </p> <ul> <li>New abstraction for managing services with explicit dependencies and ordered upgrades. Enables smoother rollouts and future multi-provider support.</li> </ul> </li> <li> <p>Observability (KOF): </p> <ul> <li>New kube-state-metrics dashboards for k0rdent CRDs and alerting.  </li> <li>\"Raw metrics\" tab in the UI for collectors/logs.  </li> <li>Promxy tuned for heavy queries to avoid OOMs.</li> </ul> </li> <li> <p>Platform &amp; Dependency Updates: </p> <ul> <li>k0smotron upgraded to v1.6.2 </li> <li>Cluster API Azure provider upgraded to v1.20.2 </li> <li>Go toolchain moved to 1.24.6, with Helm and testing libraries updated.</li> </ul> </li> <li> <p>Stability &amp; UX: </p> <ul> <li>Multiple fixes to credential propagation, service status collection, hosted cluster templates, Helm repos, and cloud provider templates (OpenStack, Azure).</li> </ul> </li> </ul>"},{"location":"release-notes/release-notes-v1.3.0/#upgrade-notes","title":"Upgrade Notes","text":"<ul> <li>KOF:<ul> <li>Please apply the \"Reconciling MultiClusterService\" workaround.</li> </ul> </li> <li>Providers: <ul> <li>Align pinned dependencies with new versions (k0smotron 1.6.2, CAPI Azure 1.20.2).  </li> <li>Build/test environments should use Go 1.24.6.</li> </ul> </li> <li>Observability: <ul> <li>After upgrading KOF, apply the new dashboards and validate Promxy resource settings against your workload size.</li> </ul> </li> <li>KSM/KCM APIs: <ul> <li>The new <code>ServiceSet</code> API is additive. Existing Sveltos configurations continue to work, but you should begin testing with ServiceSets for future upgrades.</li> </ul> </li> </ul>"},{"location":"release-notes/release-notes-v1.3.0/#changelog","title":"Changelog","text":""},{"location":"release-notes/release-notes-v1.3.0/#new-features","title":"New Features","text":"<ul> <li>a8c97a6: feat: kube-state-metrics dashboards for k0rdent objects (#497) by  @gmlexx</li> <li>bdb9707: feat: child telemetry tracker (#1783) by  @zerospiel</li> <li>1355a18: feat: expose services.policyRef in CD and MCS spec (#1725) by  @wahabmk</li> <li>82d3e06: feat: introduce local telemetry collector (#1845) by  @zerospiel</li> <li>f5fe7bb: feat: provider-agnostic KSM with built-in provider (#1670) by  @BROngineer</li> <li>b15d226: feat: telemetry local storage configuration (#1857) by  @zerospiel</li> <li>acb9120: feat: add http config for adopted regional cluster by  @gmlexx</li> <li>93d1064: feat: add backend for internal observability of VictoriaMetrics/Logs (#463) by  @AndrejsPon00</li> <li>6958133: feat: add VictoriaMetrics and VictoriaLogs observability page to KOF UI (#480) by  @AndrejsPon00</li> <li>1fa557d: feat: allow full vm custom objects specs definition in values (#478) by  @gmlexx</li> <li>6978d1c: feat: add tooltip for metrics description in KOF UI (#483) by  @AndrejsPon00</li> <li>370da8f: feat: update helm charts on storage secret change (#484) by  @gmlexx</li> <li>838f53a: feat: add raw metrics tab in KOF UI (#487) by  @AndrejsPon00</li> <li>1131315: feat: add custom resources to kube-state-metrics (#489) by  @gmlexx</li> <li>7832d51: feat: mothership components monitoring (#342) by  @aglarendil</li> <li>6df9757: feat: add misconfiguration check for collector scrape in KOF UI (#490) by  @AndrejsPon00</li> </ul>"},{"location":"release-notes/release-notes-v1.3.0/#notable-fixes","title":"Notable Fixes","text":"<ul> <li>9107ce6: fix: prevent duplicate metric collection (#488) by  @AndrejsPon00</li> <li>d61ff20: fix: Mount CA cert as a volume to flux components (#1844) by  @eromanova</li> <li>e11fe9e: fix: Mount CA secret to flux before installing kcm-templates (#1847) by  @eromanova</li> <li>efb50cd: fix: add default storage class for openstack standalone templates (#1871) by  @bnallapeta</li> <li>6a2ae2e: fix: allow configuration of the default HelmRelease timeout (#1830) by  @eromanova</li> <li>fe394a8: fix: continue e2e tests after deployment failure (#1840) by  @eromanova</li> <li>cfcd997: fix: credentials propagation (#1886) by  @BROngineer</li> <li>44c9e2e: fix: drop subpath from the registry in hosted helm repositories (#1890) by  @eromanova</li> <li>76e1c79: fix: install yq before generating release.yaml (#1894) by  @eromanova</li> <li>2322cbe: fix: k0s image url for azure-hosted-cp (#1856) by  @a13x5</li> <li>6e49c90: fix: multiclusterservice requeue (#1899) by  @BROngineer</li> <li>97af4ed: fix: redundant servicetemplates fetching (#1832) by  @zerospiel</li> <li>f1930e2: fix: remote-cluster (hosted) respects useSudo (#1880) by  @zerospiel</li> <li>103b439: fix: service statuses (#1888) by  @BROngineer</li> <li>55d963e: fix: services deployed &amp; clusters matched in MCS kubectl output (#1779) by  @wahabmk</li> <li>af5feae: fix: set kcm version in release.yaml before making the release (#1877) by  @eromanova</li> <li>cfba650: fix: change opencost prometheus URL to HTTP for local cluster (#451) by  @AndrejsPon00</li> <li>7ba9801: fix: correct instrumentation exporter endpoint to resolve trace export error (#452) by  @AndrejsPon00</li> <li>1dc8a60: fix: Replacing release notes with auto-generated ones, updated docs/release (#453) by  @denis-ryzhkov</li> <li>64b6f5b: fix: slow KOF UI responses due to long proxy timeout (#448) by  @AndrejsPon00</li> <li>f49b35a: fix: Customized  <code>cert-manager-startupapicheck</code>  image registry (#457) by  @denis-ryzhkov</li> <li>d15e1cb: fix: promxy server group doesn't update after http client config changes (#456) by  @AndrejsPon00</li> <li>ad3bec4: fix: increase promxy memory requests/limits to prevent OOM (#458) by  @AndrejsPon00</li> <li>2c3d50a: fix: move grafana operator to kof-operators helm chart (#461) by  @gmlexx</li> <li>ada76b5: fix: Jaeger authenticated endpoint of regional cluster became available for other clusters (#462) by  @denis-ryzhkov</li> <li>3437957: fix:  <code>istio/gateway</code>  chart repo compatibility with custom registry (#464) by  @denis-ryzhkov</li> <li>588682e: fix: add promxy suffix to promxy labels by  @gmlexx</li> <li>f3dbad0: fix: add missing env variable for goreleaser (#466) by  @gmlexx</li> <li>6ea8e64: fix: Added  <code>ServiceTemplateChain</code> <code>cert-manager-v1-16-4-from-1-16-4</code>  required for upgrade to KOF 1.2.0 (#467) by  @denis-ryzhkov</li> <li>52b9658: fix: override only defined properties with annotation on config update (#468) by  @gmlexx</li> <li>4353a1e: fix: Custom  <code>kcm.serviceMonitor.selector</code>  (#472) by  @denis-ryzhkov</li> <li>2d6104b: fix: \"Cluster Deployments Events\" dashboard vs \"From Management to Regional\" case (#469) by  @denis-ryzhkov</li> <li>5f6f3dd: fix: Custom  <code>registryCredentialsConfig</code>  in  <code>helmCharts</code>  of  <code>kof-istio</code>  (#473) by  @denis-ryzhkov</li> <li>f9ad1e9: fix: use node name in node exporter dashboards (#470) by  @gmlexx</li> <li>75f174b: fix: Two cases of  <code>chartName</code>  for  <code>cert-manager</code>  in  <code>kof-istio-network</code>  by  @denis-ryzhkov</li> <li>eb8d43f: fix: Moved  <code>kof-operators</code>  to be installed before  <code>kof-storage</code>  in  <code>kof-istio-regional</code>  to avoid \"CRDs not found\" by  @denis-ryzhkov</li> <li>06a137d: fix: Updated Jaeger secret name after movingit from  <code>kof-storage</code>  to  <code>kof-mothership</code>  in  #462  to avoid  <code>invalid ownership metadata</code>  by  @denis-ryzhkov</li> <li>48504e1: fix: ContainerHighMemUsage alert has container label missing (#477) by  @aglarendil</li> <li>a1ce5b9: fix: Typo in  <code>intervalFactor</code>  lead to 500 in \"Istio Service Dashboard\" (#479) by  @denis-ryzhkov</li> <li>3154164: fix: incorrect log level parsing for uppercase codes (#481) by  @AndrejsPon00</li> <li>c0098ec: fix: correctly parse and render total metric values and labels (not just last label) in kof UI (#486) by  @AndrejsPon00</li> <li>b63188f: fix: prevent OOM crash in promxy on large queries (#491) by  @AndrejsPon00</li> <li>93599c1: fix: correct memory queries in Grafana dashboard panels (#494) by  @AndrejsPon00</li> </ul>"},{"location":"release-notes/release-notes-v1.3.0/#notable-changes","title":"Notable Changes","text":"<ul> <li>13e3720: chore: KOF 1.3.0 release (#499) by  @denis-ryzhkov</li> <li>b3e2247: fix(os-tpls): correct identity name in identityref (#1901) by  @zerospiel</li> <li>608b448: refactor: small change to reuse already defined statemanagementprovider name (#1883) by  @wahabmk</li> <li>d3cc733: chore: setup go based on go.mod file by  @gmlexx</li> <li>fbd2d4a: chore: apply coredns patch for mothership and restart once by  @gmlexx</li> <li>7867e05: chore: add promxy port-forward target by  @gmlexx</li> <li>c62848b: test: check promxy metrics by  @gmlexx</li> <li>36154db: chore: add charts and docker images build (#465) by  @gmlexx</li> <li>29f0a2b: test: wait until vmauth creates ingress in kind-adopted-regional cluster (#471) by  @gmlexx</li> <li>1885064: chore: KOF 1.2.1 patch release by  @denis-ryzhkov</li> <li>c106d22: test: add unit tests for Victoria pages (KOF UI) (#482) by  @AndrejsPon00</li> <li>1ac30b6: chore: KOF KOF 1.3.0-rc1 (#496) by  @denis-ryzhkov</li> </ul>"},{"location":"release-notes/release-notes-v1.3.0/#known-issues","title":"Known Issues","text":"<p>KSM (fixed in k0rdent 1.3.1):</p> <ul> <li>Inconsistent services status reporting</li> <li>\"Reconciling MultiClusterService\" loop stops, pod restart is required</li> <li>Multiclusterservice cleanup issues</li> <li>Adopted clusters are not handled by ksm</li> </ul> <p>Track GitHub issues for updates.</p>"},{"location":"release-notes/release-notes-v1.3.0/#contributors","title":"Contributors","text":"<p>Thank you to everyone who contributed PRs, issues, and testing for this release, including (but certainly not limited to) @zerospiel, @BROngineer, @eromanova, @denis-ryzhkov, @a13x5, @wahabmk, and @bnallapeta.</p>"},{"location":"release-notes/release-notes-v1.3.0/#references","title":"References","text":"<ul> <li>Compare KCM v1.2.0...v1.3.0 </li> <li>Compare KOF v1.2.0...v1.3.0 </li> </ul>"},{"location":"release-notes/release-notes-v1.3.1/","title":"k0rdent 1.3.1 Release Notes","text":"<p>Release date: 2025-09-03</p>"},{"location":"release-notes/release-notes-v1.3.1/#components-versions","title":"Components Versions","text":"Provider Name Version Cluster API v1.10.4 Cluster API Provider AWS v2.8.4 Cluster API Provider Azure v1.20.2 Cluster API Provider Docker v1.9.6 Cluster API Provider GCP v1.10.0 Cluster API Provider Infoblox v0.1.0-alpha.8 Cluster API Provider IPAM v1.0.2 Cluster API Provider k0smotron v1.6.2 Cluster API Provider OpenStack (ORC) v0.12.4 (v2.1.0) Cluster API Provider vSphere v1.13.1 Projectsveltos v0.57.2"},{"location":"release-notes/release-notes-v1.3.1/#changelog","title":"Changelog","text":"<p>KSM has received several critical fixes in the v1.3.1 release:</p>"},{"location":"release-notes/release-notes-v1.3.1/#notable-fixes","title":"Notable Fixes","text":"<ul> <li>b809ba8: fix: collecting ksm service statuses (#1952) by @BROngineer</li> <li>66af56f: fix: enable multiclusterservice requeue on status update (#1946) by @BROngineer</li> <li>030ea58: fix: handle adopted cluster in ksm (#1948) by @BROngineer</li> <li>c66bbe2: fix: multiclusterservice cleanup (#1947) by @BROngineer</li> </ul>"},{"location":"release-notes/release-notes-v1.3.1/#references","title":"References","text":"<ul> <li>Compare KCM v1.3.0...v1.3.1 </li> </ul>"},{"location":"release-notes/release-notes-v1.4.0/","title":"k0rdent 1.4.0 Release Notes","text":"<p>Release date: 2025-09-30</p>"},{"location":"release-notes/release-notes-v1.4.0/#components-versions","title":"Components Versions","text":"Provider Name Version Cluster API v1.10.5 Cluster API Provider AWS v2.9.1 Cluster API Provider Azure v1.21.0 Cluster API Provider Docker v1.9.6 Cluster API Provider GCP v1.10.0 Cluster API Provider Infoblox v0.1.0 Cluster API Provider IPAM v1.0.2 Cluster API Provider k0smotron v1.6.2 Cluster API Provider OpenStack (ORC) v0.12.5-mirantis.0 (v2.1.0) Cluster API Provider vSphere v1.13.1 Projectsveltos v0.57.2"},{"location":"release-notes/release-notes-v1.4.0/#highlights","title":"Highlights","text":"<ul> <li> <p>k0rdent Cluster Manager (KCM):</p> <ul> <li>Distributed Regional Management Control Planes (technical preview)</li> <li>Label based role aggregation to simplify OOT provider itegration</li> </ul> </li> <li> <p>k0rdent Serivce Manager (KSM):</p> <ul> <li>Inter-service dependencies in a ClusterDeployment or MultiClusterService spec</li> <li>Support for helm options to use with service templayes</li> <li>Improved observability of events and metrics for IPAM</li> </ul> </li> <li> <p>Observability (KOF):</p> <ul> <li>KOF UI monitoring for KSM objects</li> </ul> </li> <li> <p>Platform &amp; Dependency Updates:</p> <ul> <li>Cluster API upgraded to v1.10.5</li> <li>Cluster API AWS provider upgraded to v2.9.1</li> <li>Cluster API Azure provider upgraded to v1.21.0</li> <li>Cluster API IPAM provider upgraded to v1.0.2</li> <li>Cluster API OpenStack provider forked version v0.12.5-mirantis.0</li> </ul> </li> </ul>"},{"location":"release-notes/release-notes-v1.4.0/#upgrade-notes","title":"Upgrade Notes","text":"<ul> <li> <p>KCM:</p> <ul> <li>Due to CAPA upgrade to <code>v2.9.1</code> use of the empty selector in the   <code>.spec.allowedNamespaces</code> in the cluster identity resources (such as   <code>AWSClusterStaticIdentity</code>) is no longer valid and may cause <code>Namespace is   not permitted</code> error. To mitigate that change the value to the empty object   like so: <code>.spec.allowedNamespaces: {}</code> (ref: #2022).</li> </ul> </li> <li> <p>KOF:</p> <ul> <li> <p>Because of <code>PromxyServerGroup</code> CRD is moved to the <code>templates</code> directory for   more flexible updates you must use the <code>--take-ownership</code> flag while   upgrading <code>kof-mothership</code> to <code>1.4.0</code>. For example:</p> <pre><code>    helm upgrade --take-ownership \\\n        --reset-values --wait -n kof kof-mothership -f mothership-values.yaml \\\n        oci://ghcr.io/k0rdent/kof/charts/kof-mothership --version 1.4.0\n</code></pre> </li> </ul> </li> </ul>"},{"location":"release-notes/release-notes-v1.4.0/#changelog","title":"Changelog","text":""},{"location":"release-notes/release-notes-v1.4.0/#new-features","title":"New Features","text":"<ul> <li>694718e44b4b62e13ab60fe73a29ec15bdd0e90a: feat(backups): add region support (#2040) by @zerospiel</li> <li>330248da1a1c968d3b085d39b4af56886198ff39: feat: adapt services reconciliation to regional clusters (#2011) by @eromanova</li> <li>2700e734a157adf2ebeedee1a2f78ebfb75a1411: feat: add JSON Schema configmap for templates if available (#1972) by @Kshatrix</li> <li>e074793a89c7db66350fd16e4e6fe72f8136b516: feat: add Region CRD and controller (#1958) by @eromanova</li> <li>39e448b1c43c34cb6f9d81cd2fff44311ef6c68d: feat: add aggregate role for kcm manager (#1976) by @Kshatrix</li> <li>04338d8e418d5ca0ce328207886ff8cbcadfe2ea: feat: add helm options to service templates (#1969) by @kylewuolle</li> <li>fb5618f83be005bbc69ceac50506a4664b2efb70: feat: cleanup orphaned cloud resources (#1973) by @zerospiel</li> <li>f1fcc57a631900775c672d74ba57dabd8563c197: feat: copy certificate secrets to regional clusters (#2025) by @eromanova</li> <li>afbefe3544dac44d5f1ef7c0804ee3f5ddeb68f6: feat: deprecate the Provider Interface controller (#2001) by @eromanova</li> <li>ad4e94f03220b894c187b2003d072579f8b70538: feat: implementation &amp; validation of service dependencies (#1968) by @wahabmk</li> <li>d44581c457e1783819b73ef37997a14707032b52: feat: introduce Region field for Credential (#1980) by @eromanova</li> <li>9b1f977db98992ba2a0f35810d191bea3d4a9c15: feat: move provider rbac to corresponding provider templates (#2007) by @Kshatrix</li> <li>0087c5583ffce792891bc65ecdc6604292ca92ee: feat: observability of events and metrics for ipam (#1882) by @kylewuolle</li> <li>0e7d07617c5d5e6a927e7d985a19115e9b894a79: feat: rework ClusterTemplate valid status check (#2002) by @eromanova</li> <li>44ec8fb1724824a3873175fbc25a9353dfd8c6eb: feat: several minor Region improvements (#2010) by @eromanova</li> <li>b31f72904d1723815916051aae34891bfee35cf4: feat: add cluster deployment monitoring page to KOF UI (#502) by @AndrejsPon00</li> <li>032fd301e52b95e1b7333e83c969ffc7d5933ca2: feat: add cluster summaries monitoring page to KOF UI (#505) by @AndrejsPon00</li> <li>ffc72f46a3e822085aba001f90d4bf3939b80f9f: feat: Add multi cluster services monitoring page to KOF UI (#508) by @AndrejsPon00</li> <li>48eb9d3f9b4c72c7a4abd64f69c4f942af416ada: feat: add state management provider monitoring to KOF UI (#509) by @AndrejsPon00</li> <li>4a3c1429265dde9ad325f57ed87b9a6c7338ded5: feat: add service set monitoring page to KOF UI (#519) by @AndrejsPon00</li> <li>faa2c3142d3cba5afa237eaccc7b601a77932432: feat: migrate to receiver_creator for filelog/containers to support annotation-based discovery (#529) by @gmlexx</li> <li>3803b5a36a32934c19600bbdf54cd20e57505e7e: feat: add sveltos clusters monitoring page to KOF UI (#531) by @AndrejsPon00</li> <li>783fe3ae93d3a76fad0b421952db5502b6406ad3: feat: add k8s audit logs collector config (#539) by @AndrejsPon00</li> <li>fbf250bb8cc2c907595b7ea8969cba8d0d027d38: feat: add parser for key-value logs (#528) by @AndrejsPon00</li> <li>665c3a834ab05c1a658d3585b6b9c5572bb0ded1: feat: add filestore for filelogreceivers to store offsets (#544) by @gmlexx</li> <li>cae148859a60526883ebfc16244e40d9243bf8cf: feat: add alerts for CAPI Objects states (#526) by @AndrejsPon00</li> <li>ad2ff7807cd5a1f8eee84b3ed831fc33f6dc9e83: feat: add adopted clusters support for Istio (#551) by @gmlexx</li> </ul>"},{"location":"release-notes/release-notes-v1.4.0/#notable-fixes","title":"Notable Fixes","text":"<ul> <li>c6acb4b1b037dc9261e9ee1871a4ead51f3d0017: fix(infoblox): support arm64 (#1938) by @zerospiel</li> <li>eb8d5fbd751a01b9c059d12d835eb55e0b60376f: fix(templates): allow null location policy for GKE (#2036) by @zerospiel</li> <li>f02213401a014eb29d267561fe4f071821d20462: fix(utils): patch object's component labels (#1949) by @zerospiel</li> <li>e1d81638900170b5144955f13d28b8198dbbdc98: fix: add configmap rbac permissions for controller (#2024) by @Kshatrix</li> <li>b2c5ec6355441c2cc86ce75af04d9b3905f2fba6: fix: add regional section to the Release spec (#2027) by @eromanova</li> <li>f7130057c6e95d9ab45d482ebb8b57a196b24e65: fix: add service as an available upgrade to itself (#2051) by @wahabmk</li> <li>059e801179d92c7a3b14dac311b82ea5592c5405: fix: drop regional section from Release (#1996) by @eromanova</li> <li>fd2c0996626d0fce533b1f80b7c7cfe8fe53ae7a: fix: drop selector from dev aws credential (#2021) by @eromanova</li> <li>a931aabd1574b0fd47bf248c2517d54b4eb29c8b: fix: enable multiclusterservice requeue on status update (#1924) by @BROngineer</li> <li>874c56cd26f0fef7af853aaa16ced90b1bacbd8a: fix: increase default helm timeout for dev setup (#2030) by @eromanova</li> <li>17e432b6d7a5df2443ef0fc93f53ac1465688b4b: fix: missed registry creds config (#1974) by @BROngineer</li> <li>1255e750fac124e15607e3a35a9e5595c5f15676: fix: moved network configuration for standalone deployments into values.yaml (#1887) by @vtrenton</li> <li>3232eeb434258e40d2cd7b807740067de48fcda9: fix: network cfg in hosted tpls; fix standalone (#1895) by @zerospiel</li> <li>b473ea6ee3d6fcd85ee513d602afd87588173c63: fix: openstackclusteridentities permissions (#2046) by @Kshatrix</li> <li>ba8ac300dd01eed2a5f555ef09b3a9012bfc7554: fix: reflect deletion status in Region conditions (#2014) by @eromanova</li> <li>72346855ed3bd8e346453b8e8c2ff93d0b16725e: fix: set current kcm-regional version as kcm dependency (#1982) by @eromanova</li> <li>8c209e4a024e7dbafd2bcfb8ebc36bd7b41461db: fix: several fixes and improvements for regions (#2035) by @eromanova</li> <li>82683aa74956f83e1e529c4aa6ea0792bfbe8b59: fix: remove timestamp metrics from kube-state custom resources (#498) by @gmlexx</li> <li>fe99e29556261358e75d98754dd9de8ce840bfc0: fix: Typo <code>grafana-operator.enables/enabled</code>, dedup of this subchart, updated descriptions (#506) by @denis-ryzhkov</li> <li>2e6c66eacdfcb7cf9e4ef75ea43f18ab2470877c: fix: Fix of warnings on helm install/upgrade of kof-collectors (#504) by @denis-ryzhkov</li> <li>6d2e339eb04e9878067e5d3b26a6ad921866241b: fix: flatten event fields for better filtering (#510) by @gmlexx</li> <li>cc20148369b901f000b4cb40f1853f75d55d9dc6: fix: Auto-upgrade KOF CRD PromxyServerGroup (#546) by @denis-ryzhkov</li> <li>90cd7ab5b4fe9295121c0e983f4cc5061aa0effa: fix: Security fix of vite (#548) by @denis-ryzhkov</li> <li>9be5d400768644698109e001922d15c4e783f7dd: fix: show log line field in dashboard (#559) by @gmlexx</li> <li>79b2f8052d14fe7c86139d8ad371cd900a8f5f69: fix: move collectors service extensions list to upper charts values (#558) by @gmlexx</li> </ul>"},{"location":"release-notes/release-notes-v1.4.0/#references","title":"References","text":"<ul> <li>Compare KCM v1.3.1...v1.4.0</li> </ul>"},{"location":"release-notes/release-notes-v1.5.0/","title":"k0rdent 1.5.0 Release Notes","text":"<p>Release date: October 30, 2025</p>"},{"location":"release-notes/release-notes-v1.5.0/#components-versions","title":"Components Versions","text":"Provider Name Version Cluster API v1.11.2 Cluster API Provider AWS v2.9.2 Cluster API Provider Azure v1.21.1 Cluster API Provider Docker v1.9.6 Cluster API Provider GCP v1.10.0 Cluster API Provider Infoblox v0.1.0 Cluster API Provider IPAM v1.0.2 Cluster API Provider k0smotron v1.9.0 Cluster API Provider OpenStack (ORC) v0.12.5-mirantis.0 (v2.1.0) Cluster API Provider vSphere v1.13.1 Projectsveltos v0.57.2 k0s (control plane runtime) 1.32.8 cert-manager (charts) v1.19.1"},{"location":"release-notes/release-notes-v1.5.0/#highlights","title":"Highlights","text":"<ul> <li>Regions: Regions can now be restored from backup, and can recognize existing <code>ClusterDeployments</code>.</li> <li>Migrate Child Clusters: A <code>ClusterDeployment</code> can now be moved from one management cluster to another.</li> <li>Credentials &amp; Identities: Distribute cluster identities across regions.</li> <li>RBAC Manager: Introduced as a first-class KCM management component.</li> <li>Dependency Recognition: k0rdent now understands <code>MultiClusterService</code> dependencies, avoiding extraneous error messages.</li> <li>Operational changes: You can now pause Sveltos profile reconciliation via a <code>ServiceSet</code> annotation, easing tasks like troubleshooting and disaster recovery.</li> </ul>"},{"location":"release-notes/release-notes-v1.5.0/#upgrade-notes","title":"Upgrade Notes","text":"<ul> <li> <p>IMPORTANT: Avoid upgrading to 1.5.0 if you plan to have or have existing <code>ClusterDeployments</code> using <code>aws-eks-1-0-*</code> <code>ClusterTemplates</code>. Use k0rdent 1.4.0 instead.</p> </li> <li> <p>New/stricter Region validation may surface issues with configurations that were previously accepted. Dry-run manifests before rollout.</p> </li> <li> <p>Region restoration flow is new; confirm backup/restore procedures for regional components.</p> </li> <li> <p>CI images switched base images from <code>scratch</code> to <code>gcr.io/distroless/static-debian12:nonroot</code></p> </li> </ul>"},{"location":"release-notes/release-notes-v1.5.0/#changelog","title":"Changelog","text":""},{"location":"release-notes/release-notes-v1.5.0/#new-features","title":"New Features","text":"<ul> <li>feat: upgrade cluster-api@v1.11.2 (#2032) by @zerospiel</li> <li>feat(restore): support regions restoration (#2073) by @zerospiel</li> <li>feat: adapt existing validation for Regions (#2065) by @eromanova</li> <li>feat: region validation (#2063) by @eromanova</li> <li>feat: credential cluster identity distribution (#2075) by @eromanova</li> <li>feat: support <code>ClusterDeployment</code> reference in Region spec (#2096) by @eromanova</li> <li>feat: add RBAC manager as the KCM management component (#2109) by @eromanova</li> <li>feat: pause reconciliation of sveltos profiles via ServiceSet annotation (no PR listed) by @kylewuolle</li> <li>feat: implement MultiClusterService dependencies (#2009) by @wahabmk</li> <li>feat: regional telemetry collection (#2113) by @zerospiel</li> </ul>"},{"location":"release-notes/release-notes-v1.5.0/#notable-fixes","title":"Notable Fixes","text":"<ul> <li>fix: cert-manager-dependent regional components disabling (#2061) by @eromanova</li> <li>fix: update Credential status when Region is not ready (#2066) by @eromanova</li> <li>chore: remove multiclusterservice validation webhook (#2071) by @BROngineer</li> <li>fix: self-management panics &amp; incorrect profile type (#2074) by @wahabmk</li> <li>fix(providerinterface): Azure ClusterIdentities (#2088) by @zerospiel</li> <li>fix(templates): AWS-CSI image paths; Azure required parameters (#2111) by @a13x5</li> <li>fix: drop kcm-regional version annotation from Release (#2117) by @eromanova</li> </ul>"},{"location":"release-notes/release-notes-v1.5.0/#dependency-tooling-bumps-partial","title":"Dependency / Tooling Bumps (partial)","text":"<ul> <li>chore: cert-manager@v1.19.1 (#2090) by @zerospiel</li> <li>chore: k0smotron provider v1.9.0 (#2115) by @zerospiel</li> <li>chore: k0s 1.32.8 (#2116) by @zerospiel</li> <li>chore: CAPA v2.9.2 (#2123) by @Kshatrix</li> <li>chore: CAPZ v1.21.1 (#2121) by @Kshatrix</li> <li>chore: bump <code>peter-evans/find-comment</code> 3.1.0\u21924.0.0 (#2059)</li> <li>chore: Velero chart bump (#2058) by @zerospiel</li> <li>chore: Ginkgo v2.26.0\u2192v2.27.1 (#2114)</li> <li>chore: Ginkgo v2.27.1\u2192v2.27.2 (#2140)</li> <li>chore: fluxcd/helm-controller API bump (#2139)</li> <li>chore: fluxcd/source-controller API bump (#2141)</li> <li>chore: <code>x/crypto</code> 0.42.0\u21920.43.0 (#2080)</li> <li>chore: <code>kubevirt.io/api</code> 1.6.0\u21921.6.2 (#2079)</li> <li>chore: <code>kubevirt.io/containerized-data-importer-api</code> bump (#2078)</li> <li>ci: distroless to embed CA certs (#2106) by @zerospiel</li> <li>chore: Dockerfile uses <code>buildplatform</code> for builder (#2110) by @a13x5</li> <li>ci(docker): produce latest telemetry image tag (#2135) by @zerospiel</li> <li>ci(post): add dev segment (#2102) by @zerospiel</li> <li>perf(dev): add troubleshoot-profiles to config (#2031) by @briantd</li> <li>chore: default E2E config to AWS if empty (#2134) by @BROngineer</li> <li>chore(telemetry): enable on CI (#2055) by @zerospiel</li> </ul>"},{"location":"release-notes/release-notes-v1.5.0/#references","title":"References","text":"<ul> <li>Compare KCM v1.4.0\u2026main</li> </ul>"},{"location":"templatehowto/","title":"Modifying and Creating k0rdent Templates","text":"<p>In k0rdent, almost everything you deploy or extend is expressed through templates. Templates provide a standardized way to describe complex Kubernetes resources and workflows so they can be reused, versioned, and shared. Whether you are adding services to a cluster, provisioning the cluster itself, or integrating a new infrastructure provider on which to run that cluster, you interact with k0rdent through one of three template types:</p> <ul> <li>ServiceTemplates: <code>ServiceTemplate</code> objects define services and applications that run on clusters, such as workloads, networking, monitoring, or data platforms.  </li> <li>ClusterTemplates: <code>ClusterTemplate</code> objects define clusters, packaging all the infrastructure, control plane, and machine objects needed to stand up a Kubernetes environment.  </li> <li>ProviderTemplates: <code>ProviderTemplate</code> objects define CAPI infrastructure or bootstrap providers themselves, making them available to clusters.  </li> </ul> <p>Although their use cases differ, these templates share the same architectural foundations.</p>"},{"location":"templatehowto/#the-role-of-helm-flux-and-capi","title":"The Role of Helm, Flux, and CAPI","text":"<p>All k0rdent templates ultimately reference Helm charts, which bundle Kubernetes manifests into reusable, versioned packages. They are retrieved and managed by FluxCD, which acts as the continuous delivery engine inside k0rdent, and reconciled by Cluster API (CAPI) providers and other controllers, which take the rendered manifests and turn them into running infrastructure and services.  </p> <ul> <li> <p>Helm provides the packaging format. A Helm chart is essentially a directory containing Kubernetes manifests (in the <code>templates/</code> folder), a metadata file (<code>Chart.yaml</code>), a values file (<code>values.yaml</code>), and optionally a schema file (<code>values.schema.json</code>). Helm\u2019s templating engine replaces placeholders in manifests with values supplied by the user.  </p> </li> <li> <p>FluxCD provides the source management and installation mechanism. Flux fetches Helm charts from Git repositories, OCI registries, or buckets, and ensures that the correct version is available to k0rdent. Flux also runs the Helm Controller, which is responsible for rendering and installing charts into clusters.  </p> </li> <li> <p>Cluster API (CAPI) provides the reconciliation logic. Once the chart is rendered into Kubernetes manifests, CAPI controllers reconcile the resources and create real infrastructure. For example, the CAPI AWS provider reconciles <code>AWSCluster</code> and <code>AWSMachine</code> objects into EC2 instances, while the k0smotron controller reconciles <code>K0sControlPlane</code> objects into running control plane nodes.  </p> </li> </ul> <p>These three components work together so that templates can create actual resources, such as a single <code>ClusterTemplate</code> reference that ultimately produces a fully functioning Kubernetes cluster.</p> <p>Understanding this architectural pattern is essential before diving into specific template types, because once you know how templating works in k0rdent, you can apply the same mental model to service and provider definitions as well.</p>"},{"location":"templatehowto/#why-templates-matter","title":"Why Templates Matter","text":"<ul> <li>Reusability: Templates can be shared across many deployments, ensuring consistent architecture.  </li> <li>Configuration management: Parameters are centralized in a <code>values.yaml</code> file, which acts like a set of variables.  </li> <li>Upgradeability: Because templates are based on versioned charts, you can move between releases safely and predictably.  </li> <li>Governance and validation: Templates can include schemas that enforce correct usage and prevent invalid settings.  </li> </ul> <p>In k0rdent, templates are not just convenience wrappers, they're the backbone of how infrastructure and services are declared, standardized, and delivered.</p> <ul> <li>The Templating System \u2013 Common Threads</li> <li>Creating and Modifying Templates</li> </ul>"},{"location":"templatehowto/modifying-clusterTemplates/","title":"How to Build or Modify a k0rdent Template","text":"<p>One of the most important benefits of k0rdent is the ability to create your own <code>ServiceTemplate</code>, <code>ClusterTemplate</code>, and <code>ProviderTemplate</code> objects. The concepts are largely the same for all three, so we'll demonstrate the process with a <code>ClusterTemplate</code>.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#anatomy-of-a-clustertemplate","title":"Anatomy of a ClusterTemplate","text":"<p>A <code>ClusterTemplate</code> in k0rdent is a Kubernetes custom resource that points to a Helm chart. The chart itself contains all the CAPI objects required to define a cluster. A typical chart structure might look like this:</p> <pre><code>\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 awscluster.yaml\n\u2502   \u251c\u2500\u2500 awsmachinetemplate-controlplane.yaml\n\u2502   \u251c\u2500\u2500 awsmachinetemplate-worker.yaml\n\u2502   \u251c\u2500\u2500 cluster.yaml\n\u2502   \u251c\u2500\u2500 k0scontrolplane.yaml\n\u2502   \u251c\u2500\u2500 k0sworkerconfigtemplate.yaml\n\u2502   \u2514\u2500\u2500 machinedeployment.yaml\n\u251c\u2500\u2500 values.schema.json\n\u2514\u2500\u2500 values.yaml\n</code></pre> <ul> <li><code>Chart.yaml</code> contains metadata: chart name, version, and description.  </li> <li><code>values.yaml</code> contains the default configuration parameters, such as cluster size, networking, and AMI IDs.  </li> <li><code>values.schema.json</code> can restrict or validate the parameters that users supply.  </li> <li>The <code>templates/</code> directory contains Kubernetes manifests for CAPI objects, such as <code>Cluster</code>, <code>AWSCluster</code>, machine templates, and control plane definitions.  </li> </ul> <p>By changing the values in <code>values.yaml</code>, you can produce many different clusters without modifying the underlying templates.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#default-valuesyaml-example","title":"Default values.yaml Example","text":"<p>The following snippet shows the default <code>values.yaml</code> for the AWS standalone control plane <code>ClusterTemplate</code>. This file is the main entry point for customization.</p> <pre><code>controlPlaneNumber: 3\nworkersNumber: 2\n\nclusterNetwork:\n  pods:\n    cidrBlocks:\n      - \"10.244.0.0/16\"\n  services:\n    cidrBlocks:\n      - \"10.96.0.0/12\"\n\nclusterLabels: {}\nclusterAnnotations: {}\n\nregion: \"\"\nsshKeyName: \"\"\npublicIP: false\nbastion:\n  enabled: false\n  disableIngressRules: false\n  allowedCIDRBlocks: []\n  instanceType: t2.micro\n  ami: \"\"\nclusterIdentity:\n  name: \"\"\n  kind: \"AWSClusterStaticIdentity\"\n\ncontrolPlane:\n  amiID: \"\"\n  iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n  instanceType: \"\"\n  rootVolumeSize: 8\n  imageLookup:\n    format: \"amzn2-ami-hvm*-gp2\"\n    org: \"137112412989\"\n    baseOS: \"\"\n  uncompressedUserData: false\n  nonRootVolumes: []\n\nworker:\n  amiID: \"\"\n  iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n  instanceType: \"\"\n  rootVolumeSize: 8\n  imageLookup:\n    format: \"amzn2-ami-hvm*-gp2\"\n    org: \"137112412989\"\n    baseOS: \"\"\n  uncompressedUserData: false\n  nonRootVolumes: []\n\nk0s:\n  version: v1.32.6+k0s.0\n  arch: amd64\n  cpArgs: []\n  workerArgs: []\n  api:\n    extraArgs: {}\n  files: []\n</code></pre>"},{"location":"templatehowto/modifying-clusterTemplates/#building-a-clustertemplate-step-by-step","title":"Building a ClusterTemplate: Step by Step","text":"<p>Follow these steps to create a custom <code>ClusterTemplate</code>.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#1-obtain-or-create-a-helm-chart","title":"1. Obtain or create a Helm chart","text":"<p>Most users start from an existing template. Mirantis maintains a set of baseline <code>ClusterTemplates</code> in the k0rdent GitHub repository. You can download these as tarballs from an OCI registry or clone them directly from GitHub. Alternatively, you can create your own chart with <code>helm create</code>.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#2-inspect-the-chart","title":"2. Inspect the chart","text":"<p>Unpack the chart and look through the <code>templates/</code> directory. Each file maps to a CAPI object. Look at how values are referenced, usually with syntax like <code>.Values.controlPlaneNumber</code>. This tells you which parameters you can customize in <code>values.yaml</code>.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#3-modify-configuration","title":"3. Modify configuration","text":"<p>Edit <code>values.yaml</code> to set the cluster size, networking ranges, AMIs, or k0s configuration. If you are creating a custom template, update <code>Chart.yaml</code> with your own name and version to distinguish it from the default. You may also extend <code>values.schema.json</code> to validate new parameters.</p> <p>For example, to adjust Calico settings you might add:</p> <pre><code>k0s:\n  version: v1.32.6+k0s.0\n  cpArgs:\n    - \"--enable-worker\"\n    - \"--enable-calico\" \n  workerArgs:\n    - \"--labels=network=calico\"\n</code></pre>"},{"location":"templatehowto/modifying-clusterTemplates/#4-package-and-upload","title":"4. Package and upload","text":"<p>Once modified, package the chart:</p> <pre><code>helm package ./my-custom-template\n</code></pre> <p>Then push it to your OCI registry:</p> <pre><code>helm push my-custom-template-0.1.0.tgz oci://registry.example.com/templates\n</code></pre> <p>You may also need to make the chart public.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#connecting-the-chart-to-k0rdent","title":"Connecting the Chart to k0rdent","text":"<p>k0rdent does not fetch charts directly. Instead, it relies on FluxCD Source objects. You must define a <code>HelmRepository</code>, <code>GitRepository</code>, or <code>Bucket</code> that points to your chart location, and you must label it so that k0rdent will recognize it.</p> <p>For example:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-repo\n  namespace: project-ottowa\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  url: oci://registry.example.com/templates\n  type: oci\n  interval: 10m\n</code></pre> <p>Make sure the repository object is in the same namespace in which you will be creating the <code>ClusterTemplate</code> and <code>ClusterDeployment</code>.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#creating-the-clustertemplate-resource","title":"Creating the ClusterTemplate Resource","text":"<p>With the source in place, you can now define a <code>ClusterTemplate</code> that references your Helm chart:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nmetadata:\n  name: custom-aws-standalone\n  namespace: project-ottowa\nspec:\n  helm:\n    chartSpec:\n      chart: my-custom-template\n      version: 0.1.0\n      interval: 10m\n      sourceRef:\n        kind: HelmRepository\n        name: custom-repo\n</code></pre> <p>At this point, the k0rdent controller validates the chart, reads the default values, and checks any annotations in <code>Chart.yaml</code> that declare required providers. For example, an AWS template might require <code>infrastructure-aws</code>, <code>control-plane-k0sproject-k0smotron</code>, and <code>bootstrap-k0sproject-k0smotron</code>. The template will only be marked \"ready\" if those providers are present.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#deploying-with-a-clustertemplate","title":"Deploying with a ClusterTemplate","text":"<p>Creating a <code>ClusterTemplate</code> does not deploy a cluster. Actual clusters are instantiated through <code>ClusterDeployment</code> objects, which reference <code>ClusterTemplates</code> and may override their default values. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster\n  namespace: project-ottowa\nspec:\n  template: custom-aws-standalone\n  credential: aws-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n      rootVolumeSize: 32\n    worker:\n      instanceType: t3.small\n      rootVolumeSize: 32\n</code></pre> <p>When this object is applied, Flux installs the chart, CAPI providers reconcile their objects, and controllers like k0smotron configure k0s. Eventually, the <code>ClusterDeployment</code> is marked ready.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#customizing-templates","title":"Customizing Templates","text":"<p>Customization is usually as simple as editing <code>values.yaml</code>. The key is to understand which variables are exposed. You can determine this by inspecting the <code>templates/</code> directory and seeing where <code>.Values</code> are used. For example, if a manifest contains:</p> <pre><code>metadata:\n  name: {{ .Values.clusterName }}\n</code></pre> <p>Then you know that adding <code>clusterName: \"my-new-cluster\"</code> to your values file will set the name.</p> <p>Because Helm supports hierarchies and conditionals, values files can become quite expressive. Over time, organizations often create their own libraries of values files tailored for different environments \u2014 dev, staging, production \u2014 while reusing the same underlying templates.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#troubleshooting-and-validation","title":"Troubleshooting and Validation","text":"<p>The most common issues when building <code>ClusterTemplates</code> involve:</p> <ul> <li>Missing providers: If the template references providers not installed in your management cluster, it will fail validation.  </li> <li>Schema violations: If you supply a value of the wrong type, Helm will reject it if a schema is defined.  </li> <li>Flux sync errors: If Flux cannot reach your repository or chart, the template will not resolve.  </li> </ul> <p>Debugging usually involves checking Flux logs (<code>kubectl logs -n flux-system deployment/helm-controller</code>), verifying that sources are labeled correctly, and ensuring that provider CRDs are installed.</p>"},{"location":"templatehowto/modifying-clusterTemplates/#next-steps","title":"Next Steps","text":"<p>Building a <code>ClusterTemplate</code> is often the first step toward customizing k0rdent for your environment. Once you understand how charts, values, and templates work together, you can extend the same model to ServiceTemplates for application add-ons and ProviderTemplates for new infrastructure backends.  </p> <p>By embracing templating as the core abstraction, k0rdent gives you a powerful system: clusters, services, and providers all managed through the same consistent pattern, with strong validation, automation, and reuse built in.</p>"},{"location":"templatehowto/the-templating-system-common-threads/","title":"The Templating System \u2013 Common Threads","text":"<p>Whether you are creating templates to add services to a cluster, or to create new clusters or infrastructure providers, templates have a few things in common.</p>"},{"location":"templatehowto/the-templating-system-common-threads/#sources","title":"Sources","text":"<p>Every template references a source, which defines the content that k0rdent will deploy. For <code>ClusterTemplate</code> and <code>ProviderTemplate</code>, that source is always a Helm chart. For a <code>ServiceTemplate</code>, the source can be a Helm chart, Kustomize, or raw resources. </p>"},{"location":"templatehowto/the-templating-system-common-threads/#source-types","title":"Source types","text":"<p>If you don't have one already, start by creating a source object. These objects can be one of multiple types.</p> <ul> <li>A HelmRepository is a typical Helm chart repository, such as <code>https://charts.bitnami.com/bitnami</code> or <code>https://prometheus-community.github.io/helm-charts</code>.</li> <li>A GitRepository is a Git repo containing manifests, Helm charts, or kustomizations, such as <code>https://github.com/kubernetes-sigs/metrics-server</code> or <code>https://github.com/bitnami/charts</code>.*</li> <li>An OCIRepository is an OCI registry that stores Helm charts or other artifacts, such as <code>oci://ghcr.io/k0rdent/kcm/charts</code> or <code>oci://registry-1.docker.io/bitnamicharts</code>.</li> <li>A Bucket is an object storage bucket containing packaged resources, such as an S3 bucket (<code>s3://my-configs</code>), a GCS bucket (<code>gs://my-kustomize-overlays</code>), or an Azure Blob container (<code>https://mystorage.blob.core.windows.net/my-container</code>).</li> <li>A <code>ConfigMap</code> is a Kubernetes-native object that can hold kustomizations or raw manifests, such as a <code>ConfigMap</code> created from <code>kustomization.tar.gz</code> or from files like <code>deployment.yaml</code> and <code>namespace.yaml</code>.</li> <li>A <code>Secret</code> is a Kubernetes-native object that can also hold kustomizations or raw manifests, such as a <code>Secret</code> created from <code>kustomization.tar.gz</code> or from files like <code>deployment.yaml</code> and <code>namespace.yaml</code>.</li> </ul> <p>To deploy a kustomization, archive the folder as <code>.tar.gz</code> and create a <code>ConfigMap</code> or <code>Secret</code> from the archive:</p> <pre><code>kubectl create configmap foo --from-file=kustomization.tar.gz\n</code></pre> <p>To deploy raw resources, create a <code>ConfigMap</code> or <code>Secret</code> from the resource files:</p> <pre><code>kubectl create configmap bar --from-file=namespace.yaml --from-file=deployment.yaml\n</code></pre> <p>Supported sources:</p> Template <code>HelmRepository</code> <code>GitRepository</code> <code>OCIRepository</code> <code>Bucket</code> <code>ConfigMap</code> <code>Secret</code> <code>ClusterTemplate</code> X X X <code>ProviderTemplate</code> X X X <code>ServiceTemplate</code> X X X X X X"},{"location":"templatehowto/the-templating-system-common-threads/#source-placement","title":"Source Placement","text":"<p>k0rdent looks for resources in specific places. <code>ProviderTemplate</code> sources must be cluster-scoped and live in the system namespace (<code>kcm-system</code> by default). <code>ServiceTemplate</code> and <code>ClusterTemplate</code> objects, on the other hand, must exist the same namespace as the <code>ClusterDeployment</code> that will ultimately use them. This is important, because the source object must also be in the same namespace as the template.</p> <p>For example, if you have a user who will create a cluster in the <code>project-ottowa</code> namespace, the template must exist there, which means the source must as well. You can set the namespace in <code>metadata.namespace</code>.</p> <p>For example, you'd start with a `HelmRepository':</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-repo\n  namespace: project-ottowa\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  url: oci://ghcr.io/project-a/k0rdent/charts\n  interval: 10m\n  type: oci\n  secretRef:\n    name: nick-repo-secret\n</code></pre> <p>Then the template that references it must also be in the <code>project-ottowa</code> namespace:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nmetadata:\n  name: custom-standalone\n  namespace: project-ottowa\nspec:\n  helm:\n    chartSpec:\n      chart: custom-standalone-cp\n      version: 1.0.1\n      interval: 10m\n      sourceRef:\n        kind: HelmRepository\n        name: custom-repo\n</code></pre> <p>And finally, so will the <code>ClusterDeployment</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: ottowa-clusterdeployment\n  namespace: project-ottowa\nspec:\n  template: custom-standalone\n  credential: ottowa-cred\n  config:\n    clusterLabels: {}\n    region: ca-central-1\n    controlPlane:\n      instanceType: t3.small\n      rootVolumeSize: 32\n    worker:\n      instanceType: t3.small\n      rootVolumeSize: 32\n</code></pre> <p>Note that built-in templates ship in the <code>kcm-system</code> namespace. For multi-namespace distribution, see Template Life Cycle Management.</p> <p>Once you have a source, you can define the template to reference it. How you do that depends on the source type.</p>"},{"location":"templatehowto/the-templating-system-common-threads/#create-a-template-using-helm","title":"Create a Template using Helm","text":"<p>You can configure Helm in exactly one of three ways:</p> <ol> <li>Reference an existing <code>HelmChart</code> object using <code>.spec.helm.chartRef</code>.</li> <li>Define the chart inline with <code>.spec.helm.chartSpec</code>.</li> <li>(ServiceTemplate only) Reference a FluxCD source in <code>.spec.helm.chartSource</code>.</li> </ol> <p>Again, these are mutally exclusive.</p>"},{"location":"templatehowto/the-templating-system-common-threads/#using-chartref","title":"Using chartRef","text":"<p>The most modular way to create a template is to use a predefined <code>HelmChart</code> object.  For example, you can create that object:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmChart\nmetadata:\n  name: custom-standalone-cp-chart\n  namespace: project-ottowa\nspec:\n  chart: custom-standalone-cp\n  version: 1.0.1\n  interval: 10m\n  sourceRef:\n    kind: HelmRepository\n    name: custom-repo\n    namespace: kcm-system\n</code></pre> <p>You can then reference that <code>HelmChart</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nmetadata:\n  name: custom-standalone\n  namespace: project-ottowa\nspec:\n  helm:\n    chartRef:\n      name: custom-standalone-cp-chart\n      namespace: project-ottowa\n</code></pre>"},{"location":"templatehowto/the-templating-system-common-threads/#using-chartspec","title":"Using chartSpec","text":"<p>Rather than creating the <code>HelmChart</code> object, you can provide the information directly in the template using <code>.spec.helm.chartSpec</code>. Configure the following parameters:</p> Field Description <code>sourceRef</code>LocalHelmChartSourceReference Source object reference (<code>HelmRepository</code>, <code>GitRepository</code>, or <code>Bucket</code>) in the same namespace as template. <code>chart</code> string Name of the chart in the source. <code>version</code> string Chart version (semver). Defaults to latest. <code>interval</code>Kubernetes meta/v1.Duration Polling frequency. Defaults to 10 minutes. <p>When you define <code>.spec.helm.chartSpec</code>, the controller creates the <code>HelmChart</code> automatically.  For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nmetadata:\n  name: custom-standalone\n  namespace: project-ottowa\nspec:\n  helm:\n    chartSpec:\n      chart: custom-standalone-cp\n      version: 1.0.1\n      interval: 10m\n      sourceRef:\n        kind: HelmRepository\n        name: custom-repo\n</code></pre>"},{"location":"templatehowto/the-templating-system-common-threads/#using-alternative-template-sources","title":"Using Alternative Template Sources","text":"<p>In addition to Helm, <code>ServiceTemplate</code> supports Kustomize and raw resources. In <code>.spec</code>, you can use one of:</p> <ul> <li><code>.spec.kustomize</code></li> <li><code>.spec.resources</code></li> </ul> <p>Each accepts a <code>SourceSpec</code>.</p> Field Description <code>deploymentType</code> <code>Local</code> or <code>Remote</code>. Deploy to the management cluster (local) or managed cluster (remote). Ignored for charts. <code>localSourceRef</code> Reference to a local source (<code>ConfigMap</code>, <code>Secret</code>, <code>GitRepository</code>, <code>Bucket</code>, <code>OCIRepository</code>). <code>remoteSourceSpec</code> Config for a remote source (<code>git</code>, <code>bucket</code>, or <code>oci</code>). Note that these are mutually exclusive. <code>path</code> Path in the source object pointing to manifests or kustomization config. Ignored for raw resources. <p>For example, to use <code>.spec.kustomize</code> your template will look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: example-kustomization\n  namespace: kcm-system\nspec:\n  kustomize:\n    deploymentType: Remote\n    remoteSourceSpec:\n      git:\n        url: https://github.com/example/repo\n        ref:\n          branch: main\n    path: ./overlays/dev\n</code></pre> <p>Using <code>.spec.resources</code> looks something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: example-resources\n  namespace: kcm-system\nspec:\n  resources:\n    deploymentType: Local\n    localSourceRef:\n      kind: ConfigMap\n      name: my-configmap\n    path: ./manifests\n</code></pre>"},{"location":"templatehowto/the-templating-system-common-threads/#required-and-exposed-providers","title":"Required and Exposed Providers","text":"<p>Each template must define its Cluster API providers. In the case of the <code>ProviderTemplate</code>, these are the providers, the template exposes. In the case of the <code>ClusterTemplate</code> and <code>ServiceTemplate</code>, these are the providers that are required in order for the template to function properly.</p> <p>There are three supported provider types: <code>infrastructure</code>, <code>bootstrap</code>, and <code>control-plane</code>.</p> <p>You can define providers in two ways:</p> <ol> <li>List them in <code>spec.providers</code> from within the template.</li> <li>Add them as annotations in <code>Chart.yaml</code>.</li> </ol> <p>To list them as part of the template, you'll use the <code>spec.providers</code> field, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nmetadata:\n  name: custom-standalone\n  namespace: project-ottowa\nspec:\n  helm:\n    chartRef:\n      name: custom-standalone-cp-chart\n      namespace: project-ottowa\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n</code></pre> <p>However, you don't need to leave this to the people creating templates. Instead, you can define them in the chart itself using annotations in <code>Chart.yaml</code>:</p> <pre><code>apiVersion: v2\nname: custom-standalone-cp\ndescription: |\n  A KCM template to deploy a k0s cluster on AWS with bootstrapped control plane nodes.\ntype: application\nversion: 1.0.1\nappVersion: \"v1.32.6+k0s.0\"\nannotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n  cluster.x-k8s.io/bootstrap-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/control-plane-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/infrastructure-aws: v1beta2\n</code></pre>"},{"location":"templatehowto/the-templating-system-common-threads/#compatibility-attributes","title":"Compatibility Attributes","text":"<p>Templates can also declare compatibility with specific CAPI versions, provider contracts, or Kubernetes versions. These values can be set in the template <code>.spec</code>, or they can be set in the chart annotations, as in the section on Required and Exposed Providers. In case of a coflict between the two, the value specified in the template <code>.spec</code> takes precedence. </p> <p>Note</p> <p> All of the compatibility attributes are optional, and validation checks only take place if both of the corresponding type attributes (for example, provider contract versions in both <code>ProviderTemplate</code> and <code>ClusterTemplate</code>) are set.</p>"},{"location":"templatehowto/the-templating-system-common-threads/#servicetemplate-and-kubernetes-version-constraints","title":"ServiceTemplate and Kubernetes Version Constraints","text":"<p>If a workload can only run on specific versions of Kubernetes, you can define Kubernetes version constraints in <code>.spec.k8sConstraint</code>:</p> <p>Spec example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: nginx-service\n  namespace: project-ottowa\nspec:\n  k8sConstraint: \"^1.30.0\"\n  helm:\n    chartRef:\n      name: nginx-chart\n      namespace: project-ottowa\n</code></pre> <p>Or you can define it in the Helm chart that backs the <code>ServiceTemplate</code>:</p> <pre><code>k0rdent.mirantis.com/k8s-version-constraint: ^1.30.0\n</code></pre>"},{"location":"templatehowto/the-templating-system-common-threads/#clustertemplate-and-kubernetes-version-compatibility","title":"ClusterTemplate and Kubernetes version compatibility","text":"<p>You can also set the target Kubernetes version for <code>ClusterTemplate</code> objects:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nspec:\n  k8sVersion: 1.30.0\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n  providerContracts:\n    bootstrap-k0sproject-k0smotron: v1beta1\n    control-plane-k0sproject-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n</code></pre> <p>You can also set it using <code>annotations</code> in the underlying chart:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n  cluster.x-k8s.io/bootstrap-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/control-plane-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/infrastructure-aws: v1beta2\n  k0rdent.mirantis.com/k8s-version: 1.30.0\n</code></pre>"},{"location":"templatehowto/the-templating-system-common-threads/#providertemplate-and-capi-contract-versions","title":"ProviderTemplate and CAPI contract versions","text":"<p>The <code>ProviderTemplate</code> resource has dedicated fields to set compatible <code>CAPI</code> contract , along with CRDs contract versions supported by the provider. These contract versions will then be set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the core <code>CAPI</code> contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core <code>CAPI</code>. For the core <code>CAPI</code> Template values should be empty.</p> <p>You can define the supported CAPI contract versions in <code>.spec.capiContracts</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ProviderTemplate\nspec:\n  providers:\n  - infrastructure-aws\n  capiContracts:\n    v1alpha3: v1alpha3\n    v1alpha4: v1alpha4\n    v1beta1: v1beta1_v1beta2\n</code></pre> <p>You can also set them in the chart <code>annotations</code>:</p> <pre><code>...\nannotations:\n  cluster.x-k8s.io/provider: infrastructure-aws\n  cluster.x-k8s.io/v1alpha3: v1alpha3\n  cluster.x-k8s.io/v1alpha4: v1alpha4\n  cluster.x-k8s.io/v1beta1: v1beta1_v1beta2\n</code></pre>"},{"location":"templatehowto/the-templating-system-common-threads/#compatibility-enforcement","title":"Compatibility Enforcement","text":"<p>The aforedescribed attributes are checked for compliance with the following rules:</p> <ul> <li>Both the exact and constraint version of the same type (for example <code>k8sVersion</code> and <code>k8sConstraint</code>) must be set, otherwise no check is performed;</li> <li>If a <code>ClusterTemplate</code> object's provider's contract version does not satisfy contract versions from the related <code>ProviderTemplate</code> object, updates to the <code>ClusterDeployment</code> object will be blocked;</li> <li>If a <code>ProviderTemplate</code> object's <code>CAPI</code> contract version (for example, in a <code>v1beta1: v1beta1_v1beta2</code> key-value pair, the key <code>v1beta1</code> is the core <code>CAPI</code> contract version) is not listed in the core <code>CAPI</code> <code>ProviderTemplate</code> object, the updates to the <code>Management</code> object will be blocked;</li> <li>If a <code>ClusterTemplate</code> object's exact kubernetes version does not satisfy the kubernetes version constraint from the related <code>ServiceTemplate</code> object, the updates to the <code>ClusterDeployment</code> object will be blocked.</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>There are some situations in which you will need to take extra care to make sure  things run smoothly.</p> <ul> <li>Inspecting k0rdent events</li> <li>AWS VPCs</li> <li>EKS</li> <li>GCP</li> <li>Custom CA Certificates</li> <li>ClusterCtl Issues</li> </ul>"},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/","title":"Troubleshooting AWS VPCs","text":""},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/#aws-vpc-not-removed-when-deleting-eks-cluster","title":"AWS VPC Not Removed When Deleting EKS Cluster","text":"<p>A bug has been fixed in CAPA (Cluster API Provider AWS) for VPC removal: kubernetes-sigs/cluster-api-provider-aws#5192</p> <p>If you find that a VPC has not been deleted, you can deal with it in three different ways:</p>"},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/#applying-ownership-information-on-vpcs","title":"Applying ownership information on VPCs","text":"<p>When VPCs have owner information, all AWS resources will be removed when the k0rdent EKS cluster is deleted. So after provisioning an EKS cluster, the operator can go and set tags (for example, <code>tag:Owner</code>) and it will be  sufficient for CAPA to manage them.</p>"},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/#guardduty-vpce","title":"GuardDuty VPCE","text":"<p>Another way to prevent an issue with non-deleted VPCs is to disable GuardDuty. GuardDuty creates an extra VPCE (VPC Endpoint) not managed by CAPA and when CAPA  starts EKS cluster removal, this VPCE does not get removed.</p>"},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/#manual-removal-of-vpcs","title":"Manual removal of VPCs","text":"<p>When it is impossible to turn off GuardDuty or applying ownership tags is not permitted, you need to remove VPCs manually. Follow these steps.</p> <ol> <li> <p>Look for the affected VPC. The sign of \u201cstuck\u201d VPC looks like a hidden \u201cDelete\u201d button. </p> </li> <li> <p>Open \u201cNetwork Interfaces\u201d and attempt to detach an interface. You will see a disabled \u201cDetach\u201d button: </p> </li> <li> <p>Go to the VPC endpoints screen and remove the end-point:  </p> </li> <li> <p>OK the Endpoint deletion: </p> </li> <li> <p>Wait until the VPCE is completely removed and all network interfaces disappear. </p> </li> <li> <p>Now you can finally remove the VPC: </p> </li> </ol>"},{"location":"troubleshooting/events/","title":"Events","text":"<p>Kubernetes <code>events</code> are important system-level messages that record state changes, warnings, or other significant occurrences related to cluster components. They provide valuable insight into what is happening inside your cluster and are often essential for troubleshooting and debugging.</p> <p>In the context of k0rdent, the KCM (k0rdent Controller Manager) generates events for all major operations and reflects the current system state. If a k0rdent installation fails or behaves unexpectedly, these events can help identify the root cause.</p> <p>To retrieve all events generated by the KCM controller using the <code>kubectl</code> CLI, run:</p> <pre><code>kubectl get events --all-namespaces --field-selector reportingComponent=kcm-controller-manager\n</code></pre>"},{"location":"troubleshooting/events/#event-locations","title":"Event Locations","text":"<ul> <li>Events related to cluster-scoped objects (e.g., <code>Management</code>, <code>AccessManagement</code>, <code>Release</code>, etc.) are stored   in the <code>default</code> namespace.</li> <li>Events related to namespace-scoped objects (e.g., <code>ClusterDeployment</code>) are stored in the same namespace as the   corresponding object.</li> </ul>"},{"location":"troubleshooting/events/#viewing-events-for-specific-resources","title":"Viewing Events for Specific Resources","text":"<p>To view events for a Management:</p> <pre><code>kubectl events --for management/kcm\n</code></pre> <p>To view events for a specific ClusterDeployment:</p> <pre><code>kubectl events -n &lt;cluster-deployment-namespace&gt; --for clusterdeployment/&lt;cluster-deployment-name&gt;\n</code></pre>"},{"location":"troubleshooting/known-issues-clusterctl/","title":"Issues with clusterctl","text":"<p>When using <code>clusterctl</code> to work with child cluster objects, you may run into the following error:</p> <p><pre><code>clusterctl describe cluster kubevirt-demo\n</code></pre> <pre><code>Error: this version of clusterctl could be used only with \"v1beta2\" management clusters, \"v1beta1\" detected\n</code></pre></p> <p>This error occurs because the current CAPI operator doesn't support CAPI v1.11 yet. To solve this problem,  install a version of <code>clusterctl</code> below 1.11, such as Clusterctl v1.10.5.</p>"},{"location":"troubleshooting/known-issues-custom-ca/","title":"Helm Extensions Cannot Be Pulled from Private Registry on Hosted ClusterDeployments","text":"<p>Related issue: KCM #1612</p> <p>Note</p> <p> The issue is resolved in k0s versions <code>v1.32.6+k0s.0</code>, <code>v1.33.2+k0s.0</code>.</p> <p>When deploying Hosted ClusterDeployments on a management cluster configured with a custom container registry, and the registry uses a certificate signed by an unknown certificate authority, Helm extensions (e.g., CCM or CSI drivers) may fail to install.</p> <p>You may encounter an error similar to the following in the logs of the hosted cluster controller pods:</p> <pre><code>can''t locate chart `oci://172.19.125.101:5001/charts/vsphere-csi-driver-0.0.3`:\nfailed to do request: Head \"https://172.19.125.101:5001/v2/charts/vsphere-csi-driver/manifests/0.0.3\":\ntls: failed to verify certificate: x509: certificate signed by unknown authority\n</code></pre> <p>As a result, the Helm extension fails to install.</p> <p>The controller cannot verify the registry\u2019s TLS certificate because the required custom CA certificate is not trusted by the k0s environment.</p> <p>Support for supplying custom CA certificates for Helm extensions from OCI registries is being added in Allow providing CA certificate for helm extensions that use OCI registries. When using a k0s version that includes this fix, the workaround is no longer required.</p> <p>Workaround</p> <p>You can work around this issue by manually mounting the registry CA certificate into the hosted cluster controller\u2019s StatefulSet. The secret with the registry CA should already be present in your system namespace (as you configured <code>registryCertSecret</code> parameter):</p> <ol> <li>Patch the StatefulSet for the hosted cluster controller:</li> <li>Add a volume from the Secret</li> <li>Mount it inside the container at a known location (e.g., <code>/etc/ssl/certs/registry-ca.pem</code>)</li> </ol> <pre><code>kubectl patch statefulset kmc-&lt;CLUSTER_DEPLOYMENT_NAME&gt; \\\n  -n &lt;CLUSTER_DEPLOYMENT_NAMESPACE&gt; \\\n  --type='json' \\\n  -p='[\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/template/spec/volumes/-\",\n      \"value\": {\n        \"name\": \"registry-ca\",\n        \"secret\": {\n          \"secretName\": \"&lt;REGISTRY_SECRET_NAME&gt;\",\n          \"items\": [\n            {\n              \"key\": \"ca.crt\",\n              \"path\": \"registry-ca.pem\"\n            }\n          ]\n        }\n      }\n    },\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/template/spec/containers/0/volumeMounts/-\",\n      \"value\": {\n        \"name\": \"registry-ca\",\n        \"mountPath\": \"/etc/ssl/certs/registry-ca.pem\",\n        \"subPath\": \"registry-ca.pem\",\n        \"readOnly\": true\n      }\n    }\n  ]'\n</code></pre>"},{"location":"troubleshooting/known-issues-custom-ca/#kcm-installation-fails-with-custom-registry-using-redirects-tls-certificate-mismatch","title":"KCM Installation Fails with Custom Registry Using Redirects: TLS Certificate Mismatch","text":"<p>Note</p> <p> This issue is automatically handled starting from K0rdent v1.3.0.</p> <p>When installing KCM with a custom registry that uses a custom CA and supports redirects, installation may fail due to a known Flux bug: certSecretRef does not handle redirects.</p> <p>The error typically looks like:</p> <pre><code>x509: certificate is valid for a, not b\n</code></pre> <p>Workaround</p> <p>Until K0rdent v1.3.0 (or if you cannot upgrade), you can apply the following steps manually:</p> <ol> <li>Mount the Registry CA Certificate into Flux Source Controller</li> </ol> <p>Ensure the secret with your registry CA is present in the system namespace (<code>kcm-system</code> by default), as configured by the <code>registryCertSecret</code> parameter. Then patch the <code>source-controller</code> Deployment:</p> <pre><code>kubectl patch deployment source-controller \\\n  -n kcm-system \\\n  --type='json' \\\n  -p='[\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/template/spec/volumes/-\",\n      \"value\": {\n        \"name\": \"registry-ca\",\n        \"secret\": {\n          \"secretName\": \"&lt;REGISTRY_SECRET_NAME&gt;\",\n          \"items\": [\n            {\n              \"key\": \"ca.crt\",\n              \"path\": \"registry-ca.pem\"\n            }\n          ]\n        }\n      }\n    },\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/template/spec/containers/0/volumeMounts/-\",\n      \"value\": {\n        \"name\": \"registry-ca\",\n        \"mountPath\": \"/etc/ssl/certs/registry-ca.pem\",\n        \"subPath\": \"registry-ca.pem\",\n        \"readOnly\": true\n      }\n    }\n  ]'\n</code></pre> <ol> <li>Remove <code>certSecretRef</code> from the HelmRepository</li> </ol> <p>Patch the <code>kcm-templates</code> HelmRepository to remove the <code>certSecretRef</code> field:</p> <pre><code>kubectl patch helmrepository kcm-templates \\\n  -n kcm-system \\\n  --type=json \\\n  -p='[{\"op\": \"remove\", \"path\": \"/spec/certSecretRef\"}]'\n</code></pre>"},{"location":"troubleshooting/known-issues-eks/","title":"EKS Machines Are not Created: ControlPlaneIsStable Preflight Check Failed","text":"<p>Related issue: KCM #907</p> <p>The deployment of the EKS cluster is stuck waiting for the machines to be provisioned. The <code>MachineDeployment</code> resource is showing the following conditions:</p> <pre><code>Type: MachineSetReady\nStatus: False\nReason: PreflightCheckFailed\nMessage: ekaz-eks-dev-eks-md: AWSManagedControlPlane kcm-system/ekaz-eks-dev-eks-cp is provisioning (\"ControlPlaneIsStable\" preflight check failed)\nType: Available\nStatus: False\nReason: WaitingForAvailableMachines\nMessage: ekaz-eks-dev-eks-md: Minimum availability requires 1 replicas, current 0 available\nType: Ready\nStatus: False\nReason: WaitingForAvailableMachines\nMessage: ekaz-eks-dev-eks-md: Minimum availability requires 1 replicas, current 0 available\n</code></pre> <p>As a result, the cluster was successfully created in EKS but no nodes are available.</p> <p>Workaround</p> <ol> <li> <p>Edit the <code>MachineDeployment</code> object: <pre><code>kubectl --kubeconfig &lt;management-kubeconfig&gt; edit MachineDeployment -n &lt;cluster-namespace&gt; &lt;cluster-name&gt;-md\n</code></pre></p> </li> <li> <p>Add <code>machineset.cluster.x-k8s.io/skip-preflight-checks: \"ControlPlaneIsStable\"</code> annotation to skip the <code>ControlPlaneIsStable</code> preflight check: <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: aws-eks-dev\n    meta.helm.sh/release-namespace: kcm-system\n    machineset.cluster.x-k8s.io/skip-preflight-checks: \"ControlPlaneIsStable\" # add new annotation\n  name: aws-eks-dev-md\n</code></pre></p> </li> <li> <p>Save and exit</p> </li> </ol>"},{"location":"troubleshooting/known-issues-gcp/","title":"GCP ClusterDeployment Deletion Stuck: Resource Is in Use by Another Resource","text":"<p>Related issue: KCM #1476</p> <p>When deleting a GCP cluster, the process can become stuck during the VPC deletion phase. The following error may appear in the <code>cluster-api-provider-gcp</code> logs:</p> <pre><code>E0505 13:33:29.573486       1 controller.go:324] \"Reconciler error\" err=\"googleapi:\nError 400: RESOURCE_IN_USE_BY_ANOTHER_RESOURCE - The network resource 'projects/k0rdent-dev/global/networks/network-name'\nis already being used by 'projects/k0rdent-dev/global/routes/kubernetes-f85faa80-9d44-4a73-a679-97aa072ecb4e'\"\ncontroller=\"gcpcluster\" controllerGroup=\"infrastructure.cluster.x-k8s.io\" controllerKind=\"GCPCluster\"\nGCPCluster=\"kcm-system/gcp-ekaz\" namespace=\"kcm-system\" name=\"gcp-ekaz\" reconcileID=\"74d04828-3892-4ad5-80c9-dd9187857c6e\"\n</code></pre> <p>This typically occurs when orphaned routes remain in the VPC and reference deleted compute instances.</p> <p>As a result, the <code>ClusterDeployment</code> object cannot be fully deleted until these route resources are removed.</p> <p>Workaround</p> <p>Manually delete all orphaned routes using the <code>gcloud</code> CLI or the Google Cloud Console. For example, using the <code>gcloud</code> CLI:</p> <ol> <li> <p>Identify the orphaned route from the <code>cluster-api-provider-gcp</code> logs (e.g., <code>kubernetes-f85faa80-9d44-4a73-a679-97aa072ecb4e</code>).</p> </li> <li> <p>List all routes in the affected network:</p> <pre><code>gcloud compute routes list --filter=\"network:&lt;networkName&gt;\"\n</code></pre> <p>Example output for the network <code>test-net</code>: <pre><code>NAME                                             NETWORK   DEST_RANGE     NEXT_HOP                                                           PRIORITY\ndefault-route-r-e39cfb79ee7071c0                 test-net  10.128.0.0/20  test-net                                                           0\ndefault-route-r-ffb0b56ff2dc7b8a                 test-net  10.212.0.0/20  test-net                                                           0\nkubernetes-24a016be-92d0-4298-abec-772bf45ba189  test-net  10.244.4.0/24  europe-west4-a/instances/test-ksi-gcp-18apr-20-n2d-cp-2            1000\nkubernetes-3ec1f296-768b-445d-b280-23560bed3ed1  test-net  10.244.2.0/24  europe-west4-a/instances/test-ksi-gcp-18apr-20-n2d-md-rqr5q-968gx  1000\n</code></pre></p> </li> <li> <p>Delete orphaned routes where the instance in the NEXT_HOP column no longer exists:</p> <pre><code>gcloud compute routes delete &lt;routeName&gt;\n</code></pre> <p>Repeat for each orphaned route to allow the VPC to be deleted successfully.</p> </li> </ol>"},{"location":"user/","title":"k0rdent User Guide","text":"<p>This guide walks you through the basics of using k0rdent, such as spinning up your own Kubernetes clusters, and then gets into adding extras, such as tools, applications, and services, to those clusters. We'll show you how to use templates to make that easier, add those extras to your deployments, and keep an eye on everything. Plus, we'll cover how to make sure your clusters avoid configuration drift. Basically, it's all about getting your apps running smoothly with k0rdent.</p> <ul> <li>Creating clusters as a k0rdent user</li> <li>Creating and adding services to clusters as a k0rdent user </li> <li>Enabling drift detection</li> </ul>"},{"location":"user/user-create-cluster/","title":"Deploying a Cluster","text":"<p>k0rdent simplifies the process of deploying and managing Kubernetes clusters across various cloud platforms through the use of <code>ClusterDeployment</code> objects, which include all of the information k0rdent needs to know in order to create the cluster you want. This <code>ClusterDeployment</code> system relies on predefined templates and credentials.</p> <p>A cluster deployment typically involves:</p> <ol> <li>Credentials for the infrastructure provider (such as AWS, vSphere, and so on).</li> <li>A template that defines the desired cluster configuration (for example, number of nodes or instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster:</p> <ol> <li> <p>Obtain the <code>Credential</code> object</p> <p>k0rdent needs credentials to communicate with the infrastructure provider (for example, AWS, Azure, or vSphere). These credentials enable k0rdent to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created ahead of time and made available to users. You can see all of the existing <code>Credential</code> objects by querying the management cluster:</p> <pre><code>kubectl get credentials -n accounting\n</code></pre> <p>When you find a <code>Credential</code> that looks appropriate, you can get more information by <code>describe</code>-ing it, as in:</p> <pre><code>kubectl describe credential accounting-cluster-credential -n accounting\n</code></pre> <p>You'll see the YAML for the <code>Credential</code> object, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: accounting-cluster-credential\n  namespace: accounting\nspec:\n  description: \"Credentials for Accounting AWS account\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: accountingd-cluster-identity\n</code></pre> <p>As you can see, the <code>.spec.description</code> field gives more information about the <code>Credential</code>.</p> <p>If the <code>Credential</code> you need doesn't yet exist, you can ask your cloud administrator to create it, or you can follow the instructions in the Credential System, as well as the specific instructions for your target infrastructure, to create it yourself.</p> <p>Tip</p> <p> Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in k0rdent are predefined configurations that describe how to set up the cluster. Templates include details such as:</p> <ul> <li>The number and type of control plane and worker nodes.</li> <li>Networking settings.</li> <li>Regional deployment preferences.</li> </ul> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-3                   true\nkcm-system   aws-hosted-cp-1-0-16             true\nkcm-system   aws-standalone-cp-1-0-16         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-19           true\nkcm-system   azure-standalone-cp-1-0-17       true\nkcm-system   docker-hosted-cp-1-0-4          true\nkcm-system   gcp-gke-1-0-6                   true\nkcm-system   gcp-hosted-cp-1-0-16             true\nkcm-system   gcp-standalone-cp-1-0-15         true\nkcm-system   openstack-standalone-cp-1-0-17   true\nkcm-system   remote-cluster-1-0-17            true\nkcm-system   vsphere-hosted-cp-1-0-15         true\nkcm-system   vsphere-standalone-cp-1-0-15     true\n</code></pre> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-1-0-16 -n kcm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>Once you have the <code>Credential</code> and the <code>ClusterTemplate</code> you can create the <code>ClusterDeployment</code> object configuration. It includes:</p> <ul> <li>The template to use.</li> <li>The credentials for the infrastructure provider.</li> <li>Optional customizations such as instance types, regions, and networking.</li> </ul> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  cleanupOnDeletion: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-16\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> <p>Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> <p>Tip</p> <p> If automatic cleanup of potentially orphaned LoadBalancer Services and Storage devices during deletion of the <code>ClusterDeployment</code> object is required, set <code>.spec.cleanupOnDeletion</code> to <code>true</code>. This is a best-effort cleanup: if there is no possibility to acquire a managed cluster's kubeconfig, the cleanup will not happen.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the k0rdent management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to k0rdent.</p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>k0rdent will now start provisioning resources (for example, VMs or networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, you can retrieve the kubeconfig file for the new cluster so you can interact with the cluster using <code>kubectl</code>:</p> <pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> <p>You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"user/user-create-cluster/#cleanup","title":"Cleanup","text":"<p>When you're finished you'll want to remove the cluster. Because the cluster is represented by the <code>ClusterDeployment</code> object, deleting the cluster is a simple matter of deleting that object. For example:</p> <pre><code>kubectl delete clusterdeployment &lt;cluster-name&gt; -n kcm-system\n</code></pre> <p>Note that even though the Kubernetes object is deleted immediately, it will take a few minutes for the actual resources to be removed.</p>"},{"location":"user/user-enable-drift-detection/","title":"Detecting and Correcting Drift","text":""},{"location":"user/user-enable-drift-detection/#how-drift-detection-and-correction-works","title":"How Drift Detection and Correction Works","text":"<p>The drift-detection-manager watches for the deployed helm chart resources (that is, the resources deployed via a <code>ServiceTemplate</code>) and if it detects any changes  in the spec of the resources based on hash value, it updates the status of the <code>ResourceSummary</code> object. This change triggers the addon-controller in the <code>projectsveltos</code> namespace in the management cluster to update the status of the associated <code>ClusterSummary</code> object, which then triggers a reconcile to  re-deploy the spec to the target cluster.</p> <p>Note</p> <p>The <code>ResourceSummary</code> and <code>ClusterSummary</code> are CRDs provided by Sveltos.</p>"},{"location":"user/user-enable-drift-detection/#enabling-drift-detection","title":"Enabling Drift Detection","text":"<p>Set <code>.spec.serviceSpec.syncMode=Continuous</code> in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object to enable drift detection and correction. Sveltos will then automatically deploy the drift-detection-manager on the targeted clusters:</p> <p><pre><code>kubectl -n projectsveltos get deployments.apps \n</code></pre> <pre><code>NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndrift-detection-manager   1/1     1            1           152m\nsveltos-agent-manager     1/1     1            1           152m\n</code></pre></p>"},{"location":"user/user-enable-drift-detection/#using-drift-ignore","title":"Using Drift Ignore","text":"<p>Certain resources can be completely opted out of drift correction by using this feature. In the following example, the \"ingress-nginx/ingress-nginx-controller\" deployment is ignored for drift correction on the target cluster.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n      . . .\n    priority: 100\n    syncMode: ContinuousWithDriftDetection\n    driftIgnore:\n      - target:\n        group: apps\n        version: v1\n        kind: Deployment\n        name: ingress-nginx-controller\n        namespace: ingress-nginx\n  . . .\n</code></pre> <p>If we manually remove the <code>app.kubernetes.io/managed-by=Helm</code> label, we can observe that the drift is not corrected as can be seen in the following watch output.</p> <p><pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller --show-labels -w\n</code></pre> <pre><code>NAME                       READY   UP-TO-DATE   AVAILABLE   AGE     LABELS\ningress-nginx-controller   3/3     3            3           3h58m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx,app.kubernetes.io/version=1.11.0,helm.sh/chart=ingress-nginx-4.11.0\ningress-nginx-controller   3/3     3            3           3h59m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx,app.kubernetes.io/version=1.11.0,helm.sh/chart=ingress-nginx-4.11.0\n</code></pre></p> <p>This can also be verified by observing that <code>ignoreForConfigurationDrift: true</code> is set for the targeted resource in the <code>ResourceSummary</code> spec on the target cluster.</p> <pre><code>kind: ResourceSummary\nmetadata:\n  . . .\nspec:\n  chartResources:\n  - chartName: ingress-nginx\n    group:\n    . . .\n    - group: apps\n      ignoreForConfigurationDrift: true\n      kind: Deployment\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\n      version: v1\n    releaseName: ingress-nginx\n    releaseNamespace: ingress-nginx\nstatus:\n  helmResourceHashes:\n  . . .\n</code></pre> <p>Yet another way to check if a resource is being ignored for drift is by verifying that the <code>projectsveltos.io/driftDetectionIgnore: ok</code> annotation has been applied to it, as in:</p> <pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller -o=jsonpath='{.metadata.annotations}'\n{\"deployment.kubernetes.io/revision\":\"1\",\"meta.helm.sh/release-name\":\"ingress-nginx\",\"meta.helm.sh/release-namespace\":\"ingress-nginx\",\"projectsveltos.io/driftDetectionIgnore\":\"ok\"}%\n</code></pre>"},{"location":"user/user-enable-drift-detection/#removing-drift-ignore","title":"Removing Drift Ignore","text":"<p>The drift ignore setting can be removed by removing the <code>.spec.serviceSpec.driftIgnore</code> field.</p>"},{"location":"user/user-enable-drift-detection/#using-drift-exclusions","title":"Using Drift Exclusions","text":"<p>Certain fields of a resource can be excluded from drift detection using this feature. In the following example, the <code>.spec.replicas</code> field of the <code>ingress-nginx/ingress-nginx-controller</code> deployment on the target cluster is excluded from drift detection.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n      . . .\n    priority: 100\n    syncMode: ContinuousWithDriftDetection\n    driftExclusions:\n      - paths:\n        - \"/spec/replicas\"\n        target:\n          kind: Deployment\n          name: ingress-nginx-controller\n          namespace: ingress-nginx\n  . . .\n</code></pre> <p>If we manually edit the replicas to be 1, the number of replicas is not corrected back to 3 as is indicated by the following watch output.</p> <pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller -o=jsonpath='{.spec.replicas}' -w\n3111\n</code></pre> <p>We can also verify that this is the case by observing that the <code>ResourceSummary</code> object has the following patch in its spec now.</p> <pre><code>kind: ResourceSummary\nmetadata:\n  . . .\nspec:\n  chartResources:\n  - chartName: ingress-nginx\n    group:\n    . . .\n    releaseName: ingress-nginx\n    releaseNamespace: ingress-nginx\n  patches:\n  - patch: |-\n      - op: remove\n        path: /spec/replicas\n    target:\n      kind: Deployment\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\nstatus:\n  helmResourceHashes:\n  . . .\n</code></pre>"},{"location":"user/user-enable-drift-detection/#removing-drift-exclusion","title":"Removing Drift Exclusion","text":"<p>The drift exclusion can be removed by removing the <code>.spec.serviceSpec.driftExclusion</code> field and re-triggering the drift correction by editing any field in the \"ingress-nginx/ingress-nginx-controller\" deployment. This will force a drift correction and since the drift exclusion has been removed, it will restore the deployment to it's original spec.</p>"},{"location":"user/services/","title":"Deploy Services to a Managed Cluster","text":"<p>At its heart, everything in k0rdent is based on templates that help define Kubernetes objects. For clusters, these are <code>ClusterTemplate</code> objects. For applications and services, these are <code>ServiceTemplate</code> objects.</p> <p>You can find numerous useful applications in the k0rdent Catalog, or you can create them yourself.</p> <ul> <li>Understanding ServiceTemplates</li> <li>Adding a Service to a ClusterDeployment</li> <li>Beach Head Services</li> <li>Checking Status</li> <li>Remove Beach Head Services</li> <li>ServiceTemplate Parameters</li> <li>Service Upgrades and ServiceTemplateChain</li> </ul>"},{"location":"user/services/add-service-to-clusterdeployment/","title":"Adding a <code>Service</code> to a <code>ClusterDeployment</code>","text":"<p>To add the service defined by this template to a cluster, you simply add it to the <code>ClusterDeployment</code> object when you create it, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-1-0-16\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.0\n        name: ingress-nginx\n        namespace: tenant42\n    priority: 100\n</code></pre> As you can see, you're simply referencing the template in the <code>.spec.serviceSpec.services[].template</code> field of the <code>ClusterDeployment</code> to tell k0rdent that you want this service to be part of this cluster.</p> <p>If you wanted to add this service to an existing cluster, you would simply patch the definition of the <code>ClusterDeployment</code>, as in:</p> <pre><code>kubectl patch clusterdeployment my-cluster-deployment -n tenant42 --type='merge' -p '\nspec:\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.0\n        name: ingress-nginx\n        namespace: tenant42\n</code></pre> <p>Let's look at a more complex case, involving deploying beach-head services on a single cluster.</p>"},{"location":"user/services/beach-head/","title":"Deployment of beach-head services","text":"<p>Beach-head services can be installed on a cluster deployment (that is, a target cluster) using the <code>ClusterDeployment</code> object, just as with a single service. Consider the following example of a <code>ClusterDeployment</code> object for AWS Infrastructure Provider with beach-head services.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  config:\n    clusterLabels: {}\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    controlPlane:\n      amiID: ami-0eb9fdcf0d07bd5ef\n      instanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: t3.small\n      controlPlaneNumber: 1\n      publicIP: true\n      region: ca-central-1\n    worker:\n      amiID: ami-0eb9fdcf0d07bd5ef\n      instanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: t3.small\n    workersNumber: 1\n  credential: aws-cluster-identity-cred\n  serviceSpec:\n    services:\n      - template: kyverno-3-2-6\n        name: kyverno\n        namespace: kyverno\n      - template: ingress-nginx-4-11-3\n        name: ingress-nginx\n        namespace: ingress-nginx\n    priority: 100\n  template: aws-standalone-cp-1-0-16\n</code></pre> <p>In the example above, the fields under <code>serviceSpec</code> are relevant to the deployment of beach-head services.</p> <p>Note</p> <p> Refer to the Template Guide for more detail about these fields.</p> <p>This example <code>ClusterDeployment</code> object deploys kyverno and ingress-nginx, as referred to by their service templates respectively, on the target cluster.  As before, the <code>ServiceTemplate</code> includes information on the service. For example, here is the <code>ServiceTemplate</code> for kyverno:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      version: 3.2.6\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n</code></pre> <p>The <code>k0rdent-catalog</code> helm repository hosts the actual kyverno chart version 3.2.6. For more details see the Bring your own Templates guide.</p>"},{"location":"user/services/beach-head/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Helm values can be passed to each beach-head service with the <code>.spec.serviceSpec.services[].values</code> field in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object. For example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - template: ingress-nginx-4-11-3\n      name: ingress-nginx\n      namespace: ingress-nginx\n      values: |\n        ingress-nginx:\n          controller:\n            replicaCount: 3\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n      values: |\n        kyverno:\n          admissionController:\n            replicas: 3\n    - name: motel-regional\n      namespace: motel\n      template: motel-regional-0-1-1\n      values: |\n        victoriametrics:\n          vmauth:\n            ingress:\n              host: vmauth.kcm0.example.net\n            credentials:\n              username: motel\n              password: motel\n        grafana:\n          ingress:\n            host: grafana.kcm0.example.net\n   . . .\n</code></pre> </p> <p>Note</p> <p>The values for ingress-nginx and kyverno start with the \"ingress-nginx:\" and \"kyverno:\" keys respectively because the helm charts used by the ingress-nginx-4-11-3 and kyverno-3-2-6 <code>ServiceTemplate</code> objects use the official upstream helm charts for ingress-nginx and kyverno as dependencies.</p>"},{"location":"user/services/beach-head/#templating-custom-values","title":"Templating Custom Values","text":"<p>Using the Sveltos templating feature, we can also write templates that can be useful for automatically fetching pre-existing information within the cluster. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - template: motel-1-5-0\n      name: motel\n      namespace: motel\n    - template: myappz-0-3-0\n      name: myappz\n      namespace: myappz\n      values: |\n        controlPlaneEndpointHost: {{ .Cluster.spec.controlPlaneEndpoint.host }}\n        controlPlaneEndpointPort: \"{{ .Cluster.spec.controlPlaneEndpoint.port }}\"\n    priority: 100\n    . . .        \n</code></pre> <p>In this case, the host and port information will be fetched from the spec of the CAPI cluster that hosts this <code>ClusterDeployment</code>.</p>"},{"location":"user/services/beach-head/#service-dependencies","title":"Service Dependencies","text":"<p>Dependencies among beach-head services can also be defined using the <code>.spec.serviceSpec.services[].dependsOn</code> field in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object. A service will not be deployed until all services that it depends on have been successfully deployed. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n      - template: cert-manager-1-18-2\n        name: cert-manager\n        namespace: cert-manager\n        values: |\n          cert-manager:\n            crds:\n              enabled: true\n      - template: ingress-nginx-4-13-0\n        name: nginx\n        namespace: nginx\n        dependsOn:\n          - name: cert-manager\n            namespace: cert-manager\n    . . .\n</code></pre> <p>In this case, <code>nginx</code> will not be deployed until <code>cert-manager</code> has been successfully deployed. If there already a <code>cert-manager</code> service already exists in the specified namespace (for example, if it was deployed by another <code>MultiClusterService</code> with higher priorirty), k0rdent will still recognize it as a fulfilled dependency for <code>nginx</code> and start deploying <code>nginx</code>.</p>"},{"location":"user/services/checking-status/","title":"Checking status","text":"<p>The <code>.status.services</code> field of the <code>ClusterDeployment</code> object shows the status for each of the beach-head services. For example, if you were to <code>describe</code> the <code>ClusterDeployment</code> with these services, you would see conditions that show status information, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 1\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    . . .\nstatus:\n  . . .\n  observedGeneration: 1\n  services:\n  - clusterName: my-cluster-deployment\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>Based on the information above both kyverno and ingress-nginx are installed in their respective namespaces on the target cluster. You can check to see for yourself:</p> <p><pre><code>kubectl get pod -n kyverno\n</code></pre> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\nkyverno-admission-controller-96c5d48b4-sg5ts     1/1     Running   0          2m39s\nkyverno-background-controller-65f9fd5859-tm2wm   1/1     Running   0          2m39s\nkyverno-cleanup-controller-848b4c579d-ljrj5      1/1     Running   0          2m39s\nkyverno-reports-controller-6f59fb8cd6-s8jc8      1/1     Running   0          2m39s\n</code></pre> <pre><code>kubectl get pod -n ingress-nginx \n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ningress-nginx-controller-cbcf8bf58-zhvph   1/1     Running   0          24m\n</code></pre></p> <p>Youc an get more information on how to access the child cluster in the create a cluster deployment chapter, and more on <code>ServiceTemplate</code> objects in the Template Guide.</p>"},{"location":"user/services/remove-beach-head/","title":"Removing beach-head services","text":"<p>To remove a beach-head service simply remove its entry from <code>.spec.serviceSpec.services</code>. The example below removes <code>kyverno-3-2-6</code>, so its status also removed from <code>.status.services</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 2\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    priority: 100\n    . . .\nstatus:\n  . . .\n  observedGeneration: 2\n  services:\n  - clusterName: wali-aws-dev\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre>"},{"location":"user/services/service-upgrade/","title":"Upgrading and rolling back deployed services","text":""},{"location":"user/services/service-upgrade/#service-version-upgrade","title":"Service Version Upgrade","text":"<p>When you deploy a service to a cluster, you can specify a <code>ServiceTemplateChain</code> that will be used to define available upgrade path for the service. </p> <p>Info</p> <p> Before you begin, make sure all templates you're going to add to <code>ServiceTemplateChain</code> exist in system namespace (normally <code>kcm-system</code>). Templates can be propagated to other namespaces using Template Life Cycle Management.</p> <p>First, you need to create a <code>ServiceTemplateChain</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplateChain\nmetadata:\n  name: ingress-nginx-chain\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: ingress-nginx-4-11-3\n      availableUpgrades:\n      - name: ingress-nginx-4-11-5\n    - name: ingress-nginx-4-11-5\n</code></pre> <p>This object defines a chain of templates that can be used to upgrade the service.</p> <p>Warning</p> <p> The <code>ServiceTemplateChain</code> has immutable spec. You can't change it after it's created.</p> <p>After <code>ServiceTemplateChain</code> is created, you can use it in <code>ClusterDeployment</code> or <code>MutliClusterService</code> objects to define the available upgrade path for the service:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-1-0-16\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: ingress-nginx-4-11-3\n        templateChain: ingress-nginx-chain\n        name: ingress-nginx\n        namespace: tenant42\n    priority: 100\n</code></pre> <p>Warning</p> <p> If no <code>templateChain</code> is specified for the service, the service cannot be upgraded because no path is availble. If you try to change the service template, in the logs, you'll see an error message such as:</p> <pre><code>service ingress-nginx/ingress-nginx can't be upgraded from ingress-nginx-4-11-3 to ingress-nginx-4-11-5\n</code></pre> <p>After the <code>ClusterDeployment</code> or <code>MultiClusterService</code> has been reconciled, you will see available upgrade paths for the service in the status:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  ...\nstatus:\n  servicesUpgradePaths:\n    - availableUpgrades:\n        - upgradePaths:\n            - ingress-nginx-4-11-5\n      name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n</code></pre> <p>Now you can update the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object to upgrade the service to the available version:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-1-0-16\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: ingress-nginx-4-11-5 # &lt;-- upgrade to the latest version\n        templateChain: ingress-nginx-chain\n        name: ingress-nginx\n        namespace: tenant42\n    priority: 100\n</code></pre>"},{"location":"user/services/service-upgrade/#service-version-rollback","title":"Service Version Rollback","text":"<p>In general, the process of rolling back a service to the previous version is the same as upgrading the service in the first place. You'll need to create a separate <code>ServiceTemplateChain</code>, which defines the downgrade path:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplateChain\nmetadata:\n  name: ingress-nginx-chain\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: ingress-nginx-4-11-3\n    - name: ingress-nginx-4-11-5\n      availableUpgrades:\n        - name: ingress-nginx-4-11-3\n</code></pre> <p>After the <code>ServiceTemplateChain</code> has been created, you can use it in a <code>ClusterDeployment</code> or <code>MutliClusterService</code> object to define the available rollback path for the service. Follow these steps:</p> <ol> <li>Update the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object with the rollback <code>ServiceTemplateChain</code>.</li> <li>Wait for the <code>ClusterDeployment</code> or <code>MultiClusterService</code> to be reconciled.</li> <li>Update the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object with the previous version of the service.</li> </ol>"},{"location":"user/services/servicetemplate-parameters/","title":"Parameter List","text":"<p>Here is an idea of the parameters involved.</p> Parameter Example Description <code>.spec.serviceSpec.syncMode</code> <code>Continuous</code> Specifies how beach-head services are synced i the target cluster (default:<code>Continuous</code>) <code>.spec.serviceSpec.DriftIgnore</code> specifies resources to ignore for drift detection <code>.spec.serviceSpec.DriftExclusions</code> specifies specific configurations of resources to ignore for drift detection <code>.spec.serviceSpec.priority</code> <code>100</code> Sets the priority for the beach-head services defined in this spec (default: <code>100</code>) <code>.spec.serviceSpec.stopOnConflict</code> <code>false</code> Stops deployment of beach-head services upon first encounter of a conflict (default: <code>false</code>) <code>.spec.serviceSpec.services[].template</code> <code>kyverno-3-2-6</code> Name of the <code>ServiceTemplate</code> object located in the same namespace <code>.spec.serviceSpec.services[].name</code> <code>my-kyverno-release</code> Release name for the beach-head service <code>.spec.serviceSpec.services[].namespace</code> <code>my-kyverno-namespace</code> Release namespace for the beach-head service (default: <code>.spec.services[].name</code>) <code>.spec.serviceSpec.services[].values</code> <code>replicas: 3</code> Helm values to be used with the template while deployed the beach-head services <code>.spec.serviceSpec.services[].valuesFrom</code> `` Can reference a ConfigMap or Secret containing helm values <code>.spec.serviceSpec.services[].disable</code> <code>false</code> Disable handling of this beach-head service (default: <code>false</code>)"},{"location":"user/services/understanding-servicetemplates/","title":"Understanding ServiceTemplates","text":"<p><code>ServiceTemplate</code> objects are a representation of the source where k0rdent can find a resource or set of resources to be deployed as a complete application.</p> <p><code>ServiceTemplate</code> supports the following types as a source:</p> <ul> <li><code>HelmChart</code></li> <li><code>GitRepository</code></li> <li><code>Bucket</code></li> <li><code>OCIRepository</code></li> <li><code>Secret</code></li> <li><code>ConfigMap</code></li> </ul>"},{"location":"user/services/understanding-servicetemplates/#helm-based-servicetemplate","title":"Helm-based ServiceTemplate","text":"<p>Note</p> <p> <code>ServiceTemplate</code> can be defined using <code>.spec.helm.chartSpec</code> or <code>.spec.helm.chartRef</code> if only Helm chart being defined or referred is backed by <code>HelmRepository</code> or <code>GitRepository</code> object.</p> <p>Helm-based <code>ServiceTemplate</code> can be created in three ways:</p> <ul> <li>by defining Helm chart right in the template object</li> </ul> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: foo\n  namespace: bar\nspec:\n  helm:\n    chartSpec:\n      chart: ingress-nginx\n      version: 4.11.0\n      interval: 10m\n      sourceRef:\n        kind: HelmRepository\n        name: foo-repository\n</code></pre> <p>In this case the corresponding <code>HelmChart</code> object will be created by the controller.</p> <ul> <li>by referring the existing Helm chart</li> </ul> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: foo\n  namespace: bar\nspec:\n  helm:\n    chartRef:\n      kind: HelmChart\n      name: foo-chart\n</code></pre> <ul> <li> <p>by defining Helm chart source, which can be one of types provided by FluxCD:</p> </li> <li> <p>HelmRepository</p> </li> <li>GitRepository</li> <li>Bucket</li> </ul> <p>Source can already exist or can be created by the controller.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: foo\n  namespace: bar\nspec:\n  helm:\n    chartSource:\n      localSourceRef:\n        kind: GitRepository\n        name: foo-repository\n      path: \"./charts\"\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: foo\n  namespace: bar\nspec:\n  helm:\n    chartSource:\n      remoteSourceRef:\n        git:\n          url: https://github.com/bar/foo.git\n          reference:\n            branch: main\n          interval: 10m\n        path: \"./charts\"\n</code></pre>"},{"location":"user/services/understanding-servicetemplates/#kustomize-based-servicetemplate","title":"Kustomize-based ServiceTemplate","text":"<p>Kustomize-based <code>ServiceTemplate</code> can be created with either local or remote source:</p> <ul> <li>by using existing flux source object - <code>GitRepository</code>, <code>Bucket</code> or <code>OCIRepository</code> - or using existing <code>ConfigMap</code> or <code>Secret</code></li> </ul> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: foo\n  namespace: bar\nspec:\n  kustomize:\n    localSourceRef:\n      kind: Bucket  # also can be GitRepository, OCIRepository, ConfigMap or Secret\n      name: foo-bar\n    deploymentType: Remote\n    path: \"./base\"\n</code></pre> <p><code>ConfigMap</code> or <code>Secret</code> in this case must embed the tar-gzipped archive containing the kustomization files. This can be done by the following command, assuming the the archive was already created:</p> <pre><code>kubectl create configmap foo-bar --from-file=/path/to/kustomization/archive.tar.gz\n</code></pre> <ul> <li>by defining remote source right in the template object</li> </ul> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kustomization-app\n  namespace: kcm-system\nspec:\n  kustomize:\n    remoteSourceSpec:\n      oci:\n        url: oci://ghcr.io/org/project-x\n        reference:\n          tag: latest\n        interval: 10m\n    deploymentType: Remote\n    path: \"./overlays\"\n</code></pre> <p><code>.spec.kustomize.remoteSourceSpec</code> has mutual exclusive fields <code>.git</code>, <code>.bucket</code> and <code>.oci</code> which inline <code>GitRepositorySpec</code>, <code>BucketSpec</code> and <code>OCIRepositorySpec</code> respectively.</p>"},{"location":"user/services/understanding-servicetemplates/#raw-resources-based-servicetemplate","title":"Raw-resources-based ServiceTemplate","text":"<p>Similar to kustomize-based <code>ServiceTemplate</code>, raw-resources-based <code>ServiceTemplate</code> can be created with either local or remote source. Using the remote source has no difference with kustomize-based <code>ServiceTemplate</code>, however using local source slightly differ in case <code>ConfigMap</code> or <code>Secret</code> object is referred as a source:</p> <ul> <li><code>spec.resources.localSourceRef.path</code> will be ignored</li> <li>referred <code>ConfigMap</code> or <code>Secret</code> must contain inlined resources' definitions instead of embedding tar-gzipped archive.</li> </ul>"}]}